# 第13章：多言語・多モーダルモデルの理論

この章では、テキストを超えた**多言語モデル**と、画像・音声などを統合する**多モーダルモデル**の理論を学びます。

---

## 13.1 多言語モデルのアーキテクチャ

### 13.1.1 多言語モデルの課題

**問題1：語彙の爆発**

各言語が独自の語彙を持つ

```
英語: 50,000トークン
日本語: 50,000トークン
中国語: 50,000トークン
...

単純な結合: 数百万トークン！
→ 埋め込み層が巨大化
```

**問題2：データの不均衡**

```
訓練データ:
  英語: 70%
  中国語: 5%
  スペイン語: 3%
  ...
  低資源言語: <0.1%

→ 低資源言語の性能が悪い
```

**問題3：言語間の干渉**

異なる言語が混在すると混乱

```
"I went to 東京 and ate sushi"

どの言語で処理？
```

### 13.1.2 共有語彙の構築

**Byte Pair Encoding (BPE)による統一**

言語横断的なサブワード分割

```
英語: "international" → ["inter", "national"]
日本語: "国際的" → ["国", "際", "的"]
共通接頭辞: "inter-" と "国際" が類似文脈で出現
→ 埋め込み空間で近くに配置
```

**多言語BPEの訓練：**

```python
# 全言語のデータを結合
multilingual_corpus = [
    english_data,
    japanese_data,
    chinese_data,
    ...
]

# 統一BPE語彙を学習
vocab = train_bpe(
    multilingual_corpus,
    vocab_size=250000  # 多言語対応のため大きめ
)
```

**語彙サイズの最適化：**

| モデル | 語彙サイズ | 対応言語 |
|--------|-----------|---------|
| GPT-3 | 50,257 | 主に英語 |
| mT5 | 250,000 | 101言語 |
| BLOOM | 250,680 | 46言語 |
| LLaMA | 32,000 | 多言語（効率重視） |

### 13.1.3 言語識別と切り替え

**明示的言語ID：**

各トークンに言語タグを付与

```
入力: "<en> Hello <ja> こんにちは <en> World"

各トークンに言語埋め込みを追加:
  h_total = h_token + h_position + h_language
```

**暗黙的学習：**

モデルが文脈から言語を推論

```
文脈: "Bonjour, je m'appelle..."
→ モデルが「フランス語」と推論
→ フランス語モードで処理
```

**コード切り替え対応：**

同一文内の言語切り替えを処理

```
"I want to eat 寿司 in パリ"

attention機構:
  "eat" → "寿司" (跨言語の関連性)
  "in" → "パリ" (文法的関係)
```

### 13.1.4 アーキテクチャの選択

**共有 vs 言語固有**

**完全共有モデル：**

```
全言語で同じTransformer
  ↓
利点: パラメータ効率的、言語間転移
欠点: 干渉、低資源言語で性能低下
```

**言語固有エキスパートモデル：**

```
Transformer層
  ↓
言語検出
  ↓
  ├─ 英語エキスパート
  ├─ 日本語エキスパート
  ├─ 中国語エキスパート
  └─ ...
```

**ハイブリッド（推奨）：**

```
下位層: 共有（汎用的特徴）
  ↓
中間層: 言語グループ固有
  ├─ ラテン系言語グループ
  ├─ 東アジア言語グループ
  └─ ...
  ↓
上位層: 共有（タスク固有）
```

---

## 13.2 言語間転移のメカニズム

### 13.2.1 共有埋め込み空間

**仮説：**

異なる言語が同じ概念空間にマッピング

**視覚化：**

```
埋め込み空間（2D投影）

      動物
       │
   cat │ 猫
    ●  │  ●
       │     neko
   dog │ 犬   ●
    ●  │  ●
       │
──────┼──────
       │
   red │ 赤
    ●  │  ●
       │
  blue │ 青
    ●  │  ●
       │
      色
```

**数学的定式化：**

言語 $L_1$ と $L_2$ の埋め込み

$$\phi_{L_1}(w_1) \approx \phi_{L_2}(w_2) \quad \text{（同じ意味なら）}$$

例：
$$\phi_{\text{en}}(\text{"cat"}) \approx \phi_{\text{ja}}(\text{"猫"})$$

### 13.2.2 構造的アライメント

**統語構造の共有：**

多くの言語が類似の統語パターン

```
英語: Subject - Verb - Object
      "I eat sushi"

日本語: Subject - Object - Verb
       "私は寿司を食べる"

深層構造は類似
→ Transformerが対応を学習
```

**依存関係の学習：**

```
英語: "beautiful flower"
       形容詞 → 名詞

日本語: "美しい花"
       形容詞 → 名詞

関係性は同じ
```

**Attention パターンの分析：**

```
英語と日本語で類似のAttentionパターン

英語: "The cat sat on the mat"
      注目: cat ← sat, mat ← on

日本語: "猫がマットの上に座った"
       注目: 猫 ← 座った, マット ← 上
       
→ 構造的類似性
```

### 13.2.3 ゼロショット多言語転移

**設定：**

言語 $L_1$ でタスクを訓練、言語 $L_2$ でテスト

**例：**

```
訓練: 英語の感情分析
  "This is great!" → Positive
  "This is terrible!" → Negative

テスト: 日本語（訓練データなし）
  "これは素晴らしい！" → Positive (正解！)
  "これはひどい！" → Negative (正解！)
```

**なぜ可能？**

1. **共有概念空間**
   - "great" と "素晴らしい" が近い

2. **構造の類似性**
   - 感情表現のパターンが類似

3. **多言語事前学習**
   - 言語間の対応を暗黙的に学習

**性能：**

```
転移性能（英→X）:
  高資源言語（仏、独、西）: 80-90%
  中資源言語（韓、アラビア）: 60-70%
  低資源言語（スワヒリ）: 40-50%
```

### 13.2.4 言語間の距離

**言語距離の測定：**

$$d(L_1, L_2) = \mathbb{E}_{\text{概念}}\left[\|\phi_{L_1}(w_1) - \phi_{L_2}(w_2)\|_2\right]$$

**言語系統樹との相関：**

```
言語系統的に近い → 埋め込み空間でも近い

ラテン系:
  スペイン語 ↔ イタリア語 (距離: 小)

ゲルマン系:
  英語 ↔ ドイツ語 (距離: 中)

異系統:
  英語 ↔ 日本語 (距離: 大)
```

**転移性能との関係：**

$$\text{転移性能} \propto \frac{1}{d(L_{\text{source}}, L_{\text{target}})}$$

近い言語ほど転移しやすい

---

## 13.3 Vision-Language モデル

### 13.3.1 画像とテキストの統合

**課題：**

異なるモダリティをどう統合？

```
画像: 連続値のピクセル配列（H×W×3）
テキスト: 離散トークンの系列
```

**解決策：**

両方を共通の埋め込み空間にマッピング

```
画像 → 画像エンコーダ → 埋め込み
                         ↓
                    共通空間
                         ↑
テキスト → テキストエンコーダ → 埋め込み
```

### 13.3.2 CLIPアーキテクチャ

**Contrastive Language-Image Pre-training (CLIP)**

**構造：**

```
画像                    テキスト
  ↓                       ↓
Vision Transformer    Text Transformer
  ↓                       ↓
画像埋め込み I          テキスト埋め込み T
  ↓                       ↓
     cos(I, T) = 類似度
```

**訓練目標（対照学習）：**

$$\mathcal{L}_{\text{CLIP}} = -\sum_{i} \log \frac{\exp(\text{sim}(I_i, T_i) / \tau)}{\sum_{j} \exp(\text{sim}(I_i, T_j) / \tau)}$$

- $(I_i, T_i)$：正のペア（対応する画像とテキスト）
- $\tau$：温度パラメータ

**直感：**

```
正のペア（画像と対応するキャプション）の類似度を最大化
負のペア（画像と無関係なキャプション）の類似度を最小化

画像1: 猫の写真
  ↓ 高類似度
テキスト1: "猫が座っている"
  
  ↓ 低類似度
テキスト2: "犬が走っている"
```

**ゼロショット分類：**

```
1. クラスラベルをテキスト化
   "a photo of a cat"
   "a photo of a dog"
   ...

2. 各テキストの埋め込みを計算

3. 画像との類似度を計算

4. 最大類似度のクラスを選択
```

**性能：**

ImageNetゼロショット精度：76%（訓練データなし！）

### 13.3.3 Vision-Language Transformerモデル

**統合アーキテクチャ：**

**例1：ViLT (Vision-and-Language Transformer)**

```
画像パッチ:
  画像 → 16×16パッチに分割 → 線形射影

テキストトークン:
  テキスト → トークン化 → 埋め込み

統合:
  [CLS] + 画像パッチ + [SEP] + テキストトークン
  ↓
  Transformer（共同処理）
  ↓
  出力
```

**例2：Flamingo**

```
Vision Encoder (Frozen)
  ↓
  画像特徴
  ↓
Cross-Attention
  ↑
Language Model
  ↑
  テキスト入力
```

**利点：**

- 画像とテキストの相互作用
- 複雑な視覚的推論

**タスク：**

- Visual Question Answering (VQA)
- 画像キャプション生成
- 視覚的推論

**例：**

```
入力:
  画像: [猫が木の上にいる写真]
  質問: "What is the cat doing?"

出力:
  "The cat is sitting on a tree branch."
```

### 13.3.4 Generative Vision-Language モデル

**DALL-E / Stable Diffusion系**

テキストから画像を生成

```
入力: "a cat wearing a hat in the style of Van Gogh"
  ↓
テキストエンコーダ
  ↓
拡散モデル / GAN
  ↓
出力: [生成された画像]
```

**GPT-4V (Vision)**

画像とテキストを統合処理

```
入力:
  画像: [数学の問題の写真]
  テキスト: "この問題を解いてください"

出力:
  問題を読み取り、ステップごとに解答
```

**マルチモーダル理解：**

- 画像の詳細な説明
- OCR（文字認識）
- グラフ・図表の解析
- シーン理解

---

## 13.4 マルチモーダル統合の理論

### 13.4.1 共通表現空間の学習

**目標：**

異なるモダリティを統一空間に埋め込む

$$\phi_{\text{vision}}: \mathbb{R}^{H \times W \times 3} \to \mathbb{R}^d$$
$$\phi_{\text{text}}: \mathcal{V}^* \to \mathbb{R}^d$$
$$\phi_{\text{audio}}: \mathbb{R}^T \to \mathbb{R}^d$$

全て同じ $d$ 次元空間

**アライメント：**

同じ意味内容は近くに

```
埋め込み空間

  画像（猫）●
           ╲
            ╲
             ●共通概念
            ╱
           ╱
  テキスト（"cat"）●
```

### 13.4.2 モダリティ融合戦略

**早期融合（Early Fusion）：**

```
画像特徴 ┐
        ├→ 結合 → Transformer
テキスト特徴 ┘

利点: 早期から相互作用
欠点: 計算コスト高
```

**後期融合（Late Fusion）：**

```
画像 → Vision Encoder → 画像埋め込み ┐
                                  ├→ 結合 → 出力
テキスト → Text Encoder → テキスト埋め込み ┘

利点: モダリティ独立処理、効率的
欠点: 相互作用が遅い
```

**中間融合（Cross-Modal Attention）：**

```
画像特徴
  ↓
  Query
  ↓
Cross-Attention ← Key, Value (テキスト特徴)
  ↓
融合特徴

利点: バランスが良い
欠点: 設計が複雑
```

### 13.4.3 情報理論的視点

**相互情報量最大化：**

$$\max I(X_{\text{image}}; X_{\text{text}})$$

画像とテキストの共有情報を最大化

**分解：**

$$I(X; Y) = H(X) - H(X|Y)$$

$Y$（テキスト）が与えられたとき、$X$（画像）の不確実性を減らす

**対照学習の理論：**

InfoNCE損失は相互情報量の下界

$$I(X; Y) \geq \log K - \mathcal{L}_{\text{InfoNCE}}$$

$K$：負例の数

### 13.4.4 モダリティギャップ

**観察：**

異なるモダリティの埋め込みに体系的なギャップ

```
埋め込み空間

画像埋め込み         テキスト埋め込み
  ●●●●●               ●●●●●
  ●●●●●    gap        ●●●●●
  ●●●●●  <----->      ●●●●●
```

**原因：**

1. **訓練の非対称性**
   - 画像エンコーダとテキストエンコーダの異なる初期化

2. **情報密度の違い**
   - 画像：密な情報
   - テキスト：疎な情報

3. **最適化の偏り**

**対策：**

1. **温度スケーリング**
   ```python
   similarity = cos(image_emb, text_emb) / temperature
   ```

2. **射影層の追加**
   ```
   image_emb → 線形射影 → 共通空間
   text_emb → 線形射影 → 共通空間
   ```

3. **正則化**
   $$\mathcal{L} = \mathcal{L}_{\text{contrastive}} + \lambda \|\mu_{\text{image}} - \mu_{\text{text}}\|^2$$

---

## 13.5 音声・その他のモダリティ

### 13.5.1 音声認識と生成

**Whisper（OpenAI）**

音声→テキスト

```
音声波形
  ↓
Log-Mel Spectrogram（音響特徴）
  ↓
Audio Encoder (Transformer)
  ↓
Decoder (Transformer)
  ↓
テキスト
```

**多言語対応：**

99言語に対応、ゼロショット翻訳も可能

```
入力: 日本語音声
出力: 英語テキスト（直接翻訳）
```

### 13.5.2 統合マルチモーダルモデル

**ImageBind（Meta）**

6つのモダリティを統一空間に

```
         画像
          ↓
    ┌─────┴─────┐
  テキスト      音声
    ↓           ↓
  ┌─┴─┐     ┌──┴──┐
深度   IMU   温度  動画

全て共通埋め込み空間
```

**ゼロショット転移：**

```
訓練: 画像-テキスト、音声-テキスト

テスト: 画像-音声（直接訓練なし）
  画像（犬） ←類似→ 音声（犬の鳴き声）
  
テキストを「橋渡し」として使用
```

### 13.5.3 モダリティの追加

**拡張可能なアーキテクチャ：**

```python
class MultiModalModel:
    def __init__(self):
        # 共通埋め込み空間
        self.embedding_dim = 512
        
        # モダリティごとのエンコーダ
        self.encoders = {
            'text': TextEncoder(self.embedding_dim),
            'image': VisionEncoder(self.embedding_dim),
            'audio': AudioEncoder(self.embedding_dim),
            # 新しいモダリティを追加可能
            'video': VideoEncoder(self.embedding_dim),
        }
    
    def encode(self, data, modality):
        return self.encoders[modality](data)
    
    def similarity(self, data1, mod1, data2, mod2):
        emb1 = self.encode(data1, mod1)
        emb2 = self.encode(data2, mod2)
        return cosine_similarity(emb1, emb2)
```

### 13.5.4 応用例

**マルチモーダル検索：**

```
クエリ: "夕日の画像"（テキスト）
  ↓
テキスト埋め込み
  ↓
画像データベースで類似検索
  ↓
結果: 夕日の写真
```

**クロスモーダル生成：**

```
入力: 音楽（音声）
  ↓
音声埋め込み
  ↓
DALL-E系モデル
  ↓
出力: 音楽に合った画像
```

**マルチモーダルQA：**

```
入力:
  画像: [料理の写真]
  音声: "これは何ですか？"

出力（テキスト）:
  "これはパスタです。トマトソースがかかっています。"
```

---

## 本章のまとめ

### 学んだこと

✅ **多言語モデル**
- 共有語彙（多言語BPE）
- 言語識別と切り替え
- 共有 vs 言語固有アーキテクチャ

✅ **言語間転移**
- 共有埋め込み空間
- 構造的アライメント
- ゼロショット多言語転移

✅ **Vision-Language モデル**
- CLIP（対照学習）
- ViLT（統合Transformer）
- GPT-4V（マルチモーダル理解）

✅ **マルチモーダル統合**
- 共通表現空間
- モダリティ融合戦略
- 情報理論的視点

✅ **拡張性**
- 音声、動画、その他
- ImageBind（6モダリティ）
- クロスモーダル応用

### 主要なアーキテクチャ

| モデル | モダリティ | 特徴 |
|--------|----------|------|
| CLIP | 画像+テキスト | 対照学習、ゼロショット |
| ViLT | 画像+テキスト | 統合Transformer |
| Flamingo | 画像+テキスト | Few-shot学習 |
| GPT-4V | 画像+テキスト | 統合理解・生成 |
| Whisper | 音声→テキスト | 多言語、ロバスト |
| ImageBind | 6モダリティ | 統一埋め込み |

### 多言語転移の鍵

```
共有埋め込み空間
  ↓
構造的類似性の学習
  ↓
概念レベルのマッピング
  ↓
ゼロショット転移
```

### マルチモーダル統合の原則

1. **共通空間**: 全モダリティを同じ空間に
2. **アライメント**: 対応する概念を近くに
3. **対照学習**: 正例と負例を区別
4. **相互作用**: Cross-Attentionで融合

### 次章の予告

第14章では、**効率化技術の理論**を学びます：
- モデル圧縮（量子化、プルーニング）
- パラメータ効率的ファインチューニング（LoRA、Adapter）
- 効率的推論（KVキャッシュ、投機的デコーディング）
- 分散学習と推論

実用的なLLMの効率化技術を理論的に理解していきます。

---

## 練習問題

### 問題1：語彙サイズ
100言語をカバーする多言語モデルで、各言語が平均30,000の固有語彙を持つとき、言語ごとに独立な語彙を使うと総サイズは？共有BPE（250,000トークン）と比較せよ。

### 問題2：CLIP損失
バッチサイズ32のCLIP訓練で、画像 $i$ とテキスト $i$ のペアが正例のとき、負例の数は？

### 問題3：言語距離
英語とフランス語の埋め込み距離が0.3、英語と日本語が0.8のとき、フランス語→英語のタスク転移性能が90%なら、日本語→英語では概算で何%と予想されるか？（距離に反比例と仮定）

### 問題4：モダリティ融合
画像特徴512次元、テキスト特徴512次元を早期融合（結合）する場合と、後期融合（平均）する場合、出力次元は？

### 解答

**問題1:**

独立語彙： $100 \times 30,000 = 3,000,000$ トークン

共有BPE： $250,000$ トークン

削減率： $\frac{3,000,000}{250,000} = 12$ 倍の削減！

**問題2:**

バッチサイズ32で、画像 $i$ に対して：
- 正例：テキスト $i$（1つ）
- 負例：テキスト $j$ ($j \neq i$)（31個）

負例の数：31

**問題3:**

距離と性能が反比例すると仮定：

$$\frac{\text{性能}_{\text{日→英}}}{\text{性能}_{\text{仏→英}}} = \frac{d_{\text{仏英}}}{d_{\text{日英}}} = \frac{0.3}{0.8} = 0.375$$

$$\text{性能}_{\text{日→英}} = 90\% \times 0.375 \approx 34\%$$

（実際にはこの単純な比例関係は近似的）

**問題4:**

早期融合（結合）：
$$\text{dim} = 512 + 512 = 1024$$

後期融合（平均）：
$$\text{dim} = 512$$

（両方とも512次元に射影することも可能）

---

**📖 前章：[第12章 創発的能力と汎化の理論](./第12章_創発的能力と汎化の理論.md)**  
**📖 次章：[第14章 効率化技術の理論](./第14章_効率化技術の理論.md)**
