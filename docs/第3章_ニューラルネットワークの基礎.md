# 第3章：ニューラルネットワークの基礎

この章では、LLMの基盤となる**ニューラルネットワーク**の理論を学びます。脳の神経細胞からヒントを得た単純なモデルが、どのように複雑な関数を学習できるのかを理解していきます。

---

## 3.1 パーセプトロンと線形分離可能性

### 3.1.1 パーセプトロンの誕生

**歴史：**

1958年、フランク・ローゼンブラットが**パーセプトロン**を発明。人工知能の最初の一歩。

**生物学的モチベーション：**

```
脳の神経細胞（ニューロン）
        樹状突起
         ↓ ↓ ↓  入力信号
         ●────→  細胞体で統合
        /|\       ↓
              軸索を通じて出力
              
複数の入力 → 統合 → 出力
```

### 3.1.2 パーセプトロンのモデル

**数学的定義：**

入力 $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ に対して：

$$y = \begin{cases}
1 & \text{if } \sum_{i=1}^{n} w_ix_i + b \geq 0 \\
0 & \text{otherwise}
\end{cases}$$

または、活性化関数を使って：

$$y = \sigma\left(\sum_{i=1}^{n} w_ix_i + b\right) = \sigma(\mathbf{w}^\top\mathbf{x} + b)$$

ここで：
- $\mathbf{x}$：入力ベクトル
- $\mathbf{w}$：重みベクトル（パラメータ）
- $b$：バイアス（パラメータ）
- $\sigma$：活性化関数（例：ステップ関数）

**視覚化（2入力の場合）：**

```
入力層        出力層
  x₁ ─w₁─┐
          ├─→ Σ ─→ σ ─→ y
  x₂ ─w₂─┘
    
   b ──────┘ (バイアス)
```

**直感的理解：**

> パーセプトロンは「重み付き投票機」  
> 各入力に重みを掛けて合計し、閾値を超えたら発火

### 3.1.3 パーセプトロンの幾何学的解釈

**2次元の例：**

$$w_1x_1 + w_2x_2 + b = 0$$

これは平面上の直線！

**視覚化：**

```
x₂
 ↑
 |  ○ ○ ○  (クラス1)
 |  ○ ○
 |─────────  決定境界: w₁x₁ + w₂x₂ + b = 0
 |    × ×
 |    × × ×  (クラス0)
 +──────────→ x₁
```

パーセプトロンは**線形分類器**：直線（または超平面）でクラスを分離

**3次元の場合：**

決定境界は平面になる

```
   x₃
    ↑
   /|\
  / | \  平面で2つのクラスを分離
 /  |  \
+───────→ x₂
  ↙
 x₁
```

### 3.1.4 線形分離可能性

**定義：**

データセットが**線形分離可能**であるとは、直線（超平面）で2つのクラスを完全に分離できること

**例1：ANDゲート（線形分離可能）**

| $x_1$ | $x_2$ | AND |
|-------|-------|-----|
| 0     | 0     | 0   |
| 0     | 1     | 0   |
| 1     | 0     | 0   |
| 1     | 1     | 1   |

**視覚化：**
```
x₂
1 | 0      1
  |    ╱
0 | 0 ╱    
  +──────
  0  1   x₁
  
決定境界: x₁ + x₂ - 1.5 = 0
```

重み： $w_1 = 1, w_2 = 1, b = -1.5$

**例2：XORゲート（線形分離不可能）**

| $x_1$ | $x_2$ | XOR |
|-------|-------|-----|
| 0     | 0     | 0   |
| 0     | 1     | 1   |
| 1     | 0     | 1   |
| 1     | 1     | 0   |

**視覚化：**
```
x₂
1 | 0      1
  |   ？
0 | 1      0
  +──────
  0  1   x₁
  
直線では分離不可能！
```

**パーセプトロンの限界：**

> 1層のパーセプトロンは、線形分離可能な問題しか解けない

この限界が1960-70年代のAI冬の時代を招いた。

### 3.1.5 パーセプトロン学習アルゴリズム

**目的：** 正しく分類できる重み $\mathbf{w}, b$ を見つける

**アルゴリズム：**

```
1. 重み w, バイアス b をランダムに初期化
2. 各訓練データ (x, t) に対して：
   a. 予測: y = σ(w^T x + b)
   b. 誤差: e = t - y
   c. 更新: 
      w ← w + η·e·x
      b ← b + η·e
3. すべて正しく分類できるまで繰り返し
```

ここで $\eta$ は学習率、 $t$ は正解ラベル

**収束定理（Rosenblatt, 1962）：**

> データが線形分離可能なら、パーセプトロン学習アルゴリズムは有限回で収束する

**具体例：**

訓練データ：
- $(x_1, x_2, t) = (0, 0, 0), (0, 1, 0), (1, 0, 0), (1, 1, 1)$（AND）

初期値： $w_1 = 0, w_2 = 0, b = 0$, $\eta = 0.1$

**更新の様子：**

```
エポック1:
(0,0,0): y=0, 正解 ✓
(0,1,0): y=0, 正解 ✓
(1,0,0): y=0, 正解 ✓
(1,1,1): y=0, 誤り ✗
  → w₁ = 0 + 0.1×1×1 = 0.1
  → w₂ = 0 + 0.1×1×1 = 0.1
  → b  = 0 + 0.1×1   = 0.1

エポック2以降で徐々に収束...
```

---

## 3.2 多層パーセプトロンと汎用近似定理

### 3.2.1 多層パーセプトロン（MLP）

**アイデア：**

> XOR問題を解くには、パーセプトロンを**重ねる**（層を増やす）

**2層ニューラルネットワーク：**

```
入力層   隠れ層   出力層
  x₁ ──┬─→ h₁ ──┐
       │         ├─→ y
  x₂ ──┴─→ h₂ ──┘
```

**数式：**

隠れ層：
$$h_j = \sigma\left(\sum_i w_{ij}^{(1)}x_i + b_j^{(1)}\right)$$

出力層：
$$y = \sigma\left(\sum_j w_j^{(2)}h_j + b^{(2)}\right)$$

**ベクトル形式：**

$$\mathbf{h} = \sigma(\mathbf{W}^{(1)}\mathbf{x} + \mathbf{b}^{(1)})$$
$$y = \sigma(\mathbf{w}^{(2)\top}\mathbf{h} + b^{(2)})$$

### 3.2.2 XOR問題の解決

**2層ニューラルネットワークでXORを実装：**

**構造：**
- 入力：2ノード $(x_1, x_2)$
- 隠れ層：2ノード
- 出力：1ノード

**重みの設定：**

隠れ層：
$$h_1 = \sigma(x_1 + x_2 - 0.5) \quad \text{（ORゲート）}$$
$$h_2 = \sigma(-2x_1 - 2x_2 + 3) \quad \text{（NANDゲート）}$$

出力層：
$$y = \sigma(h_1 + h_2 - 1.5) \quad \text{（ANDゲート）}$$

**真理値表：**

| $x_1$ | $x_2$ | $h_1$ (OR) | $h_2$ (NAND) | $y$ (AND) | XOR |
|-------|-------|------------|--------------|-----------|-----|
| 0     | 0     | 0          | 1            | 0         | 0   |
| 0     | 1     | 1          | 1            | 1         | 1   |
| 1     | 0     | 1          | 1            | 1         | 1   |
| 1     | 1     | 1          | 0            | 0         | 0   |

成功！ ✓

**幾何学的解釈：**

```
隠れ層で非線形変換
   ↓
特徴空間が歪む
   ↓
線形分離可能になる
```

### 3.2.3 汎用近似定理

**定理（Cybenko, 1989）：**

有限個のニューロンを持つ1層の隠れ層で、任意の連続関数を任意の精度で近似できる

**数学的表現：**

任意の連続関数 $f: [0,1]^n \to \mathbb{R}$ と $\epsilon > 0$ に対して、

$$g(\mathbf{x}) = \sum_{j=1}^{m} v_j\sigma\left(\sum_{i=1}^{n}w_{ji}x_i + b_j\right)$$

が存在して、

$$\sup_{\mathbf{x} \in [0,1]^n} |f(\mathbf{x}) - g(\mathbf{x})| < \epsilon$$

**直感的理解：**

> 「ニューラルネットワークは万能関数近似器」

**1次元での視覚化：**

目標関数 $f(x) = \sin(x)$ を近似

```
f(x) = sin(x)
    ╱‾‾╲
   ╱    ╲  ╱
  ╱      ╲╱

ReLUの組み合わせで近似:
  _╱‾╲_
 ╱    ╲_╱
```

各隠れニューロンが「折れ線」を作り、それらを足し合わせることで複雑な曲線を近似

**重要な注意点：**

1. **存在定理**：近似できることは保証するが、見つける方法は保証しない
2. **パラメータ数**：精度を上げるには多くのニューロンが必要
3. **深さの利点**：深いネットワークの方が効率的（後述）

### 3.2.4 深いネットワークの利点

**定理（Telgarsky, 2016）：**

特定の関数族に対して、深いネットワークは指数的に少ないパラメータで近似可能

**例：**

関数 $f(x) = x^{2^k}$ を近似

- 浅いネットワーク（1隠れ層）： $O(2^k)$ ニューロン必要
- 深いネットワーク（k層）： $O(k)$ ニューロンで十分

**直感的理解：**

```
浅いネットワーク：
すべてを一度に学習 → 非効率

深いネットワーク：
階層的に特徴を抽出 → 効率的

例（画像認識）:
層1: エッジ検出
層2: 形状検出  
層3: 部品検出
層4: 物体認識
```

**LLMでの応用：**

GPT-3: 96層、各層が異なるレベルの言語特徴を学習

---

## 3.3 活性化関数の性質

### 3.3.1 活性化関数の役割

**なぜ必要か：**

活性化関数がないと：

$$y = \mathbf{W}^{(2)}(\mathbf{W}^{(1)}\mathbf{x}) = \underbrace{(\mathbf{W}^{(2)}\mathbf{W}^{(1)})}_{\text{1つの行列}}\mathbf{x}$$

→ 何層重ねても線形変換のまま！

**活性化関数の役割：**

> 非線形性を導入し、複雑な関数を表現可能にする

### 3.3.2 主要な活性化関数

#### 1. シグモイド関数

**定義：**

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

**グラフ：**
```
σ(x)
1   |        ______
    |      /
0.5 |     /
    |   /
0   |__/___________
    -5  0  5      x
```

**性質：**
- 値域： $(0, 1)$
- 滑らか（微分可能）
- 出力を確率として解釈可能

**導関数：**

$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

計算が簡単！

**問題点：**
- **勾配消失**： $|x|$ が大きいと $\sigma'(x) \approx 0$
- 出力が0中心でない（後述）

#### 2. tanh（双曲線正接）

**定義：**

$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1$$

**グラフ：**
```
tanh(x)
 1  |        ______
    |      /
 0  |─────/────────
    |   /
-1  |__/___________
    -5  0  5      x
```

**性質：**
- 値域： $(-1, 1)$
- 0中心（シグモイドより良い）
- 依然として勾配消失の問題

**導関数：**

$$\tanh'(x) = 1 - \tanh^2(x)$$

#### 3. ReLU（Rectified Linear Unit）

**定義：**

$$\text{ReLU}(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\\ 0 & \text{otherwise} \end{cases}$$

**グラフ：**
```
ReLU(x)
    |    ╱
    |   ╱
    |  ╱
    | ╱
____|╱___________
    0           x
```

**性質：**
- 計算が超高速
- 勾配消失しない（正の領域）
- 生物学的妥当性（神経細胞の発火）

**導関数：**

$$\text{ReLU}'(x) = \begin{cases} 1 & \text{if } x > 0 \\\ 0 & \text{if } x \leq 0 \end{cases}$$

**利点：**
1. ✅ 計算効率
2. ✅ 勾配消失を軽減
3. ✅ スパース性（多くのニューロンが0）

**問題点：**
- **Dying ReLU**： $x < 0$ で勾配が0 → ニューロンが死ぬ

#### 4. Leaky ReLU

**定義：**

$$\text{Leaky ReLU}(x) = \max(\alpha x, x) = \begin{cases} x & \text{if } x > 0 \\\ \alpha x & \text{otherwise} \end{cases}$$

通常 $\alpha = 0.01$

**グラフ：**
```
Leaky ReLU
    |    ╱
    |   ╱
    |  ╱
    | ╱
___╱|___________
 ╱  0          x
```

**改善点：**
- 負の領域でも小さな勾配 → Dying ReLU問題を軽減

#### 5. GELU（Gaussian Error Linear Unit）

**定義：**

$$\text{GELU}(x) = x \cdot \Phi(x)$$

ここで $\Phi(x)$ は標準正規分布の累積分布関数

**近似式：**

$$\text{GELU}(x) \approx 0.5x\left(1 + \tanh\left(\sqrt{\frac{2}{\pi}}\left(x + 0.044715x^3\right)\right)\right)$$

**グラフ：**
```
GELU(x)
    |    ╱
    |   ╱ (ReLUより滑らか)
    |  ╱
    | ╱
    |╱___________
   ╱0           x
```

**性質：**
- ReLUとtanhの良いとこ取り
- 滑らかな非線形性
- **最新のLLMで標準的**（GPT、BERTなど）

**なぜLLMで人気：**
- 確率的な解釈（正規分布に基づく）
- 滑らかな勾配
- 経験的に性能が良い

### 3.3.3 活性化関数の比較

| 関数 | 値域 | 勾配消失 | 計算速度 | LLMでの使用 |
|------|------|----------|----------|-------------|
| Sigmoid | (0,1) | ✗ 強い | 遅い | × |
| tanh | (-1,1) | ✗ 強い | 遅い | △ |
| ReLU | [0,∞) | ✓ 軽減 | 速い | ○ |
| Leaky ReLU | (-∞,∞) | ✓ 軽減 | 速い | ○ |
| GELU | (-∞,∞) | ✓ 軽減 | 中程度 | ◎ |

**LLMでの典型的な選択：**
- **Transformer**：GELU（GPT、BERTなど）
- **一部のモデル**：Swish、SiLU（GELU類似）

---

## 3.4 バックプロパゲーションの導出

### 3.4.1 問題設定

**目的：** 損失関数 $L$ を最小化する重み $\mathbf{W}$ を見つける

**損失関数の例：**

$$L = \frac{1}{2}\sum_{i=1}^{N}(y_i - t_i)^2 \quad \text{（二乗誤差）}$$

**必要な計算：**

各重み $w_{jk}$ に関する勾配 $\frac{\partial L}{\partial w_{jk}}$

**問題：**

深いネットワークでは数百万〜数十億のパラメータ → 効率的な計算が必須

**解決策：バックプロパゲーション**

> 連鎖律を使って勾配を効率的に計算

### 3.4.2 連鎖律の復習

**1変数の連鎖律：**

$$\frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx}$$

**例：**

$$z = \sin(x^2)$$

$y = x^2$ とおくと $z = \sin(y)$

$$\frac{dz}{dx} = \cos(y) \cdot 2x = 2x\cos(x^2)$$

**多変数の連鎖律：**

$$\frac{\partial z}{\partial x} = \sum_i \frac{\partial z}{\partial y_i} \cdot \frac{\partial y_i}{\partial x}$$

### 3.4.3 計算グラフ

**例：簡単なニューラルネットワーク**

```
x ─→ [w₁] ─→ (a₁=w₁x) ─→ [σ] ─→ (h₁=σ(a₁)) ─→ [w₂] ─→ (a₂=w₂h₁) ─→ [σ] ─→ (y=σ(a₂)) ─→ [L] ─→ L
```

**順伝播（Forward Pass）：**

左から右へ値を計算

$$a_1 = w_1x, \quad h_1 = \sigma(a_1), \quad a_2 = w_2h_1, \quad y = \sigma(a_2), \quad L = \frac{1}{2}(y-t)^2$$

**逆伝播（Backward Pass）：**

右から左へ勾配を計算

$$\frac{\partial L}{\partial y} \to \frac{\partial L}{\partial a_2} \to \frac{\partial L}{\partial w_2} \to \frac{\partial L}{\partial h_1} \to \frac{\partial L}{\partial a_1} \to \frac{\partial L}{\partial w_1}$$

### 3.4.4 逆伝播の詳細計算

**ステップ1：出力層の勾配**

$$\frac{\partial L}{\partial y} = \frac{\partial}{\partial y}\left[\frac{1}{2}(y-t)^2\right] = y - t$$

**ステップ2：活性化関数の前**

$$\frac{\partial L}{\partial a_2} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial a_2} = (y-t) \cdot \sigma'(a_2)$$

**ステップ3：重み $w_2$ の勾配**

$$\frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial a_2} \cdot \frac{\partial a_2}{\partial w_2} = \frac{\partial L}{\partial a_2} \cdot h_1$$

**ステップ4：隠れ層への逆伝播**

$$\frac{\partial L}{\partial h_1} = \frac{\partial L}{\partial a_2} \cdot \frac{\partial a_2}{\partial h_1} = \frac{\partial L}{\partial a_2} \cdot w_2$$

**ステップ5：隠れ層の活性化**

$$\frac{\partial L}{\partial a_1} = \frac{\partial L}{\partial h_1} \cdot \frac{\partial h_1}{\partial a_1} = \frac{\partial L}{\partial h_1} \cdot \sigma'(a_1)$$

**ステップ6：重み $w_1$ の勾配**

$$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial a_1} \cdot \frac{\partial a_1}{\partial w_1} = \frac{\partial L}{\partial a_1} \cdot x$$

### 3.4.5 一般的な形式

**記号：**
- $l$：層のインデックス
- $a^{(l)}$：層 $l$ の活性化前の値
- $h^{(l)}$：層 $l$ の活性化後の値
- $\delta^{(l)} = \frac{\partial L}{\partial a^{(l)}}$：層 $l$ の誤差項

**逆伝播の公式：**

1. **出力層：**
   $$\delta^{(L)} = \frac{\partial L}{\partial a^{(L)}} = \nabla_y L \odot \sigma'(a^{(L)})$$

2. **隠れ層：**
   $$\delta^{(l)} = ((\mathbf{W}^{(l+1)})^\top \delta^{(l+1)}) \odot \sigma'(a^{(l)})$$

3. **重みの勾配：**
   $$\frac{\partial L}{\partial \mathbf{W}^{(l)}} = \delta^{(l)} (h^{(l-1)})^\top$$

4. **バイアスの勾配：**
   $$\frac{\partial L}{\partial \mathbf{b}^{(l)}} = \delta^{(l)}$$

ここで $\odot$ は要素ごとの積（Hadamard積）

**アルゴリズム：**

```
【バックプロパゲーション】

1. 順伝播：各層の a^(l), h^(l) を計算
2. 出力層の誤差：δ^(L) を計算
3. for l = L-1 down to 1:
     δ^(l) を計算（上の公式2）
     ∂L/∂W^(l), ∂L/∂b^(l) を計算（公式3,4）
```

**計算量：**

- 順伝播： $O(W)$（$W$ はパラメータ数）
- 逆伝播： $O(W)$

→ 勾配計算が順伝播と同じオーダー！（効率的）

### 3.4.6 具体例：2層ネットワーク

**設定：**
- 入力： $x = 2$
- 正解： $t = 1$
- 重み： $w_1 = 0.5, w_2 = 0.5$
- 活性化：シグモイド

**順伝播：**

$$a_1 = w_1 x = 0.5 \times 2 = 1$$
$$h_1 = \sigma(1) = \frac{1}{1+e^{-1}} \approx 0.731$$
$$a_2 = w_2 h_1 = 0.5 \times 0.731 = 0.366$$
$$y = \sigma(0.366) \approx 0.590$$
$$L = \frac{1}{2}(0.590 - 1)^2 = 0.084$$

**逆伝播：**

$$\frac{\partial L}{\partial y} = 0.590 - 1 = -0.410$$

$$\frac{\partial L}{\partial a_2} = -0.410 \times \sigma'(0.366) = -0.410 \times 0.590 \times 0.410 \approx -0.099$$

$$\frac{\partial L}{\partial w_2} = -0.099 \times 0.731 \approx -0.072$$

$$\frac{\partial L}{\partial h_1} = -0.099 \times 0.5 = -0.050$$

$$\frac{\partial L}{\partial a_1} = -0.050 \times \sigma'(1) \approx -0.050 \times 0.197 = -0.010$$

$$\frac{\partial L}{\partial w_1} = -0.010 \times 2 = -0.020$$

**更新（学習率 $\eta = 0.1$）：**

$$w_1 \leftarrow 0.5 - 0.1 \times (-0.020) = 0.502$$
$$w_2 \leftarrow 0.5 - 0.1 \times (-0.072) = 0.507$$

損失が減少する方向に重みが更新される！

---

## 3.5 勾配消失・爆発問題の理論的解析

### 3.5.1 勾配消失問題

**問題：**

深いネットワークで、初期層の勾配が極めて小さくなる現象

**数学的分析：**

L層のネットワークで、第1層の重みの勾配：

$$\frac{\partial L}{\partial \mathbf{W}^{(1)}} = \delta^{(1)}(h^{(0)})^\top$$

ここで

$$\delta^{(1)} = ((\mathbf{W}^{(2)})^\top \cdots (\mathbf{W}^{(L)})^\top \delta^{(L)}) \odot \sigma'(a^{(1)}) \odot \cdots$$

**連鎖積：**

$$\delta^{(1)} \propto \prod_{l=2}^{L} \mathbf{W}^{(l)} \prod_{l=1}^{L} \sigma'(a^{(l)})$$

**問題の発生：**

シグモイド関数の場合：

$$\sigma'(x) = \sigma(x)(1-\sigma(x)) \leq \frac{1}{4}$$

L層で：

$$|\delta^{(1)}| \lesssim \left(\frac{1}{4}\right)^L \to 0 \quad (L \to \infty)$$

**視覚化：**

```
層   勾配の大きさ
L    ████████████  1.0
L-1  ██████        0.5
L-2  ███           0.25
L-3  █             0.125
...
1    ·             ≈0 (消失！)
```

**影響：**

> 初期層が学習しない → 深いネットワークが訓練できない

### 3.5.2 勾配爆発問題

**問題：**

勾配が指数的に大きくなる現象

**発生条件：**

$$\|\mathbf{W}^{(l)}\| > 1 \quad \text{かつ} \quad \sigma'(a^{(l)}) \approx 1$$

このとき：

$$|\delta^{(1)}| \gtrsim \left(\|\mathbf{W}\|\right)^L \to \infty \quad (L \to \infty)$$

**視覚化：**

```
更新ステップでの重みの変化

正常:  w → w+Δw  (Δwが適切)

爆発:  w → w+巨大なΔw
          ↓
       パラメータが発散
          ↓
       NaN (数値オーバーフロー)
```

**症状：**
- 損失が NaN になる
- パラメータが異常な値
- 訓練が不安定

### 3.5.3 解決策

#### 1. 適切な活性化関数

**ReLUの使用：**

$$\text{ReLU}'(x) = \begin{cases} 1 & x > 0 \\\ 0 & x \leq 0 \end{cases}$$

- 正の領域で勾配が1 → 消失しない
- 多くのニューロンが0 → 事実上のネットワークが浅い

#### 2. 重みの初期化

**Xavier（Glorot）初期化：**

$$w_{ij} \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)$$

ここで $n_{\text{in}}, n_{\text{out}}$ は入出力ニューロン数

**目的：** 各層で分散を保つ

**He初期化（ReLU用）：**

$$w_{ij} \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)$$

**理論的背景：**

各層の出力の分散：

$$\text{Var}(a^{(l)}) = n_{\text{in}} \cdot \text{Var}(w) \cdot \text{Var}(h^{(l-1)})$$

初期化で $\text{Var}(w) = \frac{1}{n_{\text{in}}}$ とすると：

$$\text{Var}(a^{(l)}) = \text{Var}(h^{(l-1)})$$

→ 分散が保たれる！

#### 3. バッチ正規化

**アイデア：**

各層の出力を正規化して、分布を安定させる

$$\hat{h} = \frac{h - \mathbb{E}[h]}{\sqrt{\text{Var}(h) + \epsilon}}$$

**効果：**
- 勾配の流れを改善
- 学習率を大きくできる
- 正則化効果

#### 4. 残差接続（ResNet）

**アイデア：**

$$h^{(l+1)} = h^{(l)} + F(h^{(l)})$$

スキップ接続により、勾配が直接流れる経路を確保

**視覚化：**

```
h^(l) ──┬────────────┬→ h^(l+1)
        │            │
        └→ [層] → F ─┘ (加算)
```

**勾配の流れ：**

$$\frac{\partial L}{\partial h^{(l)}} = \frac{\partial L}{\partial h^{(l+1)}} \left(1 + \frac{\partial F}{\partial h^{(l)}}\right)$$

最低でも勾配が1倍で伝わる！

**LLMでの応用：**

Transformerの各サブ層で残差接続を使用

#### 5. 勾配クリッピング

**アイデア：**

勾配のノルムが閾値を超えたら、スケールダウン

$$\mathbf{g} \leftarrow \begin{cases}
\mathbf{g} & \text{if } \|\mathbf{g}\| \leq \theta \\
\frac{\theta}{\|\mathbf{g}\|}\mathbf{g} & \text{otherwise}
\end{cases}$$

**効果：**

勾配爆発を防ぐ

**LLMでの使用：**

ほぼすべてのLLM訓練で標準的に使用

---

## 本章のまとめ

### 学んだこと

✅ **パーセプトロン**
- 線形分離可能な問題のみ解ける
- XOR問題は解けない（限界）

✅ **多層パーセプトロン**
- 層を重ねることで非線形問題を解決
- 汎用近似定理：任意の関数を近似可能

✅ **活性化関数**
- シグモイド、tanh：勾配消失の問題
- ReLU：効率的、勾配消失を軽減
- GELU：LLMで標準的

✅ **バックプロパゲーション**
- 連鎖律で効率的に勾配計算
- 計算量は順伝播と同オーダー

✅ **勾配消失・爆発問題**
- 深いネットワークの課題
- 解決策：ReLU、適切な初期化、バッチ正規化、残差接続

### 重要な公式

| 概念 | 公式 |
|------|------|
| シグモイド | $\sigma(x) = \frac{1}{1+e^{-x}}$ |
| ReLU | $\text{ReLU}(x) = \max(0, x)$ |
| 順伝播 | $h^{(l)} = \sigma(\mathbf{W}^{(l)}h^{(l-1)} + \mathbf{b}^{(l)})$ |
| 誤差逆伝播 | $\delta^{(l)} = ((\mathbf{W}^{(l+1)})^\top\delta^{(l+1)}) \odot \sigma'(a^{(l)})$ |
| 重みの勾配 | $\frac{\partial L}{\partial \mathbf{W}^{(l)}} = \delta^{(l)}(h^{(l-1)})^\top$ |

### 次章の予告

第4章では、いよいよLLMの中核である**Transformerアーキテクチャ**を学びます：
- アテンション機構
- マルチヘッドアテンション
- 位置エンコーディング
- Layer Normalization

これまで学んだニューラルネットワークの基礎の上に、革新的なTransformerの仕組みを理解していきます。

---

## 練習問題

### 問題1：XORの実装
2層ニューラルネットワークでXORゲートを実装せよ。隠れ層に2ニューロン、活性化関数はステップ関数を使用。

### 問題2：活性化関数の微分
以下の活性化関数の導関数を求めよ：
1. $\text{ReLU}(x) = \max(0, x)$
2. $\text{Leaky ReLU}(x) = \max(0.01x, x)$

### 問題3：バックプロパゲーション
次のネットワークで $\frac{\partial L}{\partial w_1}$ を計算せよ：
- $a_1 = w_1x$, $h_1 = a_1$（恒等関数）
- $a_2 = w_2h_1$, $y = a_2$（恒等関数）
- $L = \frac{1}{2}(y-t)^2$

与えられた値： $x=2, t=5, w_1=1, w_2=1$

### 問題4：勾配消失
10層のネットワークで各層の活性化関数がシグモイド、 $\sigma'(a) = 0.25$ とする。第1層の勾配が第10層の勾配の何倍になるか計算せよ。

### 解答

**問題1:**

隠れ層：
- $h_1 = \text{step}(x_1 + x_2 - 1.5)$ （AND的）
- $h_2 = \text{step}(-x_1 - x_2 + 0.5)$ （NOR的）

出力層：
- $y = \text{step}(-h_1 + h_2 + 0.5)$

（他の解も存在）

**問題2:**
1. $\text{ReLU}'(x) = \begin{cases} 1 & x > 0 \\ 0 & x \leq 0 \end{cases}$

2. $\text{Leaky ReLU}'(x) = \begin{cases} 1 & x > 0 \\ 0.01 & x \leq 0 \end{cases}$

**問題3:**

順伝播： $a_1 = 2, h_1 = 2, a_2 = 2, y = 2, L = \frac{1}{2}(2-5)^2 = 4.5$

逆伝播：
$$\frac{\partial L}{\partial y} = 2 - 5 = -3$$
$$\frac{\partial L}{\partial a_2} = -3$$
$$\frac{\partial L}{\partial h_1} = -3 \times 1 = -3$$
$$\frac{\partial L}{\partial a_1} = -3$$
$$\frac{\partial L}{\partial w_1} = -3 \times 2 = -6$$

**問題4:**

$$\frac{\delta^{(1)}}{\delta^{(10)}} \approx (0.25)^9 \approx 3.8 \times 10^{-6}$$

約100万分の1に減衰！（勾配消失）

---

**📖 前章：[第2章 数学的基礎](./第2章_数学的基礎.md)**  
**📖 次章：[第4章 Transformerアーキテクチャの基礎](./第4章_Transformerアーキテクチャの基礎.md)**
