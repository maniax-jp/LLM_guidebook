# 第1章：序論

## 1.1 大規模言語モデル（LLM）とは

### LLMの定義

**大規模言語モデル（Large Language Model, LLM）** とは、膨大なテキストデータから学習した、人間の言語を理解し生成できる人工知能システムです。

簡単な比喩で説明しましょう：

> 📚 **図書館の例え**  
> LLMは、何千万冊もの本を読んで記憶した超優秀な図書館司書のようなものです。この司書は：
> - あらゆる質問に対して適切な回答を組み立てられる
> - 文章の続きを予測できる
> - 様々なスタイルで文章を書ける

### LLMの特徴

LLMには以下の3つの主要な特徴があります：

1. **大規模性（Scale）**
   - パラメータ数：数十億〜数千億個
   - 学習データ：インターネット上の膨大なテキスト

2. **汎用性（Generality）**
   - 特定のタスクだけでなく、様々な言語タスクに対応
   - 翻訳、要約、質問応答、コード生成など

3. **創発能力（Emergence）**
   - モデルが大きくなると、明示的に教えていない能力が突然現れる
   - 例：推論能力、数学的問題解決能力

### LLMの動作原理（概要）

```mermaid
graph LR
    A[入力テキスト] --> B[トークン化]
    B --> C[数値ベクトルに変換]
    C --> D[ニューラルネットワーク処理]
    D --> E[次の単語の確率分布]
    E --> F[出力テキスト]
```

**基本的な仕組み：**

LLMの核心は「**次の単語を予測する**」という単純なタスクです。

例えば：
- 入力：「今日の天気は」
- モデルの予測：「晴れ」「曇り」「雨」...（確率付き）

この単純なタスクを、膨大なデータで学習することで、言語の深い理解が生まれます。

### 数学的表現（入門）

LLMは確率モデルとして表現できます：

$$P(\text{文章}) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_1,w_2) \cdots P(w_n|w_1,\ldots,w_{n-1})$$

ここで：
- $w_i$ は $i$ 番目の単語（トークン）
- $P(w_i|w_1,\ldots,w_{i-1})$ は、これまでの単語が与えられたときの次の単語の確率

**直感的理解：**
- 各単語は、それまでの文脈を見て決定される
- 「天気」の後には「晴れ」「雨」が来やすい（高確率）
- 「天気」の後に「パソコン」は来にくい（低確率）

---

## 1.2 本書の目的と構成

### 本書の目的

本書は、LLMの理論的基礎を**初学者でも理解できる**形で提供することを目的としています。

**3つの学習目標：**

1. **理論の理解**
   - なぜLLMが動作するのか、数学的背景を理解する
   
2. **直感の獲得**
   - 複雑な概念を図解と比喩で直感的に把握する
   
3. **応用への橋渡し**
   - 理論と実践をつなぐ知識を得る

### 学習の進め方

```
【段階的学習アプローチ】

レベル1: 直感的理解
├─ 比喩と具体例
└─ 視覚的図解

レベル2: 数学的理解
├─ 基礎的な数式
└─ 簡単な証明

レベル3: 深い理解
├─ 厳密な定式化
└─ 理論的考察
```

### 本書の構成

本書は6部構成になっています：

#### **第I部：基礎理論**（第1章〜第3章）
- 数学的準備
- ニューラルネットワークの基礎
- 🎯 目標：LLMを理解するための土台作り

#### **第II部：アーキテクチャの理論**（第4章〜第6章）
- Transformerの仕組み
- 自己回帰モデル
- スケーリング法則
- 🎯 目標：LLMの構造を理解する

#### **第III部：学習アルゴリズムの理論**（第7章〜第9章）
- 事前学習
- ファインチューニング
- 人間フィードバックからの学習
- 🎯 目標：LLMの学習方法を理解する

#### **第IV部：推論と生成の理論**（第10章〜第12章）
- テキスト生成
- プロンプトエンジニアリング
- 創発能力
- 🎯 目標：LLMの使い方を理解する

#### **第V部：先進的トピックと応用**（第13章〜第15章）
- 多言語・マルチモーダル
- 効率化手法
- 解釈可能性
- 🎯 目標：最新の研究動向を知る

#### **第VI部：理論的限界と展望**（第16章〜第18章）
- 計算複雑性
- 認知科学との接点
- 未来の研究方向
- 🎯 目標：LLMの可能性と限界を考察する

---

## 1.3 必要な数学的予備知識

### 前提知識のチェックリスト

本書を読むために必要な最低限の数学知識：

#### ✅ **必須知識（高校レベル）**

1. **基礎的な代数**
   - 方程式の解法
   - 関数の概念
   - 指数・対数

2. **基礎的な微分**
   - 導関数の意味
   - 簡単な微分計算
   - 極値問題

3. **基礎的なベクトル**
   - ベクトルの加法・スカラー倍
   - 内積の計算

4. **基礎的な確率**
   - 確率の定義
   - 期待値の概念

#### 📚 **本書で学ぶ知識（大学レベル）**

これらは本書の中で丁寧に説明します：

1. **線形代数**
   - 行列演算
   - 固有値・固有ベクトル
   - ベクトル空間

2. **微分積分**
   - 多変数関数の微分
   - 偏微分
   - 勾配

3. **確率・統計**
   - 確率分布
   - ベイズの定理
   - 情報理論

4. **最適化理論**
   - 勾配降下法
   - 凸最適化

### 数学的直感の養い方

**🔑 重要な学習姿勢：**

> 数式は「厳密な言語」であり、概念を正確に伝えるツールです。  
> まずは**直感的理解**を優先し、その後で**数式による表現**を学びます。

#### 学習ステップの例

**概念：「内積」を理解する**

1. **直感的理解**
   ```
   2つのベクトルがどれくらい「同じ方向を向いているか」を測る
   ```

2. **視覚化**
   ```
   ↗  ↗   同じ方向 → 内積が大きい
   ↗  ↙   逆方向   → 内積が小さい（負）
   ↗  →   垂直     → 内積がゼロ
   ```

3. **数式表現**

   $$\mathbf{a} \cdot \mathbf{b} = |\mathbf{a}||\mathbf{b}|\cos\theta$$

4. **計算方法**

   $$\mathbf{a} \cdot \mathbf{b} = a_1b_1 + a_2b_2 + \cdots + a_nb_n$$

---

## 1.4 記号と表記法

### 基本的な数学記号

本書では以下の記号を使用します：

#### **スカラー（数値）**
- 小文字のイタリック： $x, y, z, a, b, c$
- ギリシャ文字： $\alpha, \beta, \gamma, \theta$

**例：** $x = 5$（xは5という数値）

#### **ベクトル（数値の列）**
- 小文字の太字： $\mathbf{x}, \mathbf{y}, \mathbf{v}, \mathbf{w}$

**例：** 

$$\mathbf{x} = \begin{bmatrix} 1 \\\ 2 \\\ 3 \end{bmatrix}$$

（3次元ベクトル）

**直感的理解：**
```
ベクトル = 複数の数値をまとめたもの
例：位置 = (x座標, y座標, z座標)
```

#### **行列（数値の表）**
- 大文字の太字： $\mathbf{A}, \mathbf{B}, \mathbf{W}, \mathbf{X}$

**例：** 

$$\mathbf{A} = \begin{bmatrix} 1 & 2 \\\ 3 & 4 \\\ 5 & 6 \end{bmatrix}$$

（3行2列の行列）

**直感的理解：**
```
行列 = 数値を格子状に並べたもの
例：画像 = ピクセル値の表
```

#### **集合**
- 大文字の黒板太字： $\mathbb{R}, \mathbb{N}, \mathbb{Z}$
- カリグラフィック体： $\mathcal{D}, \mathcal{L}, \mathcal{V}$

**例：**
- $\mathbb{R}$：実数全体の集合
- $\mathbb{R}^n$：n次元実数ベクトルの集合
- $\mathcal{D}$：データセット

### 演算記号

#### **基本演算**
- 加法： $a + b$
- 減法： $a - b$
- 乗法： $a \times b$ または $ab$
- 除法： $a / b$ または $\frac{a}{b}$

#### **ベクトル・行列演算**
- 内積： $\mathbf{a} \cdot \mathbf{b}$ または $\mathbf{a}^\top \mathbf{b}$
- 行列積： $\mathbf{AB}$
- 転置： $\mathbf{A}^\top$
- 逆行列： $\mathbf{A}^{-1}$

#### **総和と総積**
- 総和： $\sum_{i=1}^{n} x_i = x_1 + x_2 + \cdots + x_n$
- 総積： $\prod_{i=1}^{n} x_i = x_1 \times x_2 \times \cdots \times x_n$

**例：**

$$\sum_{i=1}^{3} i^2 = 1^2 + 2^2 + 3^2 = 1 + 4 + 9 = 14$$

### 関数記号

#### **一般的な関数**
- $f(x)$：xを入力とする関数f
- $f: X \to Y$：XからYへの関数（写像）

#### **特殊な関数**
- $\exp(x)$ または $e^x$：指数関数
- $\log(x)$：自然対数（底はe）
- $\log_2(x)$：底が2の対数
- $\sin(x), \cos(x)$：三角関数

#### **活性化関数（第3章で詳述）**
- $\sigma(x)$：シグモイド関数
- $\text{ReLU}(x)$：正規化線形関数
- $\text{softmax}(\mathbf{x})$：ソフトマックス関数

### 確率・統計記号

#### **確率**
- $P(A)$：事象Aの確率
- $P(A|B)$：Bが起きたときのAの条件付き確率
- $P(X=x)$：確率変数Xが値xを取る確率

#### **期待値と分散**
- $\mathbb{E}[X]$ または $\mathrm{E}[X]$：期待値（平均）
- $\text{Var}(X)$：分散
- $\mathbb{E}_{x \sim P}[f(x)]$：分布Pに従うxについてのf(x)の期待値

**例：**

$$\mathbb{E}[X] = \sum_{i} x_i P(X=x_i)$$

#### **分布**
- $X \sim \mathcal{N}(\mu, \sigma^2)$：XはN平均μ、分散σ²の正規分布に従う
- $X \sim \text{Uniform}(a, b)$：Xは区間[a,b]の一様分布に従う

### 微分記号

#### **1変数関数の微分**
- $\frac{df}{dx}$：fのxに関する微分
- $f'(x)$：fの導関数
- $\frac{d^2f}{dx^2}$：2階微分

#### **多変数関数の微分**
- $\frac{\partial f}{\partial x}$：fのxに関する偏微分
- $\nabla f$：fの勾配（グラディエント）ベクトル

**勾配の定義：**

$$\nabla f = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\\ \frac{\partial f}{\partial x_2} \\\ \vdots \\\ \frac{\partial f}{\partial x_n} \end{bmatrix}$$

**直感的理解：**
```
勾配 = 関数が最も急激に増加する方向を示すベクトル
登山で例えると、最も急な上り坂の方向
```

### その他の重要な記号

#### **近似と漸近**
- $a \approx b$：aはbとほぼ等しい
- $a \propto b$：aはbに比例する
- $O(n)$：オーダー記法（計算量の評価）

#### **ノルム**
- $\|\mathbf{x}\|$：ベクトルxのノルム（長さ）
- $\|\mathbf{x}\|_2$：L2ノルム（ユークリッドノルム）
- $\|\mathbf{x}\|_1$：L1ノルム（絶対値の和）

**L2ノルムの定義：**

$$\|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}$$

**直感的理解：**
```
ノルム = ベクトルの「長さ」
2次元なら、原点からの距離（ピタゴラスの定理）
```

### LLM特有の記号

#### **シーケンス（系列）**
- $(w_1, w_2, \ldots, w_n)$：単語の系列
- $w_{1:t}$：単語1からtまでの系列（ $w_1, w_2, \ldots, w_t$と同じ）

#### **埋め込み（Embedding）**
- $\mathbf{e}_i$：i番目の単語の埋め込みベクトル
- $\mathbf{E}$：埋め込み行列

#### **アテンション**
- $\mathbf{Q}, \mathbf{K}, \mathbf{V}$：Query, Key, Value行列
- $\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V})$：アテンション関数

---

## 表記法の実例

### 例1：ベクトルの内積

**問題：** 2つのベクトル 

$$\mathbf{a} = \begin{bmatrix} 1 \\\ 2 \end{bmatrix}, \mathbf{b} = \begin{bmatrix} 3 \\\ 4 \end{bmatrix}$$

 の内積を計算せよ。

**解答：**

$$\mathbf{a} \cdot \mathbf{b} = 1 \times 3 + 2 \times 4 = 3 + 8 = 11$$

### 例2：総和記号

**問題：** $\sum_{i=1}^{4} 2i$ を計算せよ。

**解答：**

$$\sum_{i=1}^{4} 2i = 2 \times 1 + 2 \times 2 + 2 \times 3 + 2 \times 4 = 2 + 4 + 6 + 8 = 20$$

### 例3：確率の条件付き

**問題：** サイコロを振って偶数が出たとき、それが4以上である確率は？

**解答：**
- $A$：4以上が出る事象 = {4, 5, 6}
- $B$：偶数が出る事象 = {2, 4, 6}
- $A \cap B$：4以上かつ偶数 = {4, 6}

$$P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{2/6}{3/6} = \frac{2}{3}$$

---

## 本章のまとめ

### 学んだこと

✅ **LLMとは何か**
- 膨大なテキストから学習した言語モデル
- 次の単語を予測することで言語を理解

✅ **本書の構成**
- 6部構成で基礎から応用まで
- 直感→数式→理論の段階的アプローチ

✅ **必要な予備知識**
- 高校レベルの数学が前提
- 大学レベルの内容は本書で学習

✅ **記号と表記法**
- スカラー、ベクトル、行列の記法
- 確率、微分、総和の記号
- LLM特有の記号

### 次章の予告

第2章では、LLMを理解するために必要な**数学的基礎**を学びます：
- 線形代数（ベクトルと行列）
- 確率論と統計学
- 最適化理論
- 関数解析の基礎

これらの数学的ツールを使って、LLMの仕組みを厳密に理解していきます。

---

## 練習問題

### 問題1：基本的な計算
次の計算をせよ：
1. $\sum_{i=1}^{5} i$
2. $\prod_{i=1}^{4} 2$

### 問題2：ベクトルの内積

$$\mathbf{x} = \begin{bmatrix} 2 \\\ -1 \\\ 3 \end{bmatrix}, \mathbf{y} = \begin{bmatrix} 1 \\\ 4 \\\ -2 \end{bmatrix}$$

 の内積を計算せよ。

### 問題3：確率の計算
コインを3回投げるとき、少なくとも1回表が出る確率を求めよ。

### 解答

**問題1**
1. $1 + 2 + 3 + 4 + 5 = 15$
2. $2 \times 2 \times 2 \times 2 = 16$

**問題2**

$$\mathbf{x} \cdot \mathbf{y} = 2 \times 1 + (-1) \times 4 + 3 \times (-2) = 2 - 4 - 6 = -8$$

**問題3**
全て裏が出る確率は $(1/2)^3 = 1/8$  
したがって、少なくとも1回表が出る確率は $1 - 1/8 = 7/8$

---

**📖 次章へ：[第2章 数学的基礎](./第2章_数学的基礎.md)**
