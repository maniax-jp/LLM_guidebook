# 第4章補足：トークナイズとベクトル化

この補足章では、LLMがテキストデータを学習する際の基盤となる**トークナイズ（Tokenization）**、**エンコード（Encoding）**、**ベクトル化（Vectorization）** の手法を詳しく学びます。

---

## 4A.1 トークナイズの基礎

### 4A.1.1 トークナイズとは

**定義：**

テキストを、モデルが処理できる小さな単位（トークン）に分割するプロセス

**例：**

```
テキスト: "Hello, world!"

文字レベル: ['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']

単語レベル: ['Hello', ',', 'world', '!']

サブワードレベル: ['Hello', ',', 'world', '!']
または: ['Hel', 'lo', ',', 'wor', 'ld', '!']
```

**なぜ必要？**

1. **固定サイズの語彙**
   - モデルは固定数のトークンを扱う
   - 無限の文字列を有限の空間で表現

2. **計算効率**
   - 文字レベル: 長いシーケンス → 計算コスト大
   - 単語レベル: 語彙が膨大 → メモリコスト大
   - サブワード: バランスの良い折衷案

3. **未知語への対応**
   - 新しい単語も既存のサブワードで表現可能

### 4A.1.2 トークナイズの種類

#### 1. 文字レベル（Character-level）

**方法：**

各文字を1つのトークンとして扱う

```
"cat" → ['c', 'a', 't']
"猫" → ['猫']
```

**利点：**
- 語彙サイズが小さい（アルファベット26文字 + 記号）
- 未知語が存在しない
- 実装が簡単

**欠点：**
- シーケンスが長くなる
- 単語の意味を直接捉えにくい
- 計算コストが高い

**適用例：**
- 文字生成タスク
- 古典的なRNN

#### 2. 単語レベル（Word-level）

**方法：**

単語を1つのトークンとして扱う

```
"I love cats" → ['I', 'love', 'cats']
```

**利点：**
- シーケンスが短い
- 単語の意味を直接表現
- 直感的

**欠点：**
- 語彙サイズが膨大（数十万〜数百万）
- 未知語問題（OOV: Out-of-Vocabulary）
- 形態的変化に弱い（"cat" と "cats" は別トークン）

**適用例：**
- 初期のNLPモデル
- 限定的なドメイン

#### 3. サブワードレベル（Subword-level）

**方法：**

単語を意味のある部分（サブワード）に分割

```
"unhappiness" → ['un', 'happiness']
または → ['un', 'happy', 'ness']

"tokenization" → ['token', 'ization']
```

**利点：**
- 語彙サイズが適度（3万〜5万程度）
- 未知語を既知のサブワードで表現可能
- 形態的情報を保持
- 計算効率とモデル性能のバランス

**欠点：**
- 学習にコーパスが必要
- 最適な分割が一意でない

**適用例：**
- 現代のLLM（GPT, BERT, LLaMAなど）
- 機械翻訳

---

## 4A.2 主要なトークナイズアルゴリズム

### 4A.2.1 Byte Pair Encoding (BPE)

**歴史：**

元々はデータ圧縮アルゴリズム（1994年）、NLPに応用（2016年）

**アルゴリズム：**

```
初期化: 各文字を独立したトークンとする

繰り返し:
  1. 隣接するトークンペアの頻度を数える
  2. 最も頻度の高いペアをマージ
  3. 新しいトークンとして語彙に追加
  4. 目標の語彙サイズに達するまで繰り返す
```

**具体例：**

```
コーパス: "low low low lower lowest"

初期: ['l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', ...]

ステップ1: 'l'+'o' が頻出 → マージ
  語彙: ['l', 'o', 'w', ' ', 'lo']
  コーパス: ['lo', 'w', ' ', 'lo', 'w', ' ', ...]

ステップ2: 'lo'+'w' が頻出 → マージ
  語彙: ['l', 'o', 'w', ' ', 'lo', 'low']
  コーパス: ['low', ' ', 'low', ' ', ...]

ステップ3: 'low'+'e' を処理...
```

**数式表現：**

語彙 $V$ を構築：

$$V_0 = \{\text{全文字}\}$$
$$V_{t+1} = V_t \cup \{\text{argmax}_{\text{pair}} \text{freq}(\text{pair}, V_t)\}$$

**特徴：**

- 決定論的
- コーパスに依存
- 頻出する文字列が優先的にトークン化

**使用例：**

- GPT-2, GPT-3
- RoBERTa

### 4A.2.2 WordPiece

**開発：**

Google（BERT, T5で使用）

**BPEとの違い：**

頻度ではなく、**尤度（Likelihood）** を最大化するペアをマージ

**アルゴリズム：**

```
初期化: 各文字を独立したトークンとする

繰り返し:
  1. 各ペアをマージした場合の尤度を計算
  2. 尤度を最も増加させるペアを選択
  3. マージして新しいトークンを作成
  4. 目標の語彙サイズに達するまで繰り返す
```

**尤度の計算：**

言語モデルの尤度を最大化：

$$\text{score}(x, y) = \frac{P(xy)}{P(x)P(y)}$$

高いスコア → 独立したトークンより一緒の方が良い

**特徴：**

- 統計的に最適
- サブワードの前に "##" を付ける（例: "playing" → ["play", "##ing"]）
- BPEより理論的に洗練

**使用例：**

- BERT
- T5
- ELECTRA

### 4A.2.3 Unigram Language Model

**開発：**

Google（2018年、SentencePieceで実装）

**アプローチ：**

BPE/WordPieceとは逆：大きな語彙から開始し、削減

**アルゴリズム：**

```
初期化: 大きな候補語彙を作成

繰り返し:
  1. 各トークンを削除した場合の尤度低下を計算
  2. 尤度への影響が小さいトークンを削除
  3. 目標の語彙サイズに達するまで繰り返す
```

**目的関数：**

コーパスの対数尤度を最大化：

$$\mathcal{L} = \sum_{i=1}^{|D|} \log P(X^{(i)})$$

各文の確率：

$$P(X) = \sum_{\mathbf{x} \in S(X)} P(\mathbf{x}) = \sum_{\mathbf{x} \in S(X)} \prod_{i=1}^{|\mathbf{x}|} P(x_i)$$

ここで：
- $S(X)$：文 $X$ の全ての可能な分割
- $\mathbf{x} = (x_1, \ldots, x_n)$：1つの分割

**最適な分割：**

Viterbiアルゴリズムで最尤分割を選択

**特徴：**

- 確率的モデル
- 複数の分割候補を考慮
- 理論的に最も洗練

**使用例：**

- T5 (SentencePiece)
- ALBERT
- XLNet

### 4A.2.4 SentencePiece

**特徴：**

言語非依存のトークナイザライブラリ

**主な利点：**

1. **前処理不要**
   - 空白文字も通常の文字として扱う
   - 言語固有の分かち書きルール不要

2. **可逆性**
   - トークン列から元のテキストを完全復元可能
   - 空白を "▁" (U+2581) で表現

3. **言語非依存**
   - 日本語、中国語、アラビア語など全言語対応

**例：**

```
テキスト: "Hello world"

通常のWordPiece: ["Hello", "world"]
→ 空白情報が失われる

SentencePiece: ["▁Hello", "▁world"]
→ 空白情報を保持
```

**対応アルゴリズム：**

- BPE
- Unigram

**使用例：**

- T5
- LLaMA
- mBART (多言語モデル)

---

## 4A.3 エンコーディング：トークンから整数へ

### 4A.3.1 語彙とトークンID

**語彙（Vocabulary）：**

全てのトークンの集合

```python
vocab = {
    "<pad>": 0,   # パディングトークン
    "<unk>": 1,   # 未知トークン
    "<s>": 2,     # 文開始トークン
    "</s>": 3,    # 文終了トークン
    "the": 4,
    "cat": 5,
    "sat": 6,
    "on": 7,
    "mat": 8,
    ...
}
```

**トークンID（Token ID）：**

各トークンに割り当てられた一意の整数

```
"the cat sat" 
  ↓ トークナイズ
["the", "cat", "sat"]
  ↓ エンコード
[4, 5, 6]
```

### 4A.3.2 特殊トークン

**主要な特殊トークン：**

| トークン | 用途 | 例 |
|---------|------|-----|
| `<pad>` | パディング | バッチ内の長さ揃え |
| `<unk>` | 未知語 | 語彙外の単語 |
| `<s>` または `<bos>` | 文開始 | Beginning of Sequence |
| `</s>` または `<eos>` | 文終了 | End of Sequence |
| `<cls>` | 分類トークン | BERT（文全体の表現） |
| `<sep>` | 区切り | BERT（文ペアの区切り） |
| `<mask>` | マスク | マスク言語モデリング |

**例（BERT）：**

```
文1: "I love cats"
文2: "They are cute"

トークン列:
[<cls>, I, love, cats, <sep>, They, are, cute, <sep>]

ID列:
[101, 1045, 2293, 8870, 102, 2027, 2024, 10140, 102]
```

### 4A.3.3 位置エンコーディングとの関係

**重要な区別：**

```
トークンエンコーディング（Token Encoding）:
  テキスト → トークンID
  "cat" → 5

位置エンコーディング（Positional Encoding）:
  トークンの位置情報
  位置0 → PE(0)
  位置1 → PE(1)
```

最終的な入力：

$$\text{Input} = \text{TokenEmbedding}(\text{ID}) + \text{PositionalEncoding}(\text{pos})$$

---

## 4A.4 ベクトル化：埋め込み表現

### 4A.4.1 埋め込み層（Embedding Layer）

**目的：**

離散的なトークンID → 連続的なベクトル表現

**数学的定義：**

埋め込み行列 $E \in \mathbb{R}^{|V| \times d}$

- $|V|$：語彙サイズ
- $d$：埋め込み次元

トークンID $i$ の埋め込み：

$$\mathbf{e}_i = E[i, :] \in \mathbb{R}^d$$

**例：**

```
語彙サイズ: 50,000
埋め込み次元: 768

埋め込み行列 E: (50000, 768)

トークンID = 5 ("cat")
  ↓
埋め込みベクトル: E[5] = [0.23, -0.45, 0.12, ..., 0.67]
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                  768次元
```

**実装（PyTorch）：**

```python
import torch
import torch.nn as nn

vocab_size = 50000
embedding_dim = 768

# 埋め込み層
embedding = nn.Embedding(vocab_size, embedding_dim)

# トークンID
token_ids = torch.tensor([4, 5, 6])  # [the, cat, sat]

# ベクトル化
embedded = embedding(token_ids)
# Shape: (3, 768)
```

### 4A.4.2 埋め込みの初期化

**ランダム初期化：**

$$E_{ij} \sim \mathcal{N}(0, \sigma^2)$$

通常、 $\sigma = 1/\sqrt{d}$

**事前学習済み埋め込み：**

Word2Vec、GloVeなどで事前学習

```python
# 例：GloVe埋め込みを使用
pretrained_embeddings = load_glove_embeddings()
embedding.weight.data.copy_(pretrained_embeddings)
```

**学習可能 vs 固定：**

```python
# 学習可能（通常）
embedding = nn.Embedding(vocab_size, embedding_dim)

# 固定
embedding = nn.Embedding(vocab_size, embedding_dim)
embedding.weight.requires_grad = False
```

### 4A.4.3 埋め込みの性質

**意味的類似性：**

類似した意味のトークンは近いベクトルを持つ

$$\text{similarity}(\text{cat}, \text{dog}) > \text{similarity}(\text{cat}, \text{car})$$

コサイン類似度：

$$\cos(\mathbf{e}_i, \mathbf{e}_j) = \frac{\mathbf{e}_i \cdot \mathbf{e}_j}{\|\mathbf{e}_i\| \|\mathbf{e}_j\|}$$

**ベクトル演算：**

有名な例：

$$\mathbf{e}_{\text{king}} - \mathbf{e}_{\text{man}} + \mathbf{e}_{\text{woman}} \approx \mathbf{e}_{\text{queen}}$$

### 4A.4.4 位置埋め込み

**問題：**

Transformerは順序情報を持たない

**解決策1：絶対位置埋め込み**

$$\mathbf{x}_i = \mathbf{e}_i + \mathbf{p}_i$$

ここで：
- $\mathbf{e}_i$：トークン埋め込み
- $\mathbf{p}_i$：位置埋め込み

**学習可能な位置埋め込み（BERT）：**

```python
position_embeddings = nn.Embedding(max_seq_length, embedding_dim)
```

**固定の正弦波位置埋め込み（元のTransformer）：**

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)$$
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)$$

**解決策2：相対位置埋め込み**

位置の差を考慮（T5, Transformer-XLなど）

**解決策3：回転位置埋め込み（RoPE）**

LLaMAなどで使用

$$\mathbf{x}_i' = \mathbf{R}(\theta_i) \mathbf{x}_i$$

---

## 4A.5 完全なパイプライン

### 4A.5.1 テキストからモデル入力まで

**ステップバイステップ：**

```
ステップ1: テキスト入力
  "The cat sat on the mat."

ステップ2: トークナイズ
  ["The", "cat", "sat", "on", "the", "mat", "."]

ステップ3: エンコード（トークンID化）
  [1996, 4937, 2938, 2006, 1996, 13523, 1012]

ステップ4: 特殊トークン追加
  [101, 1996, 4937, 2938, 2006, 1996, 13523, 1012, 102]
  [<cls>, The, cat, sat, on, the, mat, ., <sep>]

ステップ5: 埋め込み
  各ID → 768次元ベクトル
  Shape: (9, 768)

ステップ6: 位置エンコーディング追加
  各位置の埋め込みを加算
  Shape: (9, 768)

ステップ7: Transformerへ入力
```

**コード例（PyTorch + Transformers）：**

```python
from transformers import BertTokenizer, BertModel
import torch

# 1. トークナイザとモデルのロード
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# 2. テキスト入力
text = "The cat sat on the mat."

# 3-4. トークナイズとエンコード（自動で特殊トークン追加）
encoded = tokenizer(
    text,
    add_special_tokens=True,
    return_tensors='pt'
)

# 確認
print("Token IDs:", encoded['input_ids'])
print("Tokens:", tokenizer.convert_ids_to_tokens(encoded['input_ids'][0]))

# 5-7. モデルに入力（埋め込みと位置エンコーディングは内部で処理）
outputs = model(**encoded)
last_hidden_states = outputs.last_hidden_state
# Shape: (batch_size=1, seq_length=9, hidden_size=768)
```

### 4A.5.2 バッチ処理

**問題：**

異なる長さの文を同時に処理

```
文1: "I love cats"        → 長さ 3
文2: "They are very cute" → 長さ 4
```

**解決策：パディング**

```
文1: [101, 1045, 2293, 8870, 102, 0, 0]
                            ^^^^ パディング
文2: [101, 2027, 2024, 2200, 10140, 102, 0]
                                    ^^^^ パディング
```

**アテンションマスク：**

パディングトークンを無視

```python
attention_mask = [
    [1, 1, 1, 1, 1, 0, 0],  # 文1: 最初の5つのみ
    [1, 1, 1, 1, 1, 1, 0]   # 文2: 最初の6つのみ
]
```

**実装：**

```python
# バッチトークナイズ
texts = ["I love cats", "They are very cute"]
encoded = tokenizer(
    texts,
    padding=True,           # パディング
    truncation=True,        # 長すぎる場合は切る
    max_length=128,         # 最大長
    return_tensors='pt'
)

print("Input IDs shape:", encoded['input_ids'].shape)
# (batch_size=2, max_length)

print("Attention Mask shape:", encoded['attention_mask'].shape)
# (batch_size=2, max_length)
```

---

## 4A.6 実践的な考慮事項

### 4A.6.1 語彙サイズの選択

**トレードオフ：**

| 語彙サイズ | 利点 | 欠点 |
|-----------|------|------|
| 小（1万） | メモリ効率 | 長いシーケンス |
| 中（3-5万） | バランス | 標準的 |
| 大（10万+） | 短いシーケンス | メモリコスト大 |

**一般的な値：**

- GPT-2: 50,257
- BERT: 30,522
- T5: 32,000
- LLaMA: 32,000

### 4A.6.2 多言語対応

**課題：**

異なる言語、異なる語彙

**解決策1：言語ごとのトークナイザ**

```
英語用: BERTトークナイザ
日本語用: MeCab + WordPiece
中国語用: Jieba + BPE
```

**解決策2：多言語共通トークナイザ**

```
XLM-R, mBERT: 100言語対応
SentencePiece: 言語非依存
```

**例（日本語）：**

```python
# 日本語対応トークナイザ
from transformers import BertJapaneseTokenizer

tokenizer = BertJapaneseTokenizer.from_pretrained(
    'cl-tohoku/bert-base-japanese'
)

text = "私は猫が好きです"
tokens = tokenizer.tokenize(text)
# ['私', 'は', '猫', 'が', '好き', 'です']
```

### 4A.6.3 特殊なケース

**コード：**

```python
# コード専用トークナイザ（CodeBERT, CodeGen）
code = "def hello(): print('Hello')"
tokens = tokenizer.tokenize(code)
# ['def', 'hello', '(', ')', ':', 'print', '(', "'", 'Hello', "'", ')']
```

**数式：**

```python
# 数式トークナイズ
equation = "x^2 + 2x + 1 = 0"
# 特殊な処理が必要
```

**マルチモーダル：**

画像トークン + テキストトークン

```
[<img>, <img>, ..., <img>, The, cat, is, cute]
 ^^^^^^^^^^^^^^^^^^^^^^^^  ^^^^^^^^^^^^^^^^^^^
    画像パッチ                テキスト
```

---

## 4A.7 最新のトークナイズ手法

### 4A.7.1 Byte-level BPE (BBPE)

**アイデア：**

文字ではなく**バイト**を基本単位に

**利点：**

- 全ての言語・記号に対応
- 固定された256種類のバイトから開始
- 未知文字が存在しない

**使用例：**

- GPT-2, GPT-3, GPT-4
- RoBERTa

**実装：**

```python
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# 絵文字、特殊記号も処理可能
text = "Hello 🌍! ¿Cómo estás?"
tokens = tokenizer.tokenize(text)
```

### 4A.7.2 動的トークナイゼーション

**通常：**

固定されたトークナイザ

**新しいアプローチ：**

タスクやドメインに応じて適応

**例：**

医療ドメイン → 医療用語を優先的にトークン化

### 4A.7.3 トークンフリーモデル

**研究方向：**

トークナイズを完全にスキップ

**例：**

- **ByT5**: バイト直接処理
- **CANINE**: 文字レベルTransformer

**利点：**

- トークナイゼーションのバイアス除去
- 言語非依存性

**欠点：**

- シーケンスが長い
- 計算コスト大

---

## 本章のまとめ

### 学んだこと

✅ **トークナイズ**
- 文字レベル、単語レベル、サブワードレベル
- BPE, WordPiece, Unigram, SentencePiece
- 現代のLLMはサブワードレベルが主流

✅ **エンコーディング**
- トークン → トークンID
- 語彙と特殊トークン
- バッチ処理とパディング

✅ **ベクトル化**
- 埋め込み層
- トークン埋め込み + 位置埋め込み
- 意味的類似性の獲得

✅ **完全なパイプライン**
- テキスト → トークン → ID → ベクトル → Transformer
- 実装例とベストプラクティス

### 重要な公式

**BPEマージ：**
$$V_{t+1} = V_t \cup \{\text{argmax}_{\text{pair}} \text{freq}(\text{pair}, V_t)\}$$

**埋め込み：**
$$\mathbf{e}_i = E[i, :] \in \mathbb{R}^d$$

**位置エンコーディング（正弦波）：**
$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)$$

**最終入力：**
$$\text{Input} = \text{TokenEmbed}(\text{ID}) + \text{PosEmbed}(\text{pos})$$

### 実装のポイント

**トークナイザ選択：**

| モデル | トークナイザ | 語彙サイズ |
|--------|------------|-----------|
| BERT | WordPiece | 30,522 |
| GPT-2/3 | Byte-level BPE | 50,257 |
| T5 | SentencePiece (Unigram) | 32,000 |
| LLaMA | SentencePiece (BPE) | 32,000 |

**ベストプラクティス：**
- 事前学習済みトークナイザを使用
- 語彙サイズは3-5万が一般的
- バッチ処理時は必ずパディングとマスク
- 多言語対応にはSentencePiece

### 次章との接続

第4章本編の**Transformerアーキテクチャ**に戻ると：
- この章で得たベクトル表現が入力
- Self-Attentionで文脈を考慮
- 最終的に次トークン予測や分類へ

---

## 練習問題

### 問題1：トークナイズ
文 "unhappiness" をBPEでトークナイズ。語彙に "un", "happy", "ness" が存在する場合の結果は？

### 問題2：語彙サイズ
語彙サイズ50,000、埋め込み次元768の埋め込み層のパラメータ数は？

### 問題3：位置エンコーディング
位置0、次元0の正弦波位置エンコーディングの値は？（$d=512$）

### 問題4：バッチ処理
長さ3と長さ5の文をバッチ処理する場合、パディング後の形状は？（最大長に合わせる）

### 解答

**問題1:**
```
"unhappiness" → ["un", "happiness"]
または → ["un", "happy", "ness"]
```
BPEのマージ順序に依存

**問題2:**
$$\text{パラメータ数} = 50000 \times 768 = 38,400,000$$

約3840万パラメータ

**問題3:**
$$PE_{(0, 0)} = \sin\left(\frac{0}{10000^{0/512}}\right) = \sin(0) = 0$$

**問題4:**
```
形状: (batch_size=2, max_length=5)

[
  [id1, id2, id3, <pad>, <pad>],
  [id1, id2, id3, id4, id5]
]
```

---

**📖 本章：[第4章 Transformerアーキテクチャの基礎](./第4章_Transformerアーキテクチャの基礎.md)**  
**📖 次章：[第5章 自己回帰言語モデルの理論](./第5章_自己回帰言語モデルの理論.md)**
