# 第2章：数学的基礎

LLMを理解するための数学的ツールを学びます。この章では、複雑な概念を**低次元の例**と**図解**で直感的に理解してから、一般的な数式へと進みます。

---

## 2.1 線形代数の復習

### 2.1.1 ベクトル空間とノルム

#### ベクトルとは何か

**直感的理解：**

> ベクトルとは「向きと大きさを持つ量」です。  
> 日常の例：「北に3km、東に4km」という移動

**数学的定義：**

ベクトルは数値の順序付きリストです：

$$\mathbf{v} = \begin{bmatrix} v_1 \\\ v_2 \\\ \vdots \\\ v_n \end{bmatrix}$$

**例（2次元）：**
```
y軸
↑
|   ● (3, 4)
|  /|
| / |
|/__|___→ x軸
O
```

ベクトル 

$$\mathbf{v} = \begin{bmatrix} 3 \\\ 4 \end{bmatrix}$$

 は原点から点(3,4)への矢印

#### ベクトル演算の視覚化

**1. ベクトルの加法**

$$\mathbf{a} + \mathbf{b} = \begin{bmatrix} a_1 \\\ a_2 \end{bmatrix} + \begin{bmatrix} b_1 \\\ b_2 \end{bmatrix} = \begin{bmatrix} a_1 + b_1 \\\ a_2 + b_2 \end{bmatrix}$$

**視覚化：**
```
    b
   ↗
  /
 / a+b
a ⟋
↗
```
ベクトルaの終点からベクトルbを描く → 合成ベクトルa+b

**2. スカラー倍**

$$c\mathbf{v} = c \begin{bmatrix} v_1 \\\ v_2 \end{bmatrix} = \begin{bmatrix} cv_1 \\\ cv_2 \end{bmatrix}$$

**視覚化：**
```
c=2の場合：
  2v (同じ向き、2倍の長さ)
  ↗↗
 
v
↗
```

#### ベクトル空間

**定義：**

ベクトル空間 $V$ とは、ベクトルの集合で以下が成り立つもの：

1. **加法で閉じている**： $\mathbf{u}, \mathbf{v} \in V \Rightarrow \mathbf{u} + \mathbf{v} \in V$
2. **スカラー倍で閉じている**： $\mathbf{v} \in V, c \in \mathbb{R} \Rightarrow c\mathbf{v} \in V$

**直感的理解：**

> ベクトル空間は「ベクトルの遊び場」  
> どんなベクトルを組み合わせても、その空間内に留まる

**例：** $\mathbb{R}^2$（2次元平面全体）
- 平面上のどんな2つのベクトルを足しても、結果は平面上
- どんなベクトルを何倍しても、結果は平面上

#### ノルム（長さ）

**定義：**

ベクトルのノルム $\|\mathbf{v}\|$ は、ベクトルの「長さ」を表す非負の値

**主なノルム：**

**1. L2ノルム（ユークリッドノルム）**

$$\|\mathbf{v}\|_2 = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}$$

**2次元の例：**

$$\mathbf{v} = \begin{bmatrix} 3 \\\ 4 \end{bmatrix}, \quad \|\mathbf{v}\|_2 = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = 5$$

これはピタゴラスの定理そのもの！

**2. L1ノルム（マンハッタン距離）**

$$\|\mathbf{v}\|_1 = |v_1| + |v_2| + \cdots + |v_n|$$

**直感的理解：**
```
マンハッタンの街を移動する距離
（斜めに行けず、縦横にしか移動できない）

目的地
  ●
  |
  |
  |_____ スタート
      ●
```

$$\mathbf{v} = \begin{bmatrix} 3 \\\ 4 \end{bmatrix}, \quad \|\mathbf{v}\|_1 = |3| + |4| = 7$$

**3. L∞ノルム（最大ノルム）**

$$\|\mathbf{v}\|_\infty = \max(|v_1|, |v_2|, \ldots, |v_n|)$$

$$\mathbf{v} = \begin{bmatrix} 3 \\\ 4 \end{bmatrix}, \quad \|\mathbf{v}\|_\infty = \max(3, 4) = 4$$

#### 単位ベクトル（正規化）

**定義：**

長さが1のベクトルを**単位ベクトル**という

**正規化の公式：**

$$\hat{\mathbf{v}} = \frac{\mathbf{v}}{\|\mathbf{v}\|}$$

**例：**

$$\mathbf{v} = \begin{bmatrix} 3 \\\ 4 \end{bmatrix}, \quad \hat{\mathbf{v}} = \frac{1}{5}\begin{bmatrix} 3 \\\ 4 \end{bmatrix} = \begin{bmatrix} 0.6 \\\ 0.8 \end{bmatrix}$$

検証： $\|\hat{\mathbf{v}}\|_2 = \sqrt{0.6^2 + 0.8^2} = \sqrt{0.36 + 0.64} = 1$ ✓

**LLMでの応用：**
- 単語の埋め込みベクトルを正規化
- コサイン類似度の計算

#### 内積（ドット積）

**定義：**

$$\mathbf{a} \cdot \mathbf{b} = a_1b_1 + a_2b_2 + \cdots + a_nb_n = \sum_{i=1}^{n} a_ib_i$$

**幾何学的意味：**

$$\mathbf{a} \cdot \mathbf{b} = \|\mathbf{a}\| \|\mathbf{b}\| \cos\theta$$

ここで $\theta$ は2つのベクトルのなす角

**直感的理解：**

```
θ = 0° (同じ方向)
→ → → → cos(0°) = 1 → 内積が最大

θ = 90° (垂直)
↑
→     cos(90°) = 0 → 内積がゼロ

θ = 180° (反対方向)
→ ← ← ← cos(180°) = -1 → 内積が最小（負）
```

**重要な性質：**

1. **対称性**： $\mathbf{a} \cdot \mathbf{b} = \mathbf{b} \cdot \mathbf{a}$
2. **線形性**： $\mathbf{a} \cdot (c\mathbf{b} + \mathbf{d}) = c(\mathbf{a} \cdot \mathbf{b}) + \mathbf{a} \cdot \mathbf{d}$
3. **正定値性**： $\mathbf{a} \cdot \mathbf{a} = \|\mathbf{a}\|^2 \geq 0$

**例題：**

$$\mathbf{a} = \begin{bmatrix} 1 \\\ 2 \end{bmatrix}, \quad \mathbf{b} = \begin{bmatrix} 3 \\\ -1 \end{bmatrix}$$

内積： $\mathbf{a} \cdot \mathbf{b} = 1 \times 3 + 2 \times (-1) = 3 - 2 = 1$

角度：

$$\cos\theta = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|} = \frac{1}{\sqrt{5} \cdot \sqrt{10}} = \frac{1}{\sqrt{50}} \approx 0.141$$

$$\theta \approx 81.87°$$

**LLMでの応用：**
- Transformerのアテンション機構（Query と Key の内積）
- 単語間の類似度計算

#### コサイン類似度

**定義：**

$$\text{similarity}(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|} = \cos\theta$$

**値の範囲：** $[-1, 1]$
- $1$：完全に同じ方向
- $0$：直交（無関係）
- $-1$：完全に反対方向

**例（単語の類似度）：**

```
"king"のベクトル = [0.5, 0.8, 0.2, ...]
"queen"のベクトル = [0.6, 0.7, 0.3, ...]
"apple"のベクトル = [-0.2, 0.1, 0.9, ...]

cos(king, queen) = 0.92 (高い類似度)
cos(king, apple) = 0.15 (低い類似度)
```

---

### 2.1.2 行列演算と固有値分解

#### 行列とは何か

**直感的理解：**

> 行列は「ベクトルを別のベクトルに変換する機械」

**例（2次元）：**

$$\mathbf{A} = \begin{bmatrix} 2 & 0 \\\ 0 & 3 \end{bmatrix}, \quad \mathbf{v} = \begin{bmatrix} 1 \\\ 1 \end{bmatrix}$$

$$\mathbf{Av} = \begin{bmatrix} 2 & 0 \\\ 0 & 3 \end{bmatrix} \begin{bmatrix} 1 \\\ 1 \end{bmatrix} = \begin{bmatrix} 2 \\\ 3 \end{bmatrix}$$

**視覚化：**
```
変換前：     変換後：
  v           Av
  ↗            ↑
 (1,1)       (2,3)
```

この行列は、x方向に2倍、y方向に3倍に拡大する変換

#### 行列の積

**定義：**

行列 $\mathbf{A}$ (m×n) と $\mathbf{B}$ (n×p) の積 $\mathbf{C} = \mathbf{AB}$ は m×p 行列で：

$$C_{ij} = \sum_{k=1}^{n} A_{ik}B_{kj}$$

**直感的理解：**

> 「変換を2回続けて行う」  
> B で変換してから A で変換 = AB で変換

**計算例（2×2行列）：**

$$\begin{bmatrix} 1 & 2 \\\ 3 & 4 \end{bmatrix} \begin{bmatrix} 5 & 6 \\\ 7 & 8 \end{bmatrix} = \begin{bmatrix} 1 \cdot 5 + 2 \cdot 7 & 1 \cdot 6 + 2 \cdot 8 \\\ 3 \cdot 5 + 4 \cdot 7 & 3 \cdot 6 + 4 \cdot 8 \end{bmatrix} = \begin{bmatrix} 19 & 22 \\\ 43 & 50 \end{bmatrix}$$

**重要な性質：**

1. **非可換**：一般に $\mathbf{AB} \neq \mathbf{BA}$
2. **結合律**： $(\mathbf{AB})\mathbf{C} = \mathbf{A}(\mathbf{BC})$
3. **分配律**： $\mathbf{A}(\mathbf{B} + \mathbf{C}) = \mathbf{AB} + \mathbf{AC}$

#### 転置行列

**定義：**

行列 $\mathbf{A}$ の転置 $\mathbf{A}^\top$ は、行と列を入れ替えたもの：

$$(\mathbf{A}^\top)_{ij} = A_{ji}$$

**例：**

$$\mathbf{A} = \begin{bmatrix} 1 & 2 & 3 \\\ 4 & 5 & 6 \end{bmatrix}, \quad \mathbf{A}^\top = \begin{bmatrix} 1 & 4 \\\ 2 & 5 \\\ 3 & 6 \end{bmatrix}$$

**性質：**
- $(\mathbf{A}^\top)^\top = \mathbf{A}$
- $(\mathbf{AB})^\top = \mathbf{B}^\top\mathbf{A}^\top$

#### 単位行列と逆行列

**単位行列：**

対角成分が1、他が0の行列

$$\mathbf{I} = \begin{bmatrix} 1 & 0 & 0 \\\ 0 & 1 & 0 \\\ 0 & 0 & 1 \end{bmatrix}$$

**性質：** $\mathbf{AI} = \mathbf{IA} = \mathbf{A}$（何も変換しない）

**逆行列：**

$$\mathbf{AA}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}$$

を満たす $\mathbf{A}^{-1}$ を $\mathbf{A}$ の逆行列という

**直感的理解：**

> 逆行列は「変換を元に戻す操作」

**2×2行列の逆行列：**

$$\mathbf{A} = \begin{bmatrix} a & b \\\ c & d \end{bmatrix}, \quad \mathbf{A}^{-1} = \frac{1}{ad-bc}\begin{bmatrix} d & -b \\\ -c & a \end{bmatrix}$$

ただし $ad - bc \neq 0$（これを行列式という）

#### 固有値と固有ベクトル

**定義：**

行列 $\mathbf{A}$ に対して、

$$\mathbf{Av} = \lambda \mathbf{v}$$

を満たす非ゼロベクトル $\mathbf{v}$ を**固有ベクトル**、スカラー $\lambda$ を**固有値**という

**直感的理解：**

> 固有ベクトルは「行列変換で向きが変わらないベクトル」  
> 固有値は「その方向への拡大率」

**視覚化（2次元の例）：**

$$\mathbf{A} = \begin{bmatrix} 3 & 1 \\\ 0 & 2 \end{bmatrix}$$

固有値： $\lambda_1 = 3, \lambda_2 = 2$

固有ベクトル： 

$$\mathbf{v}_1 = \begin{bmatrix} 1 \\\ 0 \end{bmatrix}, \mathbf{v}_2 = \begin{bmatrix} 1 \\\ -1 \end{bmatrix}$$


```
v1方向に3倍、v2方向に2倍に拡大される
```

**計算方法：**

1. 特性方程式を解く： $\det(\mathbf{A} - \lambda\mathbf{I}) = 0$
2. 各固有値 $\lambda$ に対して $(\mathbf{A} - \lambda\mathbf{I})\mathbf{v} = \mathbf{0}$ を解く

**例題：**

$$\mathbf{A} = \begin{bmatrix} 4 & 2 \\\ 1 & 3 \end{bmatrix}$$

特性方程式：

$$\det\begin{bmatrix} 4-\lambda & 2 \\\ 1 & 3-\lambda \end{bmatrix} = (4-\lambda)(3-\lambda) - 2 = \lambda^2 - 7\lambda + 10 = 0$$

解： $\lambda_1 = 5, \lambda_2 = 2$

$\lambda_1 = 5$ に対する固有ベクトル：

$$\begin{bmatrix} -1 & 2 \\\ 1 & -2 \end{bmatrix}\begin{bmatrix} v_1 \\\ v_2 \end{bmatrix} = \begin{bmatrix} 0 \\\ 0 \end{bmatrix} \Rightarrow \mathbf{v}_1 = \begin{bmatrix} 2 \\\ 1 \end{bmatrix}$$

#### 固有値分解（対角化）

対称行列 $\mathbf{A}$ は以下のように分解できる：

$$\mathbf{A} = \mathbf{Q\Lambda Q}^\top$$

ここで：
- $\mathbf{Q}$：固有ベクトルを列に並べた直交行列
- $\mathbf{\Lambda}$：固有値を対角に並べた対角行列

**意味：**

```
任意のベクトル変換は、
1. 回転（Q^T）
2. 軸方向の拡大縮小（Λ）
3. 逆回転（Q）
の3ステップに分解できる
```

**LLMでの応用：**
- 共分散行列の分析
- 主成分分析（PCA）
- アテンション重みの解析

---

### 2.1.3 特異値分解（SVD）

#### 特異値分解とは

**定義：**

任意の m×n 行列 $\mathbf{A}$ は以下のように分解できる：

$$\mathbf{A} = \mathbf{U\Sigma V}^\top$$

ここで：
- $\mathbf{U}$：m×m 直交行列（左特異ベクトル）
- $\mathbf{\Sigma}$：m×n 対角行列（特異値）
- $\mathbf{V}$：n×n 直交行列（右特異ベクトル）

**直感的理解：**

> SVDは「任意の線形変換を3つの簡単な変換の組み合わせに分解する」

```
元の変換A = 回転(V^T) × 拡大縮小(Σ) × 回転(U)
```

#### 簡単な例（2×2行列）

$$\mathbf{A} = \begin{bmatrix} 3 & 2 \\\ 2 & 3 \end{bmatrix}$$

SVD：

$$\mathbf{A} = \begin{bmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\\ \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{bmatrix} \begin{bmatrix} 5 & 0 \\\ 0 & 1 \end{bmatrix} \begin{bmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\\ \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{bmatrix}$$

**視覚化：**

```
ステップ1 (V^T): 45度回転
ステップ2 (Σ): ある方向に5倍、別方向に1倍
ステップ3 (U): 45度回転
```

#### 低ランク近似

SVDの重要な応用：**データの次元削減**

特異値を大きい順に並べ、小さいものを無視すると：

$$\mathbf{A} \approx \mathbf{U}_k \mathbf{\Sigma}_k \mathbf{V}_k^\top$$

ここで $k$ は残す特異値の数

**例（画像圧縮）：**

```
元の画像: 100×100 = 10,000ピクセル
↓ SVDで上位10個の特異値のみ使用
圧縮画像: (100×10) + (10×10) + (10×100) = 2,100個の数値

約80%の圧縮率で、画像の主要な特徴を保持
```

**LLMでの応用：**
- 埋め込み行列の圧縮
- LoRA（低ランク適応）の理論的基礎
- モデルの効率化

---

## 2.2 確率論と統計学

### 2.2.1 確率分布と期待値

#### 確率の基礎

**確率変数：**

ランダムな実験の結果を数値で表したもの

**例：**
- サイコロの目： $X \in \{1, 2, 3, 4, 5, 6\}$
- コインの枚数： $Y \in \{0, 1\}$（0=裏、1=表）

#### 離散確率分布

**定義：**

確率変数 $X$ が取りうる各値 $x$ に対して、確率 $P(X=x)$ が定まる

**条件：**
1. $0 \leq P(X=x) \leq 1$ （各確率は0〜1）
2. $\sum_x P(X=x) = 1$ （すべての確率の和は1）

**例（サイコロ）：**

$$P(X=1) = P(X=2) = \cdots = P(X=6) = \frac{1}{6}$$

**視覚化：**
```
確率
1/6 |  ■  ■  ■  ■  ■  ■
    |
    +--1--2--3--4--5--6-- 目
```

#### 連続確率分布

**確率密度関数** $f(x)$：

$$P(a \leq X \leq b) = \int_a^b f(x)dx$$

**例（正規分布）：**

$$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$

**視覚化：**
```
確率密度
    |    /‾‾‾\
    |   /     \
    |  /       \
    | /         \
    |/___________\_____ x
         μ
```

パラメータ：
- $\mu$：平均（中心位置）
- $\sigma^2$：分散（広がり）

#### 期待値（平均）

**定義：**

**離散の場合：**

$$\mathbb{E}[X] = \sum_x x \cdot P(X=x)$$

**連続の場合：**

$$\mathbb{E}[X] = \int_{-\infty}^{\infty} x \cdot f(x)dx$$

**直感的理解：**

> 期待値は「長期的な平均値」  
> サイコロを何度も振ったときの目の平均

**例（サイコロ）：**

$$\mathbb{E}[X] = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + \cdots + 6 \cdot \frac{1}{6} = \frac{21}{6} = 3.5$$

**性質：**
1. 線形性： $\mathbb{E}[aX + b] = a\mathbb{E}[X] + b$
2. 和の期待値： $\mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y]$

#### 分散と標準偏差

**分散：**

$$\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$$

**標準偏差：**

$$\sigma = \sqrt{\text{Var}(X)}$$

**直感的理解：**

> 分散は「平均からのばらつきの大きさ」

**視覚化：**
```
小さい分散：        大きい分散：
    |               |
  /‾‾‾\           /         \
 /     \         /           \
/       \       /             \
                
データが集中      データが散らばる
```

**例（サイコロ）：**

$$\mathbb{E}[X^2] = 1^2 \cdot \frac{1}{6} + 2^2 \cdot \frac{1}{6} + \cdots + 6^2 \cdot \frac{1}{6} = \frac{91}{6}$$

$$\text{Var}(X) = \frac{91}{6} - (3.5)^2 = 15.167 - 12.25 = 2.917$$

$$\sigma = \sqrt{2.917} \approx 1.708$$

---

### 2.2.2 ベイズの定理

#### 条件付き確率

**定義：**

事象Bが起きたときに事象Aが起きる確率：

$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$

**直感的理解：**

> 「Bという情報を得たとき、Aの確率はどう変わるか」

**例（カード）：**

52枚のトランプから1枚引く

- $A$：ハートのカード
- $B$：絵札（J, Q, K）

$$P(A) = \frac{13}{52} = \frac{1}{4}$$

$$P(A|B) = \frac{P(\text{ハートの絵札})}{P(\text{絵札})} = \frac{3/52}{12/52} = \frac{3}{12} = \frac{1}{4}$$

この例では独立（情報Bは役に立たない）

#### ベイズの定理

**定理：**

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

より一般的に：

$$P(A|B) = \frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|A^c)P(A^c)}$$

**用語：**
- $P(A)$：**事前確率**（prior）
- $P(A|B)$：**事後確率**（posterior）
- $P(B|A)$：**尤度**（likelihood）
- $P(B)$：**周辺確率**（evidence）

**直感的理解：**

```
新しい証拠Bを観測
↓
事前の信念P(A)を更新
↓
事後の信念P(A|B)を得る
```

#### 実例：病気の診断

**設定：**
- 病気Dを持つ確率： $P(D) = 0.01$（1%）
- 検査が陽性になる確率：
  - 病気がある場合： $P(+|D) = 0.99$
  - 病気がない場合： $P(+|D^c) = 0.05$

**問題：** 検査が陽性だったとき、実際に病気である確率は？

**解答：**

$$P(D|+) = \frac{P(+|D)P(D)}{P(+|D)P(D) + P(+|D^c)P(D^c)}$$

$$= \frac{0.99 \times 0.01}{0.99 \times 0.01 + 0.05 \times 0.99}$$

$$= \frac{0.0099}{0.0099 + 0.0495} = \frac{0.0099}{0.0594} \approx 0.167$$

**結果：** 約16.7%

**驚きの事実：**
検査精度99%でも、陽性でも病気の確率は約17%！

**理由：**
病気が稀（1%）なので、偽陽性（5%）が多数を占める

**LLMでの応用：**
- 次の単語の予測（観測した文脈から単語の確率を更新）
- モデルの不確実性評価

---

### 2.2.3 情報理論の基礎

#### エントロピー

**定義（シャノンエントロピー）：**

$$H(X) = -\sum_{x} P(x)\log_2 P(x) = \mathbb{E}[-\log_2 P(X)]$$

**直感的理解：**

> エントロピーは「不確実性の量」または「驚きの期待値」

**例：**

**1. 確定的な事象（エントロピー = 0）**

$$P(\text{太陽が東から昇る}) = 1$$

$$H = -1 \times \log_2(1) = 0$$

驚きゼロ = 情報量ゼロ

**2. コイン投げ（エントロピー = 1）**

$$P(\text{表}) = P(\text{裏}) = 0.5$$

$$H = -2 \times (0.5 \times \log_2(0.5)) = -2 \times (0.5 \times (-1)) = 1 \text{ bit}$$

**3. 偏ったコイン**

$$P(\text{表}) = 0.9, P(\text{裏}) = 0.1$$

$$H = -(0.9\log_2 0.9 + 0.1\log_2 0.1) \approx 0.469 \text{ bits}$$

**視覚化：**
```
エントロピー
1.0 |     ●
    |    / \
0.5 |   /   \
    |  /     \
0.0 |●________●_____ p
   0.0  0.5  1.0

p=0.5のとき最大（最も不確実）
```

**データ圧縮との関係：**

> エントロピーは「平均的に必要な最小ビット数」

例：英語のテキスト
- 26文字が等確率： $\log_2(26) \approx 4.7$ bits/文字
- 実際の英語：約1.5 bits/文字（文字の出現確率が偏っているため）

#### クロスエントロピー

**定義：**

真の分布 $p$ と予測分布 $q$ の間のクロスエントロピー：

$$H(p, q) = -\sum_x p(x)\log q(x) = \mathbb{E}_{x \sim p}[-\log q(x)]$$

**意味：**

> 「真の分布pに従うデータを、予測分布qで符号化したときの平均コスト」

**性質：**

$$H(p, q) \geq H(p)$$

等号成立は $p = q$ のとき

**LLMでの応用：**

LLMの訓練では**クロスエントロピー損失**を最小化：

$$\mathcal{L} = -\sum_i \log P_{\text{model}}(w_i | w_{\text{<}i})$$

ここで：
- $w_i$：実際の次の単語（真の分布）
- $P_{\text{model}}(w_i | w_{\text{<}i})$：モデルの予測分布

**直感的理解：**

```
正解の単語に高い確率を与える → 損失小
正解の単語に低い確率を与える → 損失大
```

#### KLダイバージェンス

**定義（Kullback-Leibler情報量）：**

$$D_{\text{KL}}(p \| q) = \sum_x p(x)\log\frac{p(x)}{q(x)} = H(p, q) - H(p)$$

**意味：**

> 「2つの分布の違い」または「qを使うことによる情報の無駄」

**性質：**
1. 非負性： $D_{\text{KL}}(p \| q) \geq 0$
2. 等号成立： $p = q$ のとき
3. 非対称： $D_{\text{KL}}(p \| q) \neq D_{\text{KL}}(q \| p)$

**視覚化：**
```
p(x): 真の分布
q(x): 近似分布

  p  q
  ●  ●    重なりが大きい → KL小
 /|\/|\
  
  p    q
  ●    ●  離れている → KL大
 /|\  /|\
```

**LLMでの応用：**
- 2つのモデルの比較
- RLHF（強化学習からの人間フィードバック）で、元のモデルからの逸脱を制御

---

## 2.3 最適化理論

### 2.3.1 凸最適化

#### 凸関数

**定義：**

関数 $f$ が凸であるとは、任意の $x, y$ と $0 \leq t \leq 1$ に対して：

$$f(tx + (1-t)y) \leq tf(x) + (1-t)f(y)$$

**直感的理解：**

> 「2点を結ぶ線分が、関数のグラフより上にある」

**視覚化：**
```
凸関数：         非凸関数：
   \  /             /\
    \/             /  \  /\
                  /    \/  \
```

**例：**
- 凸関数： $f(x) = x^2$, $f(x) = e^x$, $f(x) = -\log x$ (x>0)
- 非凸関数： $f(x) = \sin(x)$, $f(x) = x^3$

#### 凸最適化問題

**問題：**

$$\min_x f(x) \quad \text{subject to } x \in \mathcal{C}$$

ここで $f$ は凸関数、 $\mathcal{C}$ は凸集合

**重要な性質：**

> 凸最適化問題では、**局所最小値 = 大域最小値**

**視覚化：**
```
凸最適化：           非凸最適化：
    \  /                /\  /\
     \/                /  \/  \  /
     ●                    ●  ●
  大域最小値          局所  大域
```

**残念なニュース：**

> ニューラルネットワークの訓練は**非凸最適化問題**

しかし、実践では良い解が見つかる（後述）

---

### 2.3.2 勾配降下法

#### 勾配の復習

**1変数関数：**

$$f'(x) = \lim_{h \to 0}\frac{f(x+h) - f(x)}{h}$$

**多変数関数の勾配：**

$$\nabla f = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\\ \frac{\partial f}{\partial x_2} \\\ \vdots \\\ \frac{\partial f}{\partial x_n} \end{bmatrix}$$

**重要な性質：**

> 勾配は「関数が最も急激に増加する方向」を指す

**視覚化（2次元）：**
```
等高線：  f(x,y) = c
     
    ∇f
     ↑
    ○ ○
   ○   ○
  ○     ○

勾配は等高線に垂直で、山の頂上を指す
```

#### 勾配降下法のアルゴリズム

**目的：** $\min_{\mathbf{x}} f(\mathbf{x})$

**アルゴリズム：**

```
1. 初期値 x⁽⁰⁾ を設定
2. for t = 0, 1, 2, ... まで：
   x⁽ᵗ⁺¹⁾ = x⁽ᵗ⁾ - η∇f(x⁽ᵗ⁾)
   収束したら終了
```

ここで $\eta$ は**学習率**（ステップサイズ）

**直感的理解：**

> 「山を下るとき、最も急な下り坂の方向に少しずつ進む」

**視覚化：**
```
スタート地点
    ●
    ↓ 勾配の逆方向に進む
   ●
   ↓
  ● 
  ↓
 ●  最小値
```

#### 学習率の重要性

**学習率が大きすぎる：**
```
   ●    ●
  /      \
 ●________● 振動して収束しない
```

**学習率が小さすぎる：**
```
●
↓ 少しずつしか進まない
●
↓
● → 収束が遅い
```

**適切な学習率：**
```
●
 \
  ●
   \
    ● → スムーズに収束
```

#### 収束の理論

**凸関数の場合：**

適切な学習率 $\eta$ のもとで：

$$f(\mathbf{x}^{(t)}) - f(\mathbf{x}^*) = O\left(\frac{1}{t}\right)$$

ここで $\mathbf{x}^*$ は最小値

**強凸関数の場合：**

$$f(\mathbf{x}^{(t)}) - f(\mathbf{x}^*) = O(e^{-ct})$$

指数的に速く収束！

---

### 2.3.3 確率的最適化

#### 確率的勾配降下法（SGD）

**問題設定：**

$$\min_{\mathbf{x}} \mathbb{E}_{\mathbf{z} \sim \mathcal{D}}[f(\mathbf{x}; \mathbf{z})]$$

例：機械学習での経験損失

$$\min_{\mathbf{w}} \frac{1}{N}\sum_{i=1}^{N} \ell(\mathbf{w}; \mathbf{x}_i, y_i)$$

**問題：** データ数 $N$ が巨大（数百万〜数十億）だと、1回の勾配計算が重い

**解決策：SGD**

```
1回の更新で全データを使わず、ランダムに選んだ小さなバッチだけ使う
```

**アルゴリズム：**

```
for t = 0, 1, 2, ... まで：
  バッチ B をランダムサンプリング
  g = (1/|B|) Σ_{i∈B} ∇ℓ(w⁽ᵗ⁾; xᵢ, yᵢ)
  w⁽ᵗ⁺¹⁾ = w⁽ᵗ⁾ - ηg
```

**メリット：**
1. 計算効率が高い
2. メモリ効率が良い
3. 局所最小値から抜け出しやすい（ノイズが助けになる）

**デメリット：**
1. 収束が不安定（振動する）
2. 学習率の調整が難しい

#### ミニバッチSGD

**実践的な設定：**

```
バッチサイズ：32, 64, 128, 256, ...
- 小さい：ノイズ大、収束不安定、計算遅い
- 大きい：ノイズ小、安定、メモリ多、並列化◎
```

**LLMでの典型値：**
- GPT-3：バッチサイズ 3.2M tokens
- 効率とハードウェアの制約のバランス

#### Adam最適化器

**モチベーション：**

SGDの改良版。各パラメータごとに学習率を自動調整

**アルゴリズム：**

```
m⁽⁰⁾ = 0  # 1次モーメント（勾配の移動平均）
v⁽⁰⁾ = 0  # 2次モーメント（勾配の二乗の移動平均）

for t = 1, 2, 3, ... まで：
  g⁽ᵗ⁾ = ∇f(x⁽ᵗ⁻¹⁾)
  m⁽ᵗ⁾ = β₁m⁽ᵗ⁻¹⁾ + (1-β₁)g⁽ᵗ⁾
  v⁽ᵗ⁾ = β₂v⁽ᵗ⁻¹⁾ + (1-β₂)(g⁽ᵗ⁾)²
  
  m̂⁽ᵗ⁾ = m⁽ᵗ⁾/(1-β₁ᵗ)  # バイアス補正
  v̂⁽ᵗ⁾ = v⁽ᵗ⁾/(1-β₂ᵗ)
  
  x⁽ᵗ⁾ = x⁽ᵗ⁻¹⁾ - η · m̂⁽ᵗ⁾/(√v̂⁽ᵗ⁾ + ε)
```

**パラメータ：**
- $\beta_1 = 0.9$：1次モーメントの減衰率
- $\beta_2 = 0.999$：2次モーメントの減衰率
- $\eta = 0.001$：学習率
- $\epsilon = 10^{-8}$：数値安定性のための小さな定数

**直感的理解：**

```
m: 勾配の「慣性」
   → 一貫した方向には速く進む
   
v: 勾配の「大きさの履歴」
   → 大きく変動する方向は慎重に進む
```

**LLMでの応用：**
- ほぼすべてのLLMの訓練でAdamまたはその変種を使用
- GPT, BERT, T5など

---

## 2.4 関数解析

### 2.4.1 ヒルベルト空間

#### 内積空間

**定義：**

ベクトル空間 $V$ に内積 $\langle \cdot, \cdot \rangle$ が定義されているとき、 $V$ を内積空間という

**内積の公理：**
1. 線形性： $\langle a\mathbf{u} + b\mathbf{v}, \mathbf{w} \rangle = a\langle \mathbf{u}, \mathbf{w} \rangle + b\langle \mathbf{v}, \mathbf{w} \rangle$
2. 対称性： $\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle$
3. 正定値性： $\langle \mathbf{v}, \mathbf{v} \rangle \geq 0$、等号は $\mathbf{v} = \mathbf{0}$ のとき

**例：**
- ユークリッド空間 $\mathbb{R}^n$ の標準内積
- 関数空間 $L^2$ の内積： $\langle f, g \rangle = \int f(x)g(x)dx$

#### ヒルベルト空間

**定義：**

完備な内積空間をヒルベルト空間という

**「完備」とは：**

> コーシー列が収束する（極限が空間内に存在する）

**直感的理解：**

> 「穴がない」空間  
> 数列が収束しそうなら、実際に収束先が存在する

**重要な例：**

1. ** $\mathbb{R}^n$**：有限次元ヒルベルト空間

2. ** $\ell^2$**：二乗和が有限な無限数列の空間

   $$\ell^2 = \left\\{(x_1, x_2, \ldots) : \sum_{i=1}^{\infty} x_i^2 < \infty\right\\}$$

3. ** $L^2([a,b])$**：二乗積分可能な関数の空間

   $$L^2 = \left\\{f : \int_a^b |f(x)|^2dx < \infty\right\\}$$

**なぜ重要か：**

> ニューラルネットワークは無限次元ヒルベルト空間の関数として解析できる

---

### 2.4.2 関数近似理論

#### 汎用近似定理

**定理（Cybenko, 1989; Hornik, 1991）：**

1層の隠れ層を持つニューラルネットワークは、任意の連続関数を任意の精度で近似できる

**数学的表現：**

任意の連続関数 $f: [0,1]^n \to \mathbb{R}$ と $\epsilon > 0$ に対して、ニューラルネットワーク

$$g(x) = \sum_{i=1}^{m} w_i \sigma(a_i^\top x + b_i)$$

が存在して、

$$\|f - g\|_\infty < \epsilon$$

ここで $\sigma$ は非多項式的な連続活性化関数（例：シグモイド、ReLU）

**直感的理解：**

> 「十分に広いニューラルネットワークは、どんな関数でも近似できる」

**視覚化（1次元の例）：**

```
目標関数 f(x)
    /‾‾\  /\
   /    \/  \

階段関数で近似
   _  _
  | || |
__|  |  |__

ReLU の線形結合で任意の精度で近似可能
```

**重要な注意：**
- 定理は「存在する」というだけ
- 実際に見つけられるかは別問題
- パラメータ数が膨大になる可能性

**深いネットワークの利点：**

同じ近似精度を達成するのに必要なパラメータ数が少ない

```
浅くて広いネットワーク：
 入力 → [10000ニューロン] → 出力

深いネットワーク：
 入力 → [100] → [100] → [100] → 出力
 
後者の方がパラメータ数が少ない！
```

---

## 本章のまとめ

### 学んだこと

✅ **線形代数**
- ベクトル、行列、内積、ノルム
- 固有値分解とSVD
- LLMでの応用：埋め込み、アテンション

✅ **確率・統計**
- 確率分布、期待値、分散
- ベイズの定理
- エントロピー、KLダイバージェンス
- LLMでの応用：言語モデリング、損失関数

✅ **最適化理論**
- 勾配降下法
- 確率的最適化（SGD、Adam）
- LLMでの応用：モデルの訓練

✅ **関数解析**
- ヒルベルト空間
- 汎用近似定理
- LLMでの応用：理論的基礎

### 重要な公式まとめ

| 概念 | 公式 | 意味 |
|------|------|------|
| L2ノルム | $\|\mathbf{v}\|_2 = \sqrt{\sum_i v_i^2}$ | ベクトルの長さ |
| 内積 | $\mathbf{a} \cdot \mathbf{b} = \sum_i a_ib_i$ | 類似度 |
| コサイン類似度 | $\cos\theta = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\|\|\mathbf{b}\|}$ | 方向の類似度 |
| 期待値 | $\mathbb{E}[X] = \sum_x xP(x)$ | 平均 |
| 分散 | $\text{Var}(X) = \mathbb{E}[(X-\mathbb{E}[X])^2]$ | ばらつき |
| エントロピー | $H(X) = -\sum_x P(x)\log P(x)$ | 不確実性 |
| クロスエントロピー | $H(p,q) = -\sum_x p(x)\log q(x)$ | 予測の損失 |
| 勾配降下 | $\mathbf{x}^{(t+1)} = \mathbf{x}^{(t)} - \eta\nabla f(\mathbf{x}^{(t)})$ | 最適化 |

### 次章の予告

第3章では、これらの数学的ツールを使って**ニューラルネットワークの基礎**を学びます：
- パーセプトロン
- 多層パーセプトロン
- バックプロパゲーション
- 活性化関数
- 勾配消失・爆発問題

LLMの核心であるニューラルネットワークの仕組みを、数式と図解で理解していきます。

---

## 練習問題

### 問題1：ベクトル演算

$$\mathbf{a} = \begin{bmatrix} 1 \\\ 2 \\\ 3 \end{bmatrix}, \mathbf{b} = \begin{bmatrix} 4 \\\ -1 \\\ 2 \end{bmatrix}$$

 に対して：
1. $\mathbf{a} + \mathbf{b}$
2. $\mathbf{a} \cdot \mathbf{b}$
3. $\|\mathbf{a}\|_2$
4. コサイン類似度

### 問題2：確率
サイコロを2回振るとき、合計が7になる確率を求めよ。

### 問題3：情報理論
確率分布 $P(X=1)=0.25, P(X=2)=0.25, P(X=3)=0.5$ のエントロピーを計算せよ。

### 問題4：勾配降下
$f(x) = x^2 - 4x + 5$ を最小化する。 $x^{(0)}=0$, $\eta=0.1$ で2回更新せよ。

### 解答

**問題1:**

1. 

$$\begin{bmatrix} 5 \\\ 1 \\\ 5 \end{bmatrix}$$

2. $1 \times 4 + 2 \times (-1) + 3 \times 2 = 8$
3. $\sqrt{1^2+2^2+3^2} = \sqrt{14} \approx 3.742$
4. $\frac{8}{\sqrt{14}\sqrt{21}} \approx 0.466$

**問題2:**
組み合わせ：(1,6), (2,5), (3,4), (4,3), (5,2), (6,1) = 6通り  
確率： $6/36 = 1/6$

**問題3:**

$$H = -(0.25\log_2 0.25 + 0.25\log_2 0.25 + 0.5\log_2 0.5)$$

$$= -(0.25 \times (-2) + 0.25 \times (-2) + 0.5 \times (-1)) = 1.5 \text{ bits}$$

**問題4:**
$f'(x) = 2x - 4$

更新1: $x^{(1)} = 0 - 0.1(2 \times 0 - 4) = 0.4$  
更新2: $x^{(2)} = 0.4 - 0.1(2 \times 0.4 - 4) = 0.4 - 0.1(-3.2) = 0.72$

真の最小値： $x^* = 2$（ $f'(x)=0$ から）

---

**📖 前章：[第1章 序論](./第1章_序論.md)**  
**📖 次章：[第3章 ニューラルネットワークの基礎](./第3章_ニューラルネットワークの基礎.md)**
