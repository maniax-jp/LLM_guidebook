# 第16章：計算複雑性の理論

この章では、LLMの**計算量とメモリ使用量**を理論的に解析し、スケーラビリティの限界と効率的設計の原理を学びます。

---

## 16.1 Transformerの計算複雑性

### 16.1.1 自己注意機構の複雑性

**標準Self-Attention：**

入力系列長 $n$、次元 $d$

**ステップ1：Query, Key, Value計算**

$$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$$

計算量： $O(nd^2)$（3つの行列積）

**ステップ2：Attention重みの計算**

$$A = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)$$

- $QK^T$：$O(n^2d)$
- softmax： $O(n^2)$

**ステップ3：出力計算**

$$\text{Output} = AV$$

計算量： $O(n^2d)$

**総計算量：**

$$O(nd^2 + n^2d)$$

**支配項：**

- 短い系列（ $n < d$）： $O(nd^2)$
- 長い系列（ $n > d$）： $O(n^2d)$ ← 問題！

**視覚化：**

```
計算量
  │
  │        ╱ O(n²d)
  │      ╱
  │    ╱
  │  ╱
  │╱_________ O(nd²)
  └──────────────→
        系列長 n
```

### 16.1.2 Multi-Head Attentionの複雑性

**構成：**

$H$ 個のヘッド、各ヘッドの次元 $d_h = d/H$

**各ヘッド：**

$$O(n d_h^2 + n^2 d_h) = O(n(d/H)^2 + n^2(d/H))$$

**総計（ $H$ ヘッド）：**

$$O\left(H \cdot n(d/H)^2 + H \cdot n^2(d/H)\right) = O(nd^2/H + n^2d)$$

**結合と射影：**

$$O(nd^2)$$

**総計算量：**

$$O(nd^2 + n^2d)$$

標準Attentionと同じオーダー（定数倍は異なる）

### 16.1.3 FFNの複雑性

**構造：**

$$\text{FFN}(x) = W_2 \cdot \text{GELU}(W_1 \cdot x + b_1) + b_2$$

- $W_1 \in \mathbb{R}^{d \times d_{\text{ff}}}$（通常 $d_{\text{ff}} = 4d$）
- $W_2 \in \mathbb{R}^{d_{\text{ff}} \times d}$

**計算量（系列長 $n$）：**

- $W_1$：$O(nd \cdot d_{\text{ff}}) = O(nd \cdot 4d) = O(4nd^2)$
- $W_2$：$O(n d_{\text{ff}} \cdot d) = O(4nd^2)$

**総計：**

$$O(8nd^2) = O(nd^2)$$

### 16.1.4 Transformer層全体の複雑性

**1層あたり：**

- Multi-Head Attention： $O(nd^2 + n^2d)$
- FFN： $O(nd^2)$
- Layer Norm、残差： $O(nd)$（無視可能）

**総計：**

$$O(nd^2 + n^2d)$$

** $L$ 層のTransformer：**

$$O(L(nd^2 + n^2d))$$

**具体例：**

```
GPT-3:
  L = 96
  d = 12,288
  n = 2,048

計算量（Forward Pass）:
  96 × (2048 × 12288² + 2048² × 12288)
  ≈ 96 × (3.1×10¹¹ + 5.2×10¹⁰)
  ≈ 3.5×10¹³ FLOPs

1トークン生成あたり！
```

---

## 16.2 メモリ複雑性

### 16.2.1 パラメータメモリ

**Transformer層のパラメータ：**

**Attention：**

$$|W_Q| + |W_K| + |W_V| + |W_O| = 4d^2$$

**FFN：**

$$|W_1| + |W_2| = d \cdot d_{\text{ff}} + d_{\text{ff}} \cdot d = 2d \cdot d_{\text{ff}} = 8d^2$$

**Layer Norm：**

$$2d$$

（スケール、シフト）× 2箇所 = $4d$

**1層あたり：**

$$4d^2 + 8d^2 + 4d \approx 12d^2$$

** $L$ 層：**

$$12Ld^2$$

**埋め込み層：**

$$|V| \cdot d$$

（ $|V|$：語彙サイズ）

**総パラメータ数：**

$$P \approx 12Ld^2 + |V|d$$

**メモリ（float32）：**

$$M = 4P \text{ bytes}$$

**例：**

```
GPT-3:
  L = 96, d = 12,288, |V| = 50,257

  P ≈ 12 × 96 × 12,288² + 50,257 × 12,288
    ≈ 1.74×10¹¹ + 6.2×10⁸
    ≈ 175B

  M = 4 × 175×10⁹ = 700GB
```

### 16.2.2 活性化メモリ

**Forward Pass：**

各層の出力を保存（バックプロパゲーション用）

**1層あたり（バッチサイズ $b$、系列長 $n$）：**

$$b \times n \times d$$

** $L$ 層：**

$$L \times b \times n \times d$$

**Attentionスコア（追加）：**

$$L \times b \times H \times n \times n$$

**総活性化メモリ：**

$$M_{\text{act}} = O(Lbnd + LbHn^2)$$

**支配項：**

長い系列で $O(LbHn^2)$

**例：**

```
GPT-3訓練:
  L = 96, b = 32, n = 2048, d = 12,288, H = 96

  Lbnd = 96 × 32 × 2048 × 12,288 × 4 bytes
       ≈ 310GB

  LbHn² = 96 × 32 × 96 × 2048² × 4 bytes
        ≈ 500GB

  総活性化: ~800GB
```

### 16.2.3 Gradient Checkpointing

**問題：**

活性化メモリが巨大

**解決策：**

活性化を保存せず、必要時に再計算

```
Forward Pass:
  チェックポイント層のみ保存
  （例: 10層ごと）

Backward Pass:
  必要な活性化を再計算
```

**トレードオフ：**

- メモリ： $O(L)$ → $O(\sqrt{L})$（チェックポイント数）
- 計算：2倍（再計算のため）

**視覚化：**

```
標準:
  Forward → 全層保存 → Backward
  メモリ: 高、計算: 1倍

Gradient Checkpointing:
  Forward → 一部保存 → Backward（再計算）
  メモリ: 低、計算: 2倍
```

---

## 16.3 長い系列への対処

### 16.3.1 問題の整理

**標準Attention：**

$$O(n^2d)$$

```
n = 512:    512² = 262K
n = 2048:   2048² = 4.2M   (16倍)
n = 8192:   8192² = 67M    (256倍)
n = 32768:  32768² = 1.1G  (4096倍)
```

**限界：**

現実的には $n \approx 2K\sim 8K$

### 16.3.2 Sparse Attention

**アイデア：**

全トークンではなく、一部のみに注目

**パターン1：固定パターン**

```
Strided (ストライド型):
  トークン i → i-k, i-2k, i-3k, ...
  (k刻みで注目)

Local (局所型):
  トークン i → i-w, ..., i, ..., i+w
  (ウィンドウ w 内)
```

**パターン2：Longformer**

```
組み合わせ:
  - 局所的Attention（全トークン）
  - グローバルトークン（特別なトークンが全体を見る）
  - ストライドAttention
```

**計算量：**

密度 $s$ のSparse Attention

$$O(sn \cdot d) \quad (s \ll n)$$

例： $s = 256$ なら

$$O(256d)$$

（ $n$ に依存しない！）

### 16.3.3 Linformer

**アイデア：**

Key, Valueを低次元に射影

$$K' = EK, \quad V' = FV$$

ここで $E, F \in \mathbb{R}^{k \times n}$、$k \ll n$

**Attention計算：**

$$\text{Attention}(Q, K', V') = \text{softmax}\left(\frac{QK'^T}{\sqrt{d}}\right)V'$$

**計算量：**

$$Q K'^T: O(nkd)$$
$$\text{softmax}(...)V': O(nkd)$$

総計： $O(nkd)$

$k$ を定数（例: 256）にすれば $O(nd)$！

### 16.3.4 Performer (FAVOR+)

**アイデア：**

カーネルトリックでAttentionを近似

**標準Attention：**

$$\text{Attention}(Q, K, V) = \text{softmax}(QK^T)V$$

**カーネル近似：**

$$\text{softmax}(QK^T) \approx \phi(Q)\phi(K)^T$$

$\phi$：特徴写像（ランダムフーリエ特徴など）

**計算順序の変更：**

$$\phi(Q)[\phi(K)^T V]$$

**計算量：**

- $\phi(K)^T V$：$O(ndr)$（$r$：特徴次元）
- $\phi(Q)[...]$：$O(ndr)$

総計： $O(ndr)$

$r$ を小さく（例: 256）すれば線形！

**視覚化：**

```
標準 (n²d):
  Q (n×d) × K^T (d×n) = (n×n)
           ↓
     (n×n) × V (n×d) = (n×d)

Performer (ndr):
  φ(K)^T (r×n) × V (n×d) = (r×d)
           ↓
  φ(Q) (n×r) × (r×d) = (n×d)
```

### 16.3.5 Flash Attention

**アイデア：**

メモリ階層を考慮したアルゴリズム

**問題：**

標準実装は中間行列 $QK^T \in \mathbb{R}^{n \times n}$ をメモリに保存

**解決策：**

タイル化してオンザフライで計算

```
for ブロック i in Q:
  for ブロック j in K, V:
    小ブロックで Attention 計算
    部分結果を累積

→ 中間行列を保存しない
```

**効果：**

- メモリ： $O(n)$（$O(n^2)$ から削減）
- 速度：2-4倍高速（メモリアクセス最適化）

**計算量は変わらず** $O(n^2d)$ だが、実装効率が向上

---

## 16.4 近似と下界

### 16.4.1 Attentionの必要性

**理論的結果：**

一部のタスクはSparse Attentionでは解けない

**例：**

```
タスク: 文中の全ての名詞を検出

Dense Attention:
  各トークンが全トークンを参照可能
  → 解ける

Sparse Attention (局所のみ):
  遠くの名詞を見られない
  → 解けない可能性
```

**トレードオフ：**

表現力 vs 計算効率

### 16.4.2 計算複雑性の下界

**定理（非公式）：**

言語モデリングタスクで、十分な精度を達成するには

$$\Omega(n^2)$$

の計算が必要（worst case）

**直感：**

$n$ 個のトークン間の全ペアの関係性を考慮する必要がある場合

**ただし：**

- 実際のデータは構造化されている
- 近似で十分な場合が多い
- 平均的ケースは下界より良い

### 16.4.3 Universal Approximation

**Transformerの表現力：**

十分な層数と幅で、任意の関数を近似可能

**定理（簡略版）：**

$L$ 層、幅 $d$ のTransformerは、 $\epsilon$ 精度で任意の系列関数を近似可能

$$\exists L, d \quad \text{s.t.} \quad |f(x) - \text{Transformer}(x)| < \epsilon$$

**必要な $L, d$：**

タスクの複雑性に依存

### 16.4.4 Sample Complexity

**汎化誤差の境界：**

VC次元理論（復習）

$$\text{汎化誤差} \leq \text{訓練誤差} + O\left(\sqrt{\frac{P \log n}{n}}\right)$$

- $P$：パラメータ数
- $n$：サンプル数

**LLMでの含意：**

$$P \sim 10^{11}, \quad n \sim 10^{11}$$

$$\sqrt{\frac{P \log n}{n}} \sim \sqrt{\frac{10^{11} \times 38}{10^{11}}} \approx 6$$

（緩すぎる上界）

**より良い理論：**

- 圧縮ベースの境界
- PAC-Bayes
- 情報理論的境界

---

## 16.5 効率的アーキテクチャの設計原理

### 16.5.1 計算とメモリのトレードオフ

**設計選択：**

| 設計 | 計算 | メモリ | 性能 |
|------|------|--------|------|
| 標準Transformer | $O(n^2d)$ | $O(n^2)$ | 高 |
| Sparse Attention | $O(nd)$ | $O(n)$ | 中 |
| Linformer | $O(nkd)$ | $O(nk)$ | 中 |
| Performer | $O(nrd)$ | $O(nr)$ | 中 |
| Flash Attention | $O(n^2d)$ | $O(n)$ | 高 |

**推奨：**

- 短い系列（ $n < 2K$）：標準Transformer
- 長い系列（ $n > 8K$）：Sparse / Linear Attention
- メモリ制約：Flash Attention

### 16.5.2 深さ vs 幅

**深いモデル：**

```
多層（L大）、狭い（d小）

利点:
  - 階層的特徴抽出
  - パラメータ効率（同じPで）

欠点:
  - 訓練が困難（勾配消失）
  - 逐次的（並列化困難）
```

**広いモデル：**

```
少層（L小）、広い（d大）

利点:
  - 訓練が容易
  - 並列化容易

欠点:
  - パラメータ非効率
  - 表現力に限界
```

**実証結果：**

バランスが重要

```
GPT-3: L=96, d=12,288
LLaMA: L=32, d=4,096 (小型)
       L=80, d=8,192 (大型)
```

### 16.5.3 Mixture of Experts (MoE)

**アイデア：**

全パラメータを常に使わず、条件付きで使用

```
FFN層を複数のエキスパートに分割:

入力 x
  ↓
ルーター（どのエキスパートを使うか選択）
  ↓
  ├─ エキスパート1 → 0.7
  ├─ エキスパート2 → 0.3
  ├─ エキスパート3 → 0
  └─ ...
  ↓
加重和: 0.7 × E₁(x) + 0.3 × E₂(x)
```

**計算量：**

$E$ 個のエキスパート、上位 $k$ 個を使用

$$\text{標準FFN}: O(nd^2)$$
$$\text{MoE}: O(k \cdot nd^2 / E)$$

$k \ll E$ なら大幅削減

**例：**

```
Switch Transformer:
  E = 2048 エキスパート
  k = 1 （top-1）

  活性化パラメータ: 1/2048
  総パラメータ: 巨大（1.6T）
  計算コスト: 小（GPT-3相当）
```

**利点：**

- パラメータ数 ↑、計算量 →
- 高い表現力

**欠点：**

- 訓練が複雑
- バランシング問題（一部エキスパートに偏る）

### 16.5.4 スケーリング則の最適化

**Chinchilla原則（復習）：**

固定計算予算 $C$ で

$$N \propto C^{0.5}, \quad D \propto C^{0.5}$$

モデルサイズ $N$ とデータサイズ $D$ を同時にスケール

**アーキテクチャへの影響：**

```
同じ計算予算で:

選択肢A: 巨大モデル、少ないデータ
選択肢B: 中規模モデル、多くのデータ ← Chinchilaは推奨

実装:
  選択肢Bは中規模アーキテクチャが有利
  → 効率的な中規模設計が重要
```

---

## 本章のまとめ

### 学んだこと

✅ **計算複雑性**
- Transformer: $O(L(nd^2 + n^2d))$
- Attention: $O(n^2d)$（ボトルネック）
- FFN: $O(nd^2)$

✅ **メモリ複雑性**
- パラメータ: $O(Ld^2)$
- 活性化: $O(Lbnd + LbHn^2)$
- Gradient Checkpointing

✅ **効率的Attention**
- Sparse Attention: $O(snd)$
- Linformer: $O(nkd)$
- Performer: $O(nrd)$
- Flash Attention: メモリ最適化

✅ **設計原理**
- 深さ vs 幅
- Mixture of Experts
- スケーリング則の最適化

### 複雑性の比較

| アーキテクチャ | 計算量 | メモリ | 系列長制限 |
|--------------|--------|--------|----------|
| 標準Transformer | $O(n^2d)$ | $O(n^2)$ | 〜8K |
| Sparse | $O(nd)$ | $O(n)$ | 〜64K |
| Linformer | $O(nkd)$ | $O(nk)$ | 〜64K |
| Performer | $O(nrd)$ | $O(nr)$ | 〜1M |
| Flash Attention | $O(n^2d)$ | $O(n)$ | 〜64K |

### 実用的ガイドライン

**系列長による選択：**

```
n < 2K:    標準Transformer
2K < n < 8K:  Flash Attention
8K < n < 64K: Sparse or Linformer
n > 64K:   Performer or 階層的モデル
```

**リソースによる選択：**

```
計算制約:  Sparse, Linformer, Performer
メモリ制約: Flash Attention, Gradient Checkpointing
両方制約:  Sparse + Flash Attention
```

### スケーリングの限界

```
現状:
  最大モデル: ~500B パラメータ
  最大系列: ~100K トークン
  訓練コスト: ~$10M

理論的限界:
  計算: 無限大
  メモリ: 物理的制限
  データ: インターネット全体（数十TB）

実用的限界:
  コスト
  エネルギー
  環境影響
```

### 次章の予告

第17章では、**認知科学との接点**を学びます：
- 言語処理の認知モデル
- 人間とLLMの比較
- 意味理解のメカニズム
- 意識と理解の問題

LLMと人間の知能の関係を、認知科学の視点から理解していきます。

---

## 練習問題

### 問題1：計算量比較
系列長 $n=4096$、次元 $d=1024$ のとき、Attention計算 $O(n^2d)$ と FFN計算 $O(nd^2)$ のどちらが大きいか？

### 問題2：メモリ削減
96層のモデルでGradient Checkpointingを8層ごとに適用した場合、保存するチェックポイント数は？

### 問題3：Sparse Attention
密度 $s=256$、系列長 $n=16384$、次元 $d=1024$ のSparse Attentionの計算量は、標準Attentionの何分の1か？

### 問題4：パラメータ数
層数 $L=48$、次元 $d=4096$、語彙サイズ $|V|=32000$ のTransformerのパラメータ数は？（$P \approx 12Ld^2 + |V|d$）

### 解答

**問題1:**

Attention: $n^2d = 4096^2 \times 1024 = 1.72 \times 10^{10}$

FFN: $nd^2 = 4096 \times 1024^2 = 4.29 \times 10^{9}$

**結論:** Attentionの方が約4倍大きい

**問題2:**

層数: 96  
チェックポイント間隔: 8層

チェックポイント数: $96 / 8 = 12$

**問題3:**

標準: $n^2d = 16384^2 \times 1024 = 2.75 \times 10^{11}$

Sparse: $snd = 256 \times 16384 \times 1024 = 4.29 \times 10^{9}$

削減率: $\frac{2.75 \times 10^{11}}{4.29 \times 10^{9}} \approx 64$

**答:** 64分の1

**問題4:**

$$P = 12Ld^2 + |V|d$$
$$= 12 \times 48 \times 4096^2 + 32000 \times 4096$$
$$= 12 \times 48 \times 16,777,216 + 131,072,000$$
$$\approx 9.66 \times 10^9 + 1.31 \times 10^8$$
$$\approx 9.8 \times 10^9$$

**答:** 約9.8B（98億）パラメータ

---

**📖 前章：[第15章 解釈可能性と安全性の理論](./第15章_解釈可能性と安全性の理論.md)**  
**📖 次章：[第17章 認知科学との接点](./第17章_認知科学との接点.md)**
