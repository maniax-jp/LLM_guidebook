# 第8章：ファインチューニングの理論

この章では、事前学習済みモデルを特定のタスクやドメインに適応させる**ファインチューニング（Fine-tuning）**の理論を学びます。

---

## 8.1 転移学習の統計学習理論

### 8.1.1 転移学習とは

**定義：**

あるタスク（ソースタスク）で学習した知識を、別のタスク（ターゲットタスク）に転用

**LLMでの転移学習：**

```
事前学習 (Pre-training)
  タスク: 次トークン予測
  データ: 大規模汎用テキスト (TB級)
  ↓
ファインチューニング (Fine-tuning)
  タスク: 特定タスク (QA, 要約, 分類など)
  データ: タスク固有データ (MB-GB級)
```

**なぜ有効？**

1. **特徴表現の共有**
   - 言語の基本的な構造は共通
   - 文法、意味、世界知識

2. **データ効率**
   - 少ないデータで高性能
   - ラベル付きデータのコスト削減

3. **計算効率**
   - ゼロからの訓練より遥かに高速

### 8.1.2 理論的枠組み

**記号：**

- $\mathcal{D}_s$：ソース（事前学習）データ
- $\mathcal{D}_t$：ターゲット（ファインチューニング）データ
- $\theta_0$：ランダム初期化
- $\theta_s$：事前学習後のパラメータ
- $\theta_t$：ファインチューニング後のパラメータ

**2つのアプローチ：**

**A. ゼロからの訓練：**
$$\theta_t = \arg\min_\theta \mathcal{L}_t(\theta; \mathcal{D}_t) \quad \text{（初期値: } \theta_0 \text{）}$$

**B. ファインチューニング：**
$$\theta_t = \arg\min_\theta \mathcal{L}_t(\theta; \mathcal{D}_t) \quad \text{（初期値: } \theta_s \text{）}$$

**理論的保証：**

ソースとターゲットのタスクが関連している場合、Bの方が：
- 少ないデータで収束
- より良い汎化性能
- より速い収束

### 8.1.3 汎化誤差の分解

**汎化誤差：**

$$\mathbb{E}[\mathcal{L}_t(\theta_t)] = \underbrace{\mathcal{L}_t(\theta^*)}_{\text{ベイズ誤差}} + \underbrace{\mathcal{L}_t(\theta_t) - \mathcal{L}_t(\theta^*)}_{\text{推定誤差}}$$

**ファインチューニングの効果：**

事前学習により、 $\theta_s$ が $\theta^*$ に近い  
→ 推定誤差が小さい  
→ 少ないデータで良い性能

**視覚化：**

```
損失空間

        ●θ₀ (ランダム初期化)
       /
      /  ゼロからの訓練
     /   (長い道のり)
    /
   ●θₛ (事前学習済み)
    \
     \ ファインチューニング
      \ (短い道のり)
       \
        ●θ* (最適解)
```

### 8.1.4 サンプル複雑性

**定理（簡略版）：**

ターゲットタスクで $\epsilon$-精度を達成するのに必要なサンプル数：

**ゼロから：** $N = O(d/\epsilon^2)$  
**ファインチューニング：** $N = O(\Delta/\epsilon^2)$

ここで：
- $d$：パラメータ次元
- $\Delta$： $\theta_s$ から $\theta^*$ までの距離

$\Delta \ll d$ なら、大幅にサンプル効率的！

**具体例：**

```
テキスト分類タスク
  パラメータ数: 1億

ゼロから:
  必要データ: 100万サンプル
  
ファインチューニング:
  必要データ: 1万サンプル (100倍効率的！)
```

---

## 8.2 ドメイン適応のモデリング

### 8.2.1 ドメインシフト

**問題：**

訓練データとテストデータの分布が異なる

**例：**

```
事前学習: 一般ウェブテキスト
  "今日は良い天気ですね"
  "新製品が発売されました"
  
ターゲット: 医療文書
  "患者は高血圧症の既往歴がある"
  "投薬後、症状の改善が見られた"
  
→ 語彙、文体、トピックが大きく異なる
```

**数学的定式化：**

- $P_s(x, y)$：ソースドメインの分布
- $P_t(x, y)$：ターゲットドメインの分布

$$P_s \neq P_t$$

### 8.2.2 ドメイン適応の戦略

#### 戦略1：継続事前学習

**アプローチ：**

ターゲットドメインのデータで事前学習を継続

$$\theta_{\text{adapt}} = \arg\min_\theta \mathcal{L}_{\text{LM}}(\theta; \mathcal{D}_{\text{domain}})$$

**例：**

```
1. ベースモデル: GPT-2 (汎用)
2. 継続学習: 医療文書100GB
3. 結果: 医療特化GPT-2
```

**効果：**

- ドメイン固有の語彙を学習
- 文体に適応
- ドメイン知識の獲得

**注意点：**

元のドメインの性能が低下する可能性（破滅的忘却、後述）

#### 戦略2：タスク特化ファインチューニング

**アプローチ：**

ラベル付きデータで教師あり学習

$$\theta_t = \arg\min_\theta \mathcal{L}_{\text{task}}(\theta; \mathcal{D}_{\text{labeled}})$$

**例：**

```
タスク: 医療文書の感情分析
データ: ラベル付き1万サンプル
  "この治療法は効果的です" → Positive
  "副作用が心配です" → Negative
```

#### 戦略3：ハイブリッド

**ステップ1：**継続事前学習（ドメイン適応）  
**ステップ2：**タスクファインチューニング

```
汎用LLM
  ↓ 継続事前学習
ドメイン適応LLM
  ↓ タスクFT
タスク特化モデル
```

**最良の結果を達成することが多い！**

### 8.2.3 適応の度合い

**トレードオフ：**

```
浅い適応              深い適応
  ↓                    ↓
少ない更新          多くの更新
汎用性保持          特化性向上
安全               リスク高
```

**実践的ガイドライン：**

| データ量 | 推奨戦略 |
|---------|---------|
| <1K | プロンプトエンジニアリング |
| 1K-10K | LoRA, Adapter等の軽量FT |
| 10K-100K | 上位層のみFT |
| 100K-1M | 全層FT（小学習率） |
| >1M | 継続事前学習 + 全層FT |

---

## 8.3 破滅的忘却の理論的解析

### 8.3.1 破滅的忘却とは

**定義：**

新しいタスクを学習する際、以前のタスクの性能が急激に低下する現象

**例：**

```
タスクA訓練後:
  タスクA性能: 90%
  タスクB性能: 0% (未学習)

タスクB訓練後:
  タスクA性能: 30% ← 破滅的忘却！
  タスクB性能: 85%
```

**視覚化：**

```
パラメータ空間

  ●θA (タスクA最適)
   
   \  タスクB訓練
    \
     \
      ●θB (タスクB最適)
      
θAとθBが離れている → 忘却発生
```

### 8.3.2 なぜ起こるか

**原因：**

1. **重みの上書き**
   - タスクAとタスクBが同じパラメータを使用
   - Bの学習がAの重みを破壊

2. **最適解の距離**
   - $\|\theta_A^* - \theta_B^*\|$ が大きい
   - 両方を同時に満たす解が存在しない

3. **学習率の問題**
   - 大きい学習率 → 急激な変化 → 忘却

**数学的モデル：**

損失ランドスケープ：

$$\mathcal{L}_{\text{total}} = \mathcal{L}_A(\theta) + \mathcal{L}_B(\theta)$$

タスクAとBの最適解が異なる領域にある場合、トレードオフが発生

### 8.3.3 緩和策

#### 1. 小さい学習率

**アプローチ：**

ファインチューニング時の学習率を事前学習より小さく

```
事前学習: η = 3×10⁻⁴
ファインチューニング: η = 1×10⁻⁵ (10-100倍小さい)
```

**効果：**

パラメータの大きな変化を防ぐ

#### 2. 早期終了

**アプローチ：**

過度に訓練しない

**基準：**

- 検証損失のモニタリング
- 固定エポック数（1-3エポック）

#### 3. 正則化

**L2正則化（重み減衰）：**

$$\mathcal{L}_{\text{FT}} = \mathcal{L}_{\text{task}} + \frac{\lambda}{2}\|\theta - \theta_s\|^2$$

元のパラメータ $\theta_s$ から離れすぎないように制約

**Elastic Weight Consolidation (EWC)：**

重要なパラメータの変更を抑制

$$\mathcal{L}_{\text{EWC}} = \mathcal{L}_{\text{task}} + \frac{\lambda}{2}\sum_i F_i(\theta_i - \theta_{s,i})^2$$

ここで $F_i$ はパラメータ $i$ の重要度（Fisherinfo）

#### 4. パラメータ効率的ファインチューニング

**LoRA（Low-Rank Adaptation）：**

元のパラメータを固定し、小さなアダプタを追加

$$W = W_0 + \Delta W = W_0 + BA$$

ここで：
- $W_0$：凍結された元の重み
- $B, A$：低ランク行列（訓練可能）

**利点：**

元のパラメータが変わらない → 忘却なし！

（詳細は第14章で）

---

## 8.4 継続学習アルゴリズムの基礎

### 8.4.1 継続学習の問題設定

**シナリオ：**

タスク列 $T_1, T_2, \ldots, T_n$ を順次学習

**目標：**

すべてのタスクで良い性能を維持

$$\min_\theta \sum_{i=1}^{n} \mathcal{L}_{T_i}(\theta)$$

**課題：**

- タスク $T_i$ 学習時、 $T_j$ (j<i) のデータは利用不可
- メモリ・計算の制約

### 8.4.2 主要なアプローチ

#### 1. リハーサル（Rehearsal）

**アイデア：**

過去のタスクのサンプルを保存し、新タスク学習時に混ぜる

```
タスクB学習時:
  バッチ = タスクBサンプル (90%) + タスクAサンプル (10%)
```

**経験再生（Experience Replay）：**

$$\mathcal{L} = \mathcal{L}_{T_{\text{new}}}(\theta; \mathcal{D}_{\text{new}}) + \alpha \mathcal{L}_{T_{\text{old}}}(\theta; \mathcal{D}_{\text{buffer}})$$

**LLMでの適用：**

```
ベースモデル: 汎用データで訓練
特化モデル: 特化データ (80%) + 汎用データ (20%)

→ 汎用性能を維持しつつ特化
```

#### 2. 知識蒸留（Knowledge Distillation）

**アイデア：**

旧モデルの出力を教師信号として使用

$$\mathcal{L}_{\text{KD}} = \mathcal{L}_{\text{task}} + \lambda \cdot D_{\text{KL}}(P_{\text{old}} \| P_{\text{new}})$$

**プロセス：**

```
1. 旧モデル(θₛ)で予測: Pₒₗd
2. 新タスク訓練時、Pₒₗdも模倣
3. → 旧知識を保持
```

**温度付きソフトマックス：**

$$P_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}$$

温度 $T > 1$ で「ソフト」な分布を使用

#### 3. パラメータ隔離

**マルチタスク学習：**

タスクごとに専用パラメータを持つ

```
共有層: 全タスクで共通
  ↓
タスク固有層:
  ├─ タスクA用ヘッド
  ├─ タスクB用ヘッド
  └─ タスクC用ヘッド
```

**アダプタ（Adapter）：**

各タスクに小さなアダプタモジュールを追加

```
Transformer層
  ↓
[アダプタA] [アダプタB] [アダプタC]
  ↓
タスクAなら、アダプタAのみ使用
```

**利点：**

完全に忘却を回避（干渉なし）

**欠点：**

パラメータ数増加

### 8.4.3 実践的なレシピ

**GPT系モデルのファインチューニング：**

```python
# 推奨設定
learning_rate = 1e-5  # 事前学習の1/30
epochs = 3
batch_size = 32
warmup_steps = 100

# 正則化
weight_decay = 0.01
dropout = 0.1

# データミックス
target_data = 0.9
general_data = 0.1  # 汎用性能維持

# 早期終了
patience = 3  # 検証損失が3回悪化したら停止
```

**チェックポイント戦略：**

```
1. ベストモデル保存 (検証性能基準)
2. 定期保存 (各エポック終了時)
3. 最終モデル保存

→ 後で最良のものを選択
```

---

## 本章のまとめ

### 学んだこと

✅ **転移学習**
- ソースタスクからターゲットタスクへ
- サンプル効率、計算効率
- 理論的保証

✅ **ドメイン適応**
- ドメインシフトの問題
- 継続事前学習
- ハイブリッド戦略

✅ **破滅的忘却**
- 新タスク学習で旧タスク忘却
- 原因：重みの上書き
- 緩和策：小学習率、正則化、LoRA

✅ **継続学習**
- リハーサル（経験再生）
- 知識蒸留
- パラメータ隔離

### 重要な公式

* L2正則化FT 

$$\mathcal{L} = \mathcal{L}_{\text{task}} + \frac{\lambda}{2}\|\theta-\theta_s\|^2$$

* 知識蒸留 

$$\mathcal{L}_{\text{KD}} = \mathcal{L}_{\text{task}} + \lambda D_{\text{KL}}(P_{\text{old}}\|P_{\text{new}})$$

* 経験再生 

$$バッチ = 新データ + 旧データサンプル$$

### 実践的ガイドライン

**学習率：**
- 事前学習の 1/10 〜 1/100

**エポック数：**
- 通常 1-3 エポック

**データミックス：**
- ターゲット: 80-90%
- 汎用: 10-20%

**評価：**
- ターゲットタスク性能
- 汎用タスク性能（忘却チェック）

### 次章の予告

第9章では、**強化学習からの人間フィードバック（RLHF）**を学びます：
- 強化学習の基礎
- PPOアルゴリズム
- 報酬モデリング
- Constitutional AI

ChatGPTのような人間の好みに合わせたモデルを作る技術を理解していきます。

---

## 練習問題

### 問題1：サンプル複雑性
パラメータ次元 $d=10^6$、 $\theta_s$ から $\theta^*$ までの距離 $\Delta=10^3$ のとき、ファインチューニングは何倍サンプル効率的か？

### 問題2：学習率設定
事前学習の学習率が $3 \times 10^{-4}$ のとき、ファインチューニングの推奨学習率は？（1/30とする）

### 問題3：データミックス
ファインチューニングで1000サンプル/エポック使用し、10%を汎用データとする場合、各エポックで何サンプルずつ使うか？

### 問題4：知識蒸留
旧モデルの予測が $P_{\text{old}} = [0.7, 0.2, 0.1]$、新モデルが $P_{\text{new}} = [0.6, 0.3, 0.1]$ のとき、KLダイバージェンスは？

### 解答

**問題1:**
$$\frac{d}{\Delta} = \frac{10^6}{10^3} = 1000\text{倍}$$

**問題2:**
$$\eta_{\text{FT}} = \frac{3 \times 10^{-4}}{30} = 1 \times 10^{-5}$$

**問題3:**
ターゲット: $1000 \times 0.9 = 900$  
汎用: $1000 \times 0.1 = 100$

**問題4:**
$$D_{\text{KL}} = \sum_i p_i \log\frac{p_i}{q_i}$$
$$= 0.7\log\frac{0.7}{0.6} + 0.2\log\frac{0.2}{0.3} + 0.1\log\frac{0.1}{0.1}$$
$$\approx 0.7 \times 0.154 + 0.2 \times (-0.405) + 0$$
$$\approx 0.108 - 0.081 = 0.027$$

---

**📖 前章：[第7章 事前学習の原理](./第7章_事前学習の原理.md)**  
**📖 次章：[第9章 強化学習からの人間フィードバック](./第9章_強化学習からの人間フィードバック.md)**
