# 第6章:スケーリング法則の基礎

この章では、LLMの性能が**モデルサイズ、データ量、計算量**とどのように関係するかを学びます。「大きいほど良い」の科学的根拠を理解していきます。

---

## 6.1 パラメータ数と性能の関係

### 6.1.1 スケーリング法則の発見

**歴史的背景：**

2020年、OpenAIの研究により、LLMの性能が予測可能な法則に従うことが発見された

**論文：** "Scaling Laws for Neural Language Models" (Kaplan et al., 2020)

**主要な発見：**

> モデルの性能（損失）は、パラメータ数のべき乗則に従う

### 6.1.2 基本的なスケーリング法則

**数式：**

$$L(N) = \left(\frac{N_c}{N}\right)^{\alpha_N}$$

ここで：
- $L(N)$：モデルの損失（テスト損失）
- $N$：パラメータ数
- $N_c$：スケーリング定数
- $\alpha_N$：スケーリング指数（約0.076）

**対数スケールでの線形関係：**

$$\log L = -\alpha_N \log N + \text{const}$$

**視覚化：**

```
Loss (対数)
  ↑
  |●
  | ●
  |  ●
  |   ●
  |    ●
  |     ●
  |      ●________
  +───────────────→ Parameters (対数)
  1M 10M 100M 1B 10B

直線！（対数-対数プロット）
```

**具体例：**

| モデル | パラメータ数 | テスト損失 (PPL) |
|--------|--------------|------------------|
| GPT-2 Small | 117M | ~25 |
| GPT-2 Medium | 345M | ~20 |
| GPT-2 Large | 762M | ~17 |
| GPT-2 XL | 1.5B | ~15 |
| GPT-3 Small | 125M | ~18 |
| GPT-3 Medium | 350M | ~12 |
| GPT-3 Large | 760M | ~10 |
| GPT-3 XL | 1.3B | ~8.6 |
| GPT-3 2.7B | 2.7B | ~7.5 |
| GPT-3 6.7B | 6.7B | ~6.7 |
| GPT-3 13B | 13B | ~6.1 |
| GPT-3 175B | 175B | ~5.2 |

**観察：**

パラメータ数を10倍にすると、損失が約0.95^(1/0.076) ≈ 0.5倍に減少

### 6.1.3 べき乗則の直感的理解

**なぜべき乗則？**

**比喩：地図の精度**

```
1万円の予算:
  地図の精度: 1km
  
10万円の予算（10倍）:
  地図の精度: 500m（2倍改善）
  
100万円の予算（100倍）:
  地図の精度: 250m（4倍改善）

パラメータを増やしても、改善率は逓減
→ べき乗則
```

**数学的背景：**

複雑な関数空間での近似理論

- パラメータ数 $N$ → 表現できる関数のクラス
- より多くのパラメータ → より複雑な関数を表現
- しかし、収穫逓減の法則

### 6.1.4 飽和の不在

**驚くべき発見：**

> 実験した範囲（〜1兆パラメータ）で、飽和（性能の頭打ち）が見られない

**視覚化：**

```
Loss
  ↑
  |●
  | ●
  |  ●
  |   ●     既存のモデル
  |    ●
  |     ●
  |      ●
  |       ●
  |        ● 
  |         ●__ まだ改善中！
  +──────────────────→ Parameters
  1M      1B      1T    10T?

飽和点が見えない
```

**含意：**

さらに大きなモデルは、さらに良い性能を達成する可能性が高い

**ただし：**

- 計算コストの問題
- 実用性の限界
- データの制約（後述）

---

## 6.2 計算量スケーリングの理論

### 6.2.1 計算量の定義

**FLOPs（浮動小数点演算数）：**

モデルの訓練に必要な計算量の指標

**1回の順伝播のFLOPs：**

Transformerモデルの場合：

$$C_{\text{forward}} \approx 2NL$$

ここで：
- $N$：パラメータ数
- $L$：訓練データのトークン数（1バッチ）

**訓練全体のFLOPs：**

$$C = 6ND$$

ここで：
- $D$：訓練データの総トークン数
- 係数6：順伝播(2) + 逆伝播(4) ≈ 6

**具体例（GPT-3 175B）：**

- $N = 175 \times 10^9$
- $D = 300 \times 10^9$ トークン

$$C = 6 \times 175 \times 10^9 \times 300 \times 10^9 = 3.15 \times 10^{23} \text{ FLOPs}$$

約315ゼタFLOPs！

**必要な時間：**

V100 GPU（約125 TFLOPs）で：

$$\frac{3.15 \times 10^{23}}{125 \times 10^{12}} \approx 2.5 \times 10^9 \text{ 秒} \approx 80\text{年}$$

→ 数千のGPUで並列化が必須

### 6.2.2 計算量とパフォーマンスの関係

**スケーリング法則（計算量版）：**

$$L(C) = \left(\frac{C_c}{C}\right)^{\alpha_C}$$

ここで：
- $C$：計算量（FLOPs）
- $\alpha_C$：スケーリング指数（約0.050）

**重要な洞察：**

> 同じ性能を達成するには：  
> - 大きいモデル × 短い訓練  
> - 小さいモデル × 長い訓練  
> どちらも可能

**視覚化：**

```
Loss
  ↑
  |    A: 小モデル × 長時間訓練
  |   ●
  |    ╲
  |     ●
  |      ╲
  |       ●_____ 同じ損失
  |            ╱
  |          ●
  |        ╱ 
  |      ●
  |    ╱  B: 大モデル × 短時間訓練
  +──────────────→ Compute

異なる経路で同じ性能に到達
```

### 6.2.3 計算最適性

**問題：**

限られた計算予算 $C$ で、最良の性能を得るには？

**トレードオフ：**

```
選択肢A: 大きいモデル
  利点: 高い表現力
  欠点: 訓練データが少ない

選択肢B: 小さいモデル
  利点: 多くのデータで訓練
  欠点: 低い表現力
```

**Chinchilla論文（2022）の発見：**

従来のLLM（GPT-3など）は：
- モデルが大きすぎる
- データが少なすぎる

**最適な配分：**

$$N_{\text{opt}} \propto C^{0.5}$$
$$D_{\text{opt}} \propto C^{0.5}$$

つまり、パラメータ数とデータ量を同じペースで増やすべき！

**具体的な推奨：**

計算量 $C$ に対して：

$$N \approx \frac{C}{20D}$$

または

$$D \approx 20N$$

**例：**

| モデルサイズ | 推奨データ量 |
|--------------|--------------|
| 1B params | 20B tokens |
| 10B params | 200B tokens |
| 100B params | 2T tokens |
| 1T params | 20T tokens |

**GPT-3との比較：**

```
GPT-3 (175B):
  データ: 300B tokens
  推奨: 3.5T tokens
  → 約12倍不足！

Chinchilla (70B):
  データ: 1.4T tokens
  → ほぼ最適
  → GPT-3より小さいが性能は同等以上
```

### 6.2.4 LLaMAの戦略

**Meta の LLaMA シリーズ：**

Chinchillaの知見を活用

**LLaMA-1 (2023):**

| モデル | パラメータ | データ | 性能 |
|--------|-----------|--------|------|
| 7B | 7B | 1T tokens | GPT-3並み |
| 13B | 13B | 1T tokens | GPT-3超え |
| 33B | 33B | 1.4T tokens | - |
| 65B | 65B | 1.4T tokens | GPT-3.5並み |

**戦略：**
- 小さいモデル
- 多くのデータ
- 計算効率的

**結果：**
- オープンソース
- 手頃な推論コスト
- 高い性能

---

## 6.3 データスケーリングの統計的解析

### 6.3.1 データ量と性能の関係

**スケーリング法則（データ版）：**

$$L(D) = \left(\frac{D_c}{D}\right)^{\alpha_D}$$

ここで：
- $D$：訓練データ量（トークン数）
- $\alpha_D$：スケーリング指数（約0.095）

**視覚化：**

```
Loss
  ↑
  |●
  | ●
  |  ●
  |   ●
  |    ●
  |     ●_______
  +──────────────→ Data
  1M  10M 100M 1B 10B 100B

対数-対数プロットで直線
```

**観察：**

データ量を10倍にすると、損失が約10^(-0.095) ≈ 0.8倍に

### 6.3.2 データの質

**重要な発見：**

> データの「質」が「量」と同等以上に重要

**質の要因：**

1. **多様性**
   - 様々なドメイン（ニュース、書籍、論文、対話...）
   - 様々な文体
   - 様々な難易度

2. **正確性**
   - 誤情報の除去
   - 文法的正しさ
   - 事実の正確性

3. **関連性**
   - タスクに関連する内容
   - 有用な情報密度

**例：**

```
低品質データ 1TB:
  損失: 10.0

高品質データ 100GB:
  損失: 8.0

→ 10倍小さくても、より良い性能
```

### 6.3.3 データの重複除去

**問題：**

同じデータを繰り返し学習すると：
- 過学習
- 多様性の低下
- 性能の頭打ち

**解決策：**

厳密な重複除去

**方法：**

1. **完全一致の除去**
   - ハッシュ値での比較
   
2. **近似一致の除去**
   - MinHash, SimHash
   - 類似度閾値（例：90%以上）

3. **部分一致の除去**
   - N-gramの重複検出

**GPT-3の例：**

元のデータ: 45TB  
重複除去後: 570GB  
→ 約80倍の圧縮！

**効果：**

```
重複除去なし:
  データ: 1TB (重複多数)
  損失: 12.0

重複除去あり:
  データ: 200GB (高品質)
  損失: 9.0

→ より少ないデータでより良い性能
```

### 6.3.4 データミックスの最適化

**問題：**

どのドメインのデータをどれだけ含めるべきか？

**データソースの例：**

| ソース | 割合 | 特徴 |
|--------|------|------|
| Common Crawl | 60% | ウェブページ（質にばらつき） |
| Books | 16% | 長文、深い内容 |
| Wikipedia | 10% | 事実情報、高品質 |
| GitHub | 10% | コード |
| ArXiv | 4% | 学術論文 |

**最適化の考え方：**

1. **下流タスクの性能で評価**
   ```
   ミックスA → 質問応答: 85%, コード生成: 70%
   ミックスB → 質問応答: 80%, コード生成: 80%
   ```

2. **反復的な調整**
   - 小規模実験
   - 性能評価
   - ミックス調整

**LLaMAの戦略：**

- Common Crawl: 67%
- C4: 15%（Common Crawlのフィルタ版）
- GitHub: 4.5%
- Wikipedia: 4.5%
- Books: 4.5%
- ArXiv: 2.5%
- StackExchange: 2%

---

## 6.4 最適な資源配分の導出

### 6.4.1 等配分の原則

**Chinchillaの重要な結論：**

計算予算 $C$ が与えられたとき：

$$N \propto C^{a}, \quad D \propto C^{b}$$

最適値：$a = b = 0.5$

**意味：**

> パラメータ数とデータ量を**同じペース**で増やす

**数学的導出（簡略版）：**

計算量：$C = 6ND$

性能：$L = f(N, D)$（パラメータとデータの関数）

制約付き最適化：

$$\min_{N,D} L(N, D) \quad \text{subject to } C = 6ND$$

ラグランジュ乗数法を使用すると：

$$\frac{\partial L}{\partial N} \propto \frac{\partial L}{\partial D}$$

経験的に：$L(N,D) \approx L_0(N) + L_0(D)$

→ $N$ と $D$ を同じペースで増やすのが最適

### 6.4.2 実践的な推奨値

**計算予算別の推奨：**

| 計算予算 (FLOPs) | モデルサイズ | データ量 | 例 |
|------------------|--------------|----------|-----|
| $10^{18}$ | 100M | 2B tokens | 小規模実験 |
| $10^{21}$ | 1B | 20B tokens | 研究用 |
| $10^{22}$ | 3B | 60B tokens | - |
| $10^{23}$ | 10B | 200B tokens | 実用的LLM |
| $10^{24}$ | 30B | 600B tokens | GPT-3.5クラス |
| $10^{25}$ | 100B | 2T tokens | GPT-4クラス？ |

**GPUリソースとの対応：**

```
A100 GPU (312 TFLOPs):

1B モデル + 20B tokens:
  FLOPs: 6 × 1B × 20B = 1.2×10²¹
  時間: 1.2×10²¹ / (312×10¹²) ≈ 44日（1GPU）
  → 440 GPU-日

10B モデル + 200B tokens:
  FLOPs: 1.2×10²³
  → 44,000 GPU-日
  → 約120台 × 1年
```

### 6.4.3 トレードオフの実例

**ケーススタディ：7Bモデルの訓練**

**選択肢A：標準訓練**
- モデル: 7B パラメータ
- データ: 1T tokens（推奨の5倍）
- GPU: 256 × A100
- 期間: 約3週間
- コスト: 約$200K

**選択肢B：過剰訓練**
- モデル: 7B パラメータ  
- データ: 2T tokens（推奨の10倍）
- GPU: 256 × A100
- 期間: 約6週間
- コスト: 約$400K
- 性能改善: わずか（収穫逓減）

**選択肢C：最適化**
- モデル: 13B パラメータ
- データ: 1T tokens
- GPU: 512 × A100
- 期間: 約3週間
- コスト: 約$400K
- 性能改善: 大幅

**結論：**

同じ予算なら、選択肢Cが最良（大きいモデル）

### 6.4.4 継続事前学習のコスト

**ベースモデルから始める場合：**

既存モデル（例：LLaMA-7B）をファインチューニング

**継続訓練のコスト：**

```
元の訓練: 7B × 1T = 7×10²¹ FLOPs

追加訓練: 7B × 100B = 7×10²⁰ FLOPs
→ 元の10%

→ 大幅なコスト削減！
```

**用途：**
- ドメイン適応（医療、法律など）
- 言語適応（日本語、中国語など）
- 知識更新

**注意点：**
- 破滅的忘却のリスク
- 学習率の調整が重要

---

## 本章のまとめ

### 学んだこと

✅ **パラメータスケーリング**
- べき乗則：$L \propto N^{-\alpha}$
- 飽和が見られない
- 大きいほど良い（計算コストと要相談）

✅ **計算量スケーリング**
- $C = 6ND$（訓練のFLOPs）
- 計算量と性能の予測可能な関係
- 計算最適性の重要性

✅ **データスケーリング**
- データ量も重要
- 質 > 量
- 重複除去の重要性

✅ **最適配分（Chinchilla則）**
- $N \propto C^{0.5}$
- $D \propto C^{0.5}$
- パラメータとデータを均等に増やす

### 重要な法則

| 法則 | 公式 | 指数 |
|------|------|------|
| パラメータ則 | $L \propto N^{-\alpha_N}$ | $\alpha_N \approx 0.076$ |
| データ則 | $L \propto D^{-\alpha_D}$ | $\alpha_D \approx 0.095$ |
| 計算量則 | $L \propto C^{-\alpha_C}$ | $\alpha_C \approx 0.050$ |
| Chinchilla則 | $D \approx 20N$ | - |

### 実践的な含意

**モデル開発者向け：**

1. **予算を決める**
   - 利用可能なGPU時間
   - 計算予算（FLOPs）

2. **最適な配分を計算**
   - $N = \sqrt{C/120}$
   - $D = 20N$

3. **実装**
   - データの収集・品質管理
   - 効率的な訓練パイプライン

**研究者向け：**

1. **小規模実験**
   - スケーリング則の検証
   - ハイパーパラメータ探索

2. **性能予測**
   - より大きなモデルの性能を予測
   - ROI（投資対効果）の計算

3. **新しい発見**
   - スケーリング則の限界
   - 新しい訓練手法

### 次章の予告

第7章では、LLMの**事前学習の原理**を詳しく学びます：
- 最尤推定と言語モデリング
- クロスエントロピー損失
- Adam最適化
- 学習率スケジューリング
- 正則化手法

実際にLLMをゼロから訓練する際の詳細な理論と実践を理解していきます。

---

## 練習問題

### 問題1：スケーリング則
パラメータ数を1Bから10Bに増やすとき、損失はおよそ何倍になるか。$\alpha_N = 0.076$ とする。

### 問題2：計算量
30Bパラメータのモデルを500Bトークンで訓練するとき、必要なFLOPsを計算せよ。

### 問題3：Chinchilla則
計算予算が $10^{24}$ FLOPsのとき、推奨されるモデルサイズとデータ量は？

### 問題4：時間見積もり
100BパラメータのモデルをA100 GPU 1000台で訓練する場合、推奨データ量（2Tトークン）での訓練にかかる日数は？（A100は312 TFLOPs）

### 解答

**問題1:**
$$\frac{L(10B)}{L(1B)} = \left(\frac{1B}{10B}\right)^{0.076} = 10^{-0.076} \approx 0.839$$

約84%（16%改善）

**問題2:**
$$C = 6 \times 30 \times 10^9 \times 500 \times 10^9 = 9 \times 10^{22} \text{ FLOPs}$$

**問題3:**

$$N = \sqrt{\frac{10^{24}}{120}} \approx 91B \text{ パラメータ}$$
$$D = 20 \times 91B \approx 1.82T \text{ トークン}$$

**問題4:**

$$C = 6 \times 100B \times 2T = 1.2 \times 10^{24} \text{ FLOPs}$$

$$\text{時間} = \frac{1.2 \times 10^{24}}{1000 \times 312 \times 10^{12}} = 3.85 \times 10^{6} \text{秒}$$

$$\approx 44.5 \text{日}$$

約1.5ヶ月

---

**📖 前章：[第5章 自己回帰言語モデルの理論](./第5章_自己回帰言語モデルの理論.md)**  
**📖 次章：[第7章 事前学習の原理](./第7章_事前学習の原理.md)**
