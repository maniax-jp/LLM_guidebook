# 第10章：テキスト生成の理論

この章では、訓練済みLLMが実際にテキストを生成する際の**デコーディング戦略**と、生成品質の理論を学びます。

---

## 10.1 デコーディング戦略の分類

### 10.1.1 生成問題の定式化

**目標：**

プロンプト $x$ に対して、応答 $y = (y_1, y_2, \ldots, y_T)$ を生成

**モデルが提供するもの：**

条件付き確率分布

$$P_\theta(y_t | x, y_{<t})$$

各時刻 $t$ で、次のトークンの確率分布

**デコーディング：**

この確率分布から系列 $y$ を構築する方法

### 10.1.2 デコーディング戦略の分類

```
デコーディング戦略
│
├─ 決定的 (Deterministic)
│  ├─ 貪欲法 (Greedy)
│  └─ ビームサーチ (Beam Search)
│
└─ 確率的 (Stochastic)
   ├─ 祖先サンプリング (Ancestral Sampling)
   ├─ Top-k サンプリング
   ├─ Top-p (Nucleus) サンプリング
   └─ 温度付きサンプリング
```

**トレードオフ：**

| 決定的 | 確率的 |
|--------|--------|
| 再現可能 | 多様 |
| 安全 | 創造的 |
| 繰り返しやすい | 驚きあり |

---

## 10.2 貪欲法とビームサーチ

### 10.2.1 貪欲デコーディング

**アルゴリズム：**

各ステップで最も確率の高いトークンを選択

$$y_t = \arg\max_{w \in \mathcal{V}} P_\theta(w | x, y_{<t})$$

**疑似コード：**

```python
def greedy_decode(model, prompt, max_length):
    y = []
    for t in range(max_length):
        # 確率分布を取得
        probs = model(prompt + y)
        
        # 最大確率のトークンを選択
        next_token = argmax(probs)
        
        # 追加
        y.append(next_token)
        
        # 終了判定
        if next_token == EOS:
            break
    
    return y
```

**例：**

```
プロンプト: "猫は"

時刻1:
  P("動物"|"猫は") = 0.4  ← 最大
  P("可愛い"|"猫は") = 0.3
  P("鳴く"|"猫は") = 0.2
  選択: "動物"

時刻2:
  P("です"|"猫は動物") = 0.6  ← 最大
  P("の"|"猫は動物") = 0.3
  選択: "です"

結果: "猫は動物です"
```

**利点：**

- 高速（各ステップで1つだけ評価）
- 実装が簡単
- 決定的（再現可能）

**欠点：**

- 局所最適に陥る
- 繰り返しが多い
- 創造性に欠ける

**局所最適の例：**

```
貪欲: "The cat sat on the mat mat mat..."
最適: "The cat sat on the comfortable mat."

各ステップでは最良でも、全体では最適でない
```

### 10.2.2 ビームサーチ

**アイデア：**

上位 $k$ 個の候補を保持し、探索空間を広げる

**パラメータ：**

- ビーム幅 $k$：保持する候補数

**アルゴリズム：**

```
初期化: beams = [prompt]

For t = 1 to max_length:
  candidates = []
  
  For each beam in beams:
    # 次トークンの確率分布
    probs = model(beam)
    
    # 上位k個を取得
    top_k_tokens = get_top_k(probs, k)
    
    For each token in top_k_tokens:
      new_beam = beam + [token]
      score = log_prob(new_beam)
      candidates.append((new_beam, score))
  
  # 全候補からtop-kを選択
  beams = select_top_k(candidates, k)
  
Return best_beam
```

**視覚化：**

```
k=2 (ビーム幅2)

時刻0:
  "猫は"

時刻1:
  ├─ "猫は動物" (score: -0.9)
  └─ "猫は可愛い" (score: -1.2)

時刻2:
  ├─ "猫は動物です" (score: -1.4)  ← top-1
  ├─ "猫は動物の" (score: -2.1)
  ├─ "猫は可愛いです" (score: -1.8)  ← top-2
  └─ "猫は可愛い生き物" (score: -2.5)
  
  保持: 上位2つ

時刻3:
  ├─ "猫は動物です。" (score: -1.6)  ← 最終的に選択
  ├─ "猫は動物ですが" (score: -2.3)
  ├─ "猫は可愛いです。" (score: -2.1)
  └─ "猫は可愛いですね" (score: -2.4)
```

**スコアリング：**

系列のログ確率

$$\text{score}(y_{1:t}) = \sum_{i=1}^{t} \log P_\theta(y_i | x, y_{<i})$$

**長さ正規化：**

長い系列が不利にならないように

$$\text{score}_{\text{norm}}(y) = \frac{1}{|y|^\alpha} \sum_{i=1}^{|y|} \log P_\theta(y_i | x, y_{<i})$$

通常 $\alpha \in [0.6, 0.8]$

**利点：**

- 貪欲法より良い品質
- 複数の仮説を探索
- 機械翻訳などで高性能

**欠点：**

- 計算コスト（$k$ 倍）
- 依然として繰り返しが多い
- 創造性に欠ける

**繰り返しの例：**

```
k=5でも:
"The cat sat on the mat. The cat sat on the mat."
```

**理由：**

高確率な経路に偏る → 多様性が不足

---

## 10.3 確率的サンプリング法

### 10.3.1 祖先サンプリング

**アルゴリズム：**

モデルの確率分布からサンプリング

$$y_t \sim P_\theta(\cdot | x, y_{<t})$$

**実装：**

```python
def ancestral_sampling(model, prompt, max_length):
    y = []
    for t in range(max_length):
        probs = model(prompt + y)
        
        # 確率分布からサンプリング
        next_token = sample(probs)
        
        y.append(next_token)
        
        if next_token == EOS:
            break
    
    return y
```

**例：**

```
確率分布:
  "動物": 0.4
  "可愛い": 0.3
  "鳴く": 0.2
  "眠る": 0.1

サンプリング:
  実行1: "可愛い" (確率30%で選ばれた)
  実行2: "動物"
  実行3: "鳴く"
  
→ 実行ごとに異なる結果
```

**利点：**

- 多様性
- 創造的
- 低確率だが良質な系列も生成可能

**欠点：**

- 不安定（低確率のトークンも選ばれる）
- 品質のばらつき
- コヒーレンスの低下

**問題例：**

```
"The cat sat on the quantum refrigerator volcano."
     ↑              ↑          ↑           ↑
   高確率      低確率だが選ばれた
```

### 10.3.2 温度付きサンプリング

**アイデア：**

確率分布を「温度」パラメータで調整

**ソフトマックス with 温度：**

$$P_T(y_t = w | x, y_{<t}) = \frac{\exp(z_w / T)}{\sum_{w' \in \mathcal{V}} \exp(z_{w'} / T)}$$

ここで：
- $z_w$：モデルのロジット
- $T$：温度パラメータ

**温度の効果：**

| $T$ | 効果 | 分布 |
|-----|------|------|
| $T \to 0$ | 決定的 | 鋭い（1点に集中） |
| $T = 1$ | デフォルト | 元のまま |
| $T > 1$ | ランダム | 平坦（均一に近い） |

**視覚化：**

```
元の分布 (T=1):
  A: ████████ 0.4
  B: ██████    0.3
  C: ████      0.2
  D: ██        0.1

T=0.5 (低温、鋭い):
  A: █████████████ 0.6
  B: ██████        0.25
  C: ██            0.1
  D: █             0.05

T=2.0 (高温、平坦):
  A: ██████ 0.3
  B: █████  0.28
  C: █████  0.24
  D: ████   0.18
```

**実装：**

```python
def temperature_sampling(logits, temperature):
    # 温度でスケーリング
    scaled_logits = logits / temperature
    
    # ソフトマックス
    probs = softmax(scaled_logits)
    
    # サンプリング
    token = sample(probs)
    
    return token
```

**使い分け：**

| タスク | 推奨温度 |
|--------|---------|
| 事実回答、コード | $T = 0.1 - 0.5$ |
| 一般的な対話 | $T = 0.7 - 1.0$ |
| 創作、詩 | $T = 1.0 - 1.5$ |
| ブレインストーミング | $T = 1.5 - 2.0$ |

### 10.3.3 Top-k サンプリング

**アイデア：**

上位 $k$ 個のトークンのみからサンプリング

**アルゴリズム：**

```python
def top_k_sampling(logits, k):
    # 上位k個のインデックスを取得
    top_k_indices = argsort(logits)[-k:]
    
    # それ以外をマスク
    masked_logits = -inf for all indices
    masked_logits[top_k_indices] = logits[top_k_indices]
    
    # ソフトマックス（top-k内で正規化）
    probs = softmax(masked_logits)
    
    # サンプリング
    token = sample(probs)
    
    return token
```

**例：**

```
k=3

元の確率:
  "動物": 0.4
  "可愛い": 0.3
  "鳴く": 0.2
  "眠る": 0.06
  "走る": 0.04
  ...

Top-3でフィルタ後（再正規化）:
  "動物": 0.44  (0.4/0.9)
  "可愛い": 0.33  (0.3/0.9)
  "鳴く": 0.22  (0.2/0.9)
  "眠る": 0  (除外)
  "走る": 0  (除外)
  ...

→ 上位3つからサンプリング
```

**利点：**

- 低品質なトークンを除外
- 多様性を保持
- 安定した品質

**欠点：**

- $k$ の選択が難しい
- 文脈によって適切な $k$ が異なる

**問題例：**

```
確率分布1（確信が高い）:
  "です": 0.95
  "ます": 0.03
  "でした": 0.02
  
→ k=10 では不要なトークンまで含む

確率分布2（不確実性が高い）:
  "赤": 0.15
  "青": 0.14
  "緑": 0.13
  ...
  
→ k=3 では選択肢が狭すぎる
```

### 10.3.4 Top-p（Nucleus）サンプリング

**アイデア：**

累積確率が $p$ を超える最小の集合からサンプリング

**アルゴリズム：**

```python
def top_p_sampling(logits, p):
    # 確率を降順でソート
    probs = softmax(logits)
    sorted_probs, sorted_indices = sort(probs, descending=True)
    
    # 累積確率を計算
    cumulative_probs = cumsum(sorted_probs)
    
    # 累積確率がpを超えるまでのインデックス
    nucleus_size = argmax(cumulative_probs >= p) + 1
    nucleus_indices = sorted_indices[:nucleus_size]
    
    # Nucleus内で再正規化
    nucleus_probs = probs[nucleus_indices]
    nucleus_probs /= sum(nucleus_probs)
    
    # サンプリング
    token = sample(nucleus_probs)
    
    return token
```

**例：**

```
p=0.9

元の確率（降順）:
  "動物": 0.4    累積: 0.4
  "可愛い": 0.3  累積: 0.7
  "鳴く": 0.2    累積: 0.9  ← ここまで（p=0.9到達）
  "眠る": 0.06   累積: 0.96
  "走る": 0.04   累積: 1.0

Nucleus（上位3つ）:
  "動物": 0.44
  "可愛い": 0.33
  "鳴く": 0.22

→ この3つからサンプリング
```

**動的なサイズ調整：**

```
確率分布1（確信が高い）:
  "です": 0.95
  
→ Nucleus サイズ = 1 (適応的に小さい)

確率分布2（不確実性が高い）:
  "赤": 0.15, "青": 0.14, "緑": 0.13, ...
  
→ Nucleus サイズ = 10+ (適応的に大きい)
```

**利点：**

- 動的にサイズ調整
- 安定した品質
- 多様性と品質のバランス

**欠点：**

- $p$ の選択が重要
- 実装が複雑

**推奨値：**

一般的に $p = 0.9$ が良いバランス

### 10.3.5 ハイブリッド戦略

**Top-k + Top-p:**

両方を組み合わせる

```python
def top_k_top_p_sampling(logits, k, p, temperature):
    # 温度でスケーリング
    logits = logits / temperature
    
    # Top-k フィルタ
    top_k_logits = apply_top_k(logits, k)
    
    # Top-p フィルタ
    top_p_logits = apply_top_p(top_k_logits, p)
    
    # サンプリング
    token = sample(softmax(top_p_logits))
    
    return token
```

**典型的な設定：**

```
創造的タスク:
  temperature = 0.8
  top_k = 40
  top_p = 0.9

事実的タスク:
  temperature = 0.3
  top_k = 10
  top_p = 0.95
```

---

## 10.4 生成品質の評価指標

### 10.4.1 パープレキシティ（復習）

**定義：**

$$\text{PPL}(y) = \exp\left(-\frac{1}{|y|} \sum_{t=1}^{|y|} \log P_\theta(y_t | y_{<t})\right)$$

**意味：**

モデルがテキストをどれだけ「驚く」か

**利点：**

- 計算が容易
- 参照テキスト不要

**欠点：**

- 人間の評価と相関が低い
- 生成品質を直接測らない

### 10.4.2 BLEU（機械翻訳）

**アイデア：**

生成テキストと参照テキストの n-gram 一致率

**定義：**

$$\text{BLEU} = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)$$

ここで：
- $p_n$：n-gram精度
- $BP$：短いペナルティ（Brevity Penalty）
- $w_n$：重み（通常 $1/N$）

**n-gram精度：**

$$p_n = \frac{\text{一致したn-gram数}}{\text{生成テキストのn-gram数}}$$

**例：**

```
参照: "The cat sat on the mat"
生成: "The cat on the mat"

1-gram: 5/5 = 1.0  (The, cat, on, the, mat)
2-gram: 3/4 = 0.75  (The cat, on the, the mat)
3-gram: 1/3 = 0.33  (the mat のみ一致)
4-gram: 0/2 = 0

BLEU-4 = exp((log 1.0 + log 0.75 + log 0.33 + log 0.001)/4)
       ≈ 0.30
```

**欠点：**

- 意味を考慮しない
- 参照テキストが必要
- 創造的タスクには不向き

### 10.4.3 ROUGE（要約）

**種類：**

- **ROUGE-N**：n-gram再現率
- **ROUGE-L**：最長共通部分列（LCS）
- **ROUGE-S**：スキップバイグラム

**ROUGE-L：**

$$\text{ROUGE-L} = \frac{\text{LCS}(\text{参照}, \text{生成})}{\text{参照の長さ}}$$

**例：**

```
参照: "A B C D E"
生成: "A C D F"

LCS: "A C D" (長さ3)
ROUGE-L = 3/5 = 0.6
```

**BLEU vs ROUGE：**

| 指標 | 重視 | 用途 |
|------|------|------|
| BLEU | 精度 | 翻訳 |
| ROUGE | 再現率 | 要約 |

### 10.4.4 BERTScore

**アイデア：**

BERT埋め込みを使った意味的類似度

**アルゴリズム：**

```
1. 参照と生成をBERTでエンコード
   参照トークン: r₁, r₂, ..., rₙ
   生成トークン: c₁, c₂, ..., cₘ

2. 各トークンペアのコサイン類似度
   sim(rᵢ, cⱼ) = cos(embed(rᵢ), embed(cⱼ))

3. 各生成トークンについて最大類似度
   Precision: Σⱼ maxᵢ sim(rᵢ, cⱼ) / m
   Recall: Σᵢ maxⱼ sim(rᵢ, cⱼ) / n
   F1: 2·P·R / (P+R)
```

**利点：**

- 意味を考慮
- パラフレーズを評価可能

**例：**

```
参照: "The cat is sleeping"
生成: "The feline is resting"

n-gram一致: 低い（"The" のみ）
BERTScore: 高い（意味が類似）
```

### 10.4.5 人間評価

**評価次元：**

1. **流暢性（Fluency）**
   - 文法的に正しいか
   - 自然な文章か

2. **関連性（Relevance）**
   - プロンプトに対応しているか

3. **一貫性（Coherence）**
   - 論理的に一貫しているか

4. **事実性（Factuality）**
   - 事実として正確か

5. **有用性（Helpfulness）**
   - タスクに役立つか

**評価方法：**

**A. リッカート尺度：**

```
流暢性: 1 (非常に悪い) 〜 5 (非常に良い)
```

**B. ペアワイズ比較：**

```
応答Aと応答B、どちらが良いか？
  A > B
  A = B
  A < B
```

**C. Win率：**

$$\text{Win率} = \frac{\text{勝った回数}}{\text{総比較回数}}$$

**利点：**

- 最も信頼性が高い
- 微妙なニュアンスを評価

**欠点：**

- コストが高い
- スケールしない
- 主観的

### 10.4.6 自動評価 vs 人間評価

**ベストプラクティス：**

```
開発フェーズ:
  自動評価（高速、安価）
  ↓
最終評価:
  人間評価（高品質、信頼性）
```

**相関研究：**

自動評価指標と人間評価の相関

| 指標 | 相関 |
|------|------|
| BLEU | 中程度（0.4-0.6） |
| ROUGE | 中程度（0.5-0.7） |
| BERTScore | 高い（0.6-0.8） |
| GPT-4評価 | 非常に高い（0.8-0.9） |

**LLM as Judge：**

強力なLLM（GPT-4など）を評価者として使用

```python
prompt = f"""
以下の応答を評価してください（1-5）：

質問: {question}
応答: {answer}

評価基準:
- 関連性
- 正確性
- 有用性

スコア:
"""

score = gpt4(prompt)
```

人間評価に近い結果を低コストで実現

---

## 10.5 デコーディングの理論的解析

### 10.5.1 最尤デコーディングの問題

**最尤デコーディング：**

$$y^* = \arg\max_y P_\theta(y | x)$$

**問題：**

1. **計算不可能**
   - 可能な系列数：$|\mathcal{V}|^T$（指数的）
   - 例：語彙50000、長さ100 → $50000^{100}$

2. **ビームサーチは近似**
   - 最適解を保証しない

3. **最尤 ≠ 最良**
   - 高確率でも繰り返しや退屈な文章

**繰り返しの理論的説明：**

ある単語 $w$ が繰り返される確率

$$P(w, w | \text{context}) = P(w | \text{context}) \cdot P(w | \text{context}, w)$$

後者が高い（$w$ の後に $w$ が来やすい）と繰り返す

### 10.5.2 確率と品質のギャップ

**観察：**

人間が書くテキストは、モデルにとって高確率ではない

**実験結果：**

```
人間の文章: PPL = 20-30
貪欲デコード: PPL = 10-15 (より高確率！)

しかし:
  人間の文章: 評価 = 高い
  貪欲デコード: 評価 = 低い（繰り返し多い）
```

**理由：**

訓練目標（尤度）と生成目標（品質）のミスマッチ

### 10.5.3 エントロピーと多様性

**予測分布のエントロピー：**

$$H(P_t) = -\sum_w P_\theta(w | x, y_{<t}) \log P_\theta(w | x, y_{<t})$$

**観察：**

```
人間の選択:
  エントロピー: 高い（多様）
  
貪欲デコード:
  エントロピー: 低い（保守的）
```

**推奨：**

サンプリング法で適度な多様性を確保

### 10.5.4 最適なデコーディング戦略

**タスク依存：**

| タスク | 推奨戦略 | 理由 |
|--------|---------|------|
| 機械翻訳 | ビームサーチ (k=4-5) | 正確性重視 |
| 要約 | ビームサーチ + 正則化 | 忠実性重視 |
| 対話 | Top-p (p=0.9) + 温度 | 多様性・自然さ |
| 創作 | Top-p (p=0.95) + 高温度 | 創造性 |
| コード生成 | 貪欲 or 低温度 | 正確性重視 |
| 推論 | ビームサーチ | 論理性 |

---

## 本章のまとめ

### 学んだこと

✅ **デコーディング戦略**
- 貪欲法：高速だが局所最適
- ビームサーチ：より良いが計算コスト高
- サンプリング法：多様性と創造性

✅ **サンプリング技術**
- 温度：分布の鋭さを調整
- Top-k：上位k個から選択
- Top-p：動的なカットオフ

✅ **評価指標**
- BLEU, ROUGE：n-gram一致
- BERTScore：意味的類似度
- 人間評価：最も信頼性が高い

✅ **理論的洞察**
- 最尤 ≠ 最良
- エントロピーと多様性
- タスク依存の戦略選択

### デコーディング戦略の選択

```
決定的タスク（翻訳、要約）:
  ビームサーチ (k=4-5)

創造的タスク（対話、創作）:
  Top-p (p=0.9) + 温度 (T=0.7-1.0)

コード・推論:
  貪欲 or 低温度 (T=0.1-0.3)
```

### 重要なパラメータ

| パラメータ | 典型値 | 効果 |
|-----------|--------|------|
| 温度 $T$ | 0.7-1.0 | 多様性調整 |
| Top-k | 40-50 | 選択肢の数 |
| Top-p | 0.9-0.95 | 動的フィルタ |
| ビーム幅 $k$ | 4-5 | 探索の広さ |

### 次章の予告

第11章では、**プロンプトエンジニアリングの理論**を学びます：
- In-Context Learning
- Chain-of-Thought
- Few-Shot Learning
- プロンプト設計の原則

LLMの能力を最大限引き出す技術を理解していきます。

---

## 練習問題

### 問題1：温度の効果
元のロジットが $[2.0, 1.0, 0.5]$ のとき、温度 $T=0.5$ と $T=2.0$ でのソフトマックス確率を計算せよ。

### 問題2：Top-p サンプリング
確率分布が $[0.5, 0.3, 0.15, 0.05]$ のとき、$p=0.8$ でのNucleusサイズは？

### 問題3：BLEU
参照："A B C D"、生成："A B D" のとき、1-gram と 2-gram 精度は？

### 問題4：ビームサーチ
ビーム幅 $k=2$、語彙サイズ $|\mathcal{V}|=50000$、長さ $T=10$ のとき、評価する系列数は最大何個？

### 解答

**問題1:**

$T=0.5$:
$$z' = [4.0, 2.0, 1.0]$$
$$P = [0.73, 0.24, 0.03]$$
(鋭い)

$T=2.0$:
$$z' = [1.0, 0.5, 0.25]$$
$$P = [0.42, 0.33, 0.25]$$
 (平坦)

**問題2:**

累積確率：
- $0.5$ (第1要素)
- $0.8$ (第2要素) ← $p=0.8$ 到達

Nucleusサイズ = 2

**問題3:**

1-gram: $3/3 = 1.0$ (A, B, D すべて一致)  
2-gram: $1/2 = 0.5$ (A B のみ一致、B D は不一致)

**問題4:**

各ステップで $k$ 個の候補、各候補から $k$ 個拡張  
総評価数 ≤ $k^2 \cdot T = 2^2 \times 10 = 40$

（実際はEOSで早期終了する可能性あり）

---

**📖 前章：[第9章 強化学習からの人間フィードバック](./第9章_強化学習からの人間フィードバック.md)**  
**📖 次章：[第11章 プロンプトエンジニアリングの理論](./第11章_プロンプトエンジニアリングの理論.md)**
