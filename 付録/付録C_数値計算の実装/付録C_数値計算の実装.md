# ä»˜éŒ²Cï¼šæ•°å€¤è¨ˆç®—ã®å®Ÿè£…

ç†è«–ã‚’å®Ÿè·µã«ç§»ã™ãŸã‚ã®Pythonå®Ÿè£…ä¾‹ã‚’æä¾›ã—ã¾ã™ã€‚

---

## C.1 åŸºæœ¬çš„ãªæ•°å­¦é–¢æ•°

### C.1.1 æ´»æ€§åŒ–é–¢æ•°

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    """ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°"""
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    """ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ã®å°é–¢æ•°"""
    s = sigmoid(x)
    return s * (1 - s)

def relu(x):
    """ReLUé–¢æ•°"""
    return np.maximum(0, x)

def relu_derivative(x):
    """ReLUã®å°é–¢æ•°"""
    return (x > 0).astype(float)

def gelu(x):
    """GELUé–¢æ•°ï¼ˆè¿‘ä¼¼ç‰ˆï¼‰"""
    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))

def gelu_exact(x):
    """GELUé–¢æ•°ï¼ˆæ­£ç¢ºç‰ˆï¼‰"""
    from scipy.stats import norm
    return x * norm.cdf(x)

def softmax(x, axis=-1):
    """Softmaxé–¢æ•°ï¼ˆæ•°å€¤å®‰å®šç‰ˆï¼‰"""
    # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢
    x_max = np.max(x, axis=axis, keepdims=True)
    exp_x = np.exp(x - x_max)
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

# å¯è¦–åŒ–
x = np.linspace(-5, 5, 100)
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.plot(x, sigmoid(x), label='Sigmoid')
plt.plot(x, np.tanh(x), label='Tanh')
plt.legend()
plt.grid(True)
plt.title('Sigmoid & Tanh')

plt.subplot(1, 3, 2)
plt.plot(x, relu(x), label='ReLU')
plt.plot(x, np.maximum(0, x) * np.minimum(1, x/6 + 0.5), label='HardSwish')
plt.legend()
plt.grid(True)
plt.title('ReLU & Variants')

plt.subplot(1, 3, 3)
plt.plot(x, gelu(x), label='GELU')
plt.plot(x, x * sigmoid(x), label='Swish/SiLU')
plt.legend()
plt.grid(True)
plt.title('Modern Activations')

plt.tight_layout()
# plt.savefig('activations.png')
plt.show()
```

### C.1.2 æå¤±é–¢æ•°

```python
def cross_entropy(y_true, y_pred, epsilon=1e-12):
    """
    ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±
    
    Args:
        y_true: çœŸã®ãƒ©ãƒ™ãƒ« (one-hot or labels)
        y_pred: äºˆæ¸¬ç¢ºç‡
        epsilon: æ•°å€¤å®‰å®šåŒ–
    """
    # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã§ log(0) ã‚’é˜²æ­¢
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    
    if len(y_true.shape) == 1:  # ãƒ©ãƒ™ãƒ«å½¢å¼
        n_samples = y_true.shape[0]
        return -np.sum(np.log(y_pred[np.arange(n_samples), y_true])) / n_samples
    else:  # one-hotå½¢å¼
        return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]

def mse_loss(y_true, y_pred):
    """å¹³å‡äºŒä¹—èª¤å·®"""
    return np.mean((y_true - y_pred)**2)

def perplexity(cross_entropy_loss):
    """ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£"""
    return np.exp(cross_entropy_loss)

# ä¾‹
y_true = np.array([1, 0, 2])  # ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«
y_pred = np.array([
    [0.1, 0.7, 0.2],  # ã‚µãƒ³ãƒ—ãƒ«1ã®äºˆæ¸¬
    [0.8, 0.1, 0.1],  # ã‚µãƒ³ãƒ—ãƒ«2ã®äºˆæ¸¬
    [0.1, 0.2, 0.7]   # ã‚µãƒ³ãƒ—ãƒ«3ã®äºˆæ¸¬
])

loss = cross_entropy(y_true, y_pred)
ppl = perplexity(loss)
print(f"Cross Entropy: {loss:.4f}")
print(f"Perplexity: {ppl:.4f}")
```

---

## C.2 ç·šå½¢ä»£æ•°ã®å®Ÿè£…

### C.2.1 è¡Œåˆ—æ¼”ç®—

```python
def matrix_multiply(A, B):
    """è¡Œåˆ—ç©ï¼ˆæ•™è‚²ç”¨ãƒ»ä½é€Ÿï¼‰"""
    m, n = A.shape
    n2, p = B.shape
    assert n == n2, "æ¬¡å…ƒãŒåˆã„ã¾ã›ã‚“"
    
    C = np.zeros((m, p))
    for i in range(m):
        for j in range(p):
            for k in range(n):
                C[i, j] += A[i, k] * B[k, j]
    return C

def batch_matrix_multiply(A, B):
    """ãƒãƒƒãƒè¡Œåˆ—ç©"""
    # A: (batch, m, n)
    # B: (batch, n, p)
    # å‡ºåŠ›: (batch, m, p)
    return np.einsum('bmn,bnp->bmp', A, B)

# NumPy ã®é«˜é€Ÿå®Ÿè£…ã‚’ä½¿ã†æ–¹ãŒå®Ÿç”¨çš„
A = np.random.randn(3, 4)
B = np.random.randn(4, 5)
C = A @ B  # Python 3.5+
# ã¾ãŸã¯
C = np.dot(A, B)
```

### C.2.2 SVD ã¨ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼

```python
def low_rank_approximation(A, k):
    """
    è¡Œåˆ—ã®ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ï¼ˆLoRAã®åŸºç¤ï¼‰
    
    Args:
        A: å…¥åŠ›è¡Œåˆ— (m, n)
        k: ãƒ©ãƒ³ã‚¯
    
    Returns:
        A_k: ãƒ©ãƒ³ã‚¯kè¿‘ä¼¼
        U_k, S_k, Vt_k: SVDæˆåˆ†
    """
    U, S, Vt = np.linalg.svd(A, full_matrices=False)
    
    # ä¸Šä½kå€‹ã®ç‰¹ç•°å€¤ã®ã¿ä½¿ç”¨
    U_k = U[:, :k]
    S_k = S[:k]
    Vt_k = Vt[:k, :]
    
    # å†æ§‹æˆ
    A_k = U_k @ np.diag(S_k) @ Vt_k
    
    return A_k, U_k, S_k, Vt_k

# ä¾‹ï¼šç”»åƒã®åœ§ç¸®
from PIL import Image

# ãƒ€ãƒŸãƒ¼ç”»åƒï¼ˆå®Ÿéš›ã«ã¯Image.open()ã§èª­ã¿è¾¼ã¿ï¼‰
img_array = np.random.rand(100, 100)

# æ§˜ã€…ãªãƒ©ãƒ³ã‚¯ã§è¿‘ä¼¼
ranks = [5, 10, 20, 50]
fig, axes = plt.subplots(1, len(ranks)+1, figsize=(15, 3))

axes[0].imshow(img_array, cmap='gray')
axes[0].set_title('Original')
axes[0].axis('off')

for idx, k in enumerate(ranks):
    A_k, _, _, _ = low_rank_approximation(img_array, k)
    axes[idx+1].imshow(A_k, cmap='gray')
    axes[idx+1].set_title(f'Rank {k}')
    axes[idx+1].axis('off')

plt.tight_layout()
# plt.savefig('svd_compression.png')
plt.show()
```

### C.2.3 ãƒãƒ«ãƒ ã¨è·é›¢

```python
def compute_norms(v):
    """å„ç¨®ãƒãƒ«ãƒ ã®è¨ˆç®—"""
    l1 = np.linalg.norm(v, ord=1)  # L1ãƒãƒ«ãƒ 
    l2 = np.linalg.norm(v, ord=2)  # L2ãƒãƒ«ãƒ 
    linf = np.linalg.norm(v, ord=np.inf)  # Lâˆãƒãƒ«ãƒ 
    
    return {
        'L1': l1,
        'L2': l2,
        'L_inf': linf
    }

def cosine_similarity(a, b):
    """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def euclidean_distance(a, b):
    """ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢"""
    return np.linalg.norm(a - b)

# ä¾‹
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])

print("Norms of v1:", compute_norms(v1))
print(f"Cosine similarity: {cosine_similarity(v1, v2):.4f}")
print(f"Euclidean distance: {euclidean_distance(v1, v2):.4f}")
```

---

## C.3 Attentionæ©Ÿæ§‹ã®å®Ÿè£…

### C.3.1 Scaled Dot-Product Attention

```python
def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Scaled Dot-Product Attention
    
    Args:
        Q: Query (batch, seq_len, d_k)
        K: Key (batch, seq_len, d_k)
        V: Value (batch, seq_len, d_v)
        mask: ãƒã‚¹ã‚¯ (batch, seq_len, seq_len) ã¾ãŸã¯ None
    
    Returns:
        output: (batch, seq_len, d_v)
        attention_weights: (batch, seq_len, seq_len)
    """
    d_k = Q.shape[-1]
    
    # ã‚¹ã‚³ã‚¢è¨ˆç®—: (batch, seq_len, seq_len)
    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)
    
    # ãƒã‚¹ã‚¯é©ç”¨
    if mask is not None:
        scores = np.where(mask == 0, -1e9, scores)
    
    # Softmax
    attention_weights = softmax(scores, axis=-1)
    
    # é‡ã¿ä»˜ãå’Œ
    output = np.matmul(attention_weights, V)
    
    return output, attention_weights

# ä¾‹
batch_size = 2
seq_len = 4
d_k = 8
d_v = 8

Q = np.random.randn(batch_size, seq_len, d_k)
K = np.random.randn(batch_size, seq_len, d_k)
V = np.random.randn(batch_size, seq_len, d_v)

# Causal maskï¼ˆè‡ªå·±å›å¸°ç”¨ï¼‰
causal_mask = np.tril(np.ones((seq_len, seq_len)))
causal_mask = causal_mask[np.newaxis, :, :]  # (1, seq_len, seq_len)

output, attn_weights = scaled_dot_product_attention(Q, K, V, causal_mask)

print("Output shape:", output.shape)
print("Attention weights shape:", attn_weights.shape)
print("\nAttention pattern (sample 0):")
print(attn_weights[0])
```

### C.3.2 Multi-Head Attention

```python
class MultiHeadAttention:
    def __init__(self, d_model, num_heads):
        """
        Multi-Head Attention
        
        Args:
            d_model: ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ
            num_heads: ãƒ˜ãƒƒãƒ‰æ•°
        """
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # é‡ã¿è¡Œåˆ—ï¼ˆç°¡ç•¥åŒ–ã®ãŸã‚ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ï¼‰
        self.W_q = np.random.randn(d_model, d_model) * 0.01
        self.W_k = np.random.randn(d_model, d_model) * 0.01
        self.W_v = np.random.randn(d_model, d_model) * 0.01
        self.W_o = np.random.randn(d_model, d_model) * 0.01
    
    def split_heads(self, x):
        """(batch, seq_len, d_model) â†’ (batch, num_heads, seq_len, d_k)"""
        batch_size, seq_len, d_model = x.shape
        x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)
        return x.transpose(0, 2, 1, 3)
    
    def combine_heads(self, x):
        """(batch, num_heads, seq_len, d_k) â†’ (batch, seq_len, d_model)"""
        batch_size, num_heads, seq_len, d_k = x.shape
        x = x.transpose(0, 2, 1, 3)
        return x.reshape(batch_size, seq_len, self.d_model)
    
    def forward(self, x, mask=None):
        """
        Forward pass
        
        Args:
            x: (batch, seq_len, d_model)
            mask: (batch, seq_len, seq_len) ã¾ãŸã¯ None
        
        Returns:
            output: (batch, seq_len, d_model)
        """
        batch_size, seq_len, _ = x.shape
        
        # Linear projections
        Q = x @ self.W_q  # (batch, seq_len, d_model)
        K = x @ self.W_k
        V = x @ self.W_v
        
        # Split into multiple heads
        Q = self.split_heads(Q)  # (batch, num_heads, seq_len, d_k)
        K = self.split_heads(K)
        V = self.split_heads(V)
        
        # Scaled dot-product attention for each head
        d_k = self.d_k
        scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(d_k)
        
        if mask is not None:
            # Broadcast mask for all heads
            mask = mask[:, np.newaxis, :, :]
            scores = np.where(mask == 0, -1e9, scores)
        
        attn_weights = softmax(scores, axis=-1)
        attn_output = np.matmul(attn_weights, V)
        
        # Combine heads
        attn_output = self.combine_heads(attn_output)
        
        # Final linear projection
        output = attn_output @ self.W_o
        
        return output

# ä¾‹
d_model = 512
num_heads = 8
batch_size = 2
seq_len = 10

mha = MultiHeadAttention(d_model, num_heads)
x = np.random.randn(batch_size, seq_len, d_model)

output = mha.forward(x)
print("Multi-Head Attention output shape:", output.shape)
```

---

## C.4 ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

### C.4.1 æ­£å¼¦æ³¢ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

```python
def positional_encoding(max_len, d_model):
    """
    æ­£å¼¦æ³¢ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    
    Args:
        max_len: æœ€å¤§ç³»åˆ—é•·
        d_model: ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ
    
    Returns:
        pe: (max_len, d_model)
    """
    pe = np.zeros((max_len, d_model))
    position = np.arange(0, max_len)[:, np.newaxis]
    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    
    pe[:, 0::2] = np.sin(position * div_term)
    pe[:, 1::2] = np.cos(position * div_term)
    
    return pe

# å¯è¦–åŒ–
max_len = 100
d_model = 128

pe = positional_encoding(max_len, d_model)

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.imshow(pe, cmap='RdBu', aspect='auto')
plt.colorbar()
plt.xlabel('Dimension')
plt.ylabel('Position')
plt.title('Positional Encoding')

plt.subplot(1, 2, 2)
# ã„ãã¤ã‹ã®æ¬¡å…ƒã‚’ãƒ—ãƒ­ãƒƒãƒˆ
for i in [0, 1, 10, 50]:
    plt.plot(pe[:, i], label=f'dim {i}')
plt.xlabel('Position')
plt.ylabel('Value')
plt.legend()
plt.title('PE by Dimension')
plt.grid(True)

plt.tight_layout()
# plt.savefig('positional_encoding.png')
plt.show()
```

### C.4.2 å›è»¢ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆRoPEï¼‰

```python
def rope(q, k, positions, d_model):
    """
    Rotary Position Embedding (RoPE)
    
    Args:
        q, k: Query, Key vectors (batch, seq_len, d_model)
        positions: ä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (seq_len,)
        d_model: ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ
    
    Returns:
        q_rope, k_rope: å›è»¢é©ç”¨å¾Œ
    """
    def rotate_half(x):
        # (batch, seq_len, d_model) â†’ å‰åŠã¨å¾ŒåŠã‚’å…¥ã‚Œæ›¿ãˆ
        x1 = x[..., :d_model//2]
        x2 = x[..., d_model//2:]
        return np.concatenate([-x2, x1], axis=-1)
    
    # å‘¨æ³¢æ•°è¨ˆç®—
    inv_freq = 1.0 / (10000 ** (np.arange(0, d_model, 2) / d_model))
    
    # ä½ç½®ã”ã¨ã®è§’åº¦
    angles = np.outer(positions, inv_freq)  # (seq_len, d_model/2)
    
    # cos, sin
    cos = np.cos(angles)  # (seq_len, d_model/2)
    sin = np.sin(angles)
    
    # ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆç”¨ã«æ‹¡å¼µ
    cos = np.repeat(cos, 2, axis=-1)[np.newaxis, :, :]  # (1, seq_len, d_model)
    sin = np.repeat(sin, 2, axis=-1)[np.newaxis, :, :]
    
    # å›è»¢é©ç”¨
    q_rope = q * cos + rotate_half(q) * sin
    k_rope = k * cos + rotate_half(k) * sin
    
    return q_rope, k_rope

# ä¾‹
batch = 1
seq_len = 10
d_model = 64

q = np.random.randn(batch, seq_len, d_model)
k = np.random.randn(batch, seq_len, d_model)
positions = np.arange(seq_len)

q_rope, k_rope = rope(q, k, positions, d_model)
print("RoPE applied Q shape:", q_rope.shape)
```

---

## C.5 æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

### C.5.1 å‹¾é…é™ä¸‹æ³•ã®å¤‰ç¨®

```python
class Optimizer:
    """åŸºåº•ã‚¯ãƒ©ã‚¹"""
    def __init__(self, learning_rate):
        self.lr = learning_rate
    
    def update(self, params, grads):
        raise NotImplementedError

class SGD(Optimizer):
    """ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•"""
    def update(self, params, grads):
        for param, grad in zip(params, grads):
            param -= self.lr * grad

class Momentum(Optimizer):
    """Momentum SGD"""
    def __init__(self, learning_rate, momentum=0.9):
        super().__init__(learning_rate)
        self.momentum = momentum
        self.v = None
    
    def update(self, params, grads):
        if self.v is None:
            self.v = [np.zeros_like(p) for p in params]
        
        for i, (param, grad) in enumerate(zip(params, grads)):
            self.v[i] = self.momentum * self.v[i] - self.lr * grad
            param += self.v[i]

class Adam(Optimizer):
    """Adam optimizer"""
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        super().__init__(learning_rate)
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = None
        self.v = None
        self.t = 0
    
    def update(self, params, grads):
        if self.m is None:
            self.m = [np.zeros_like(p) for p in params]
            self.v = [np.zeros_like(p) for p in params]
        
        self.t += 1
        lr_t = self.lr * np.sqrt(1 - self.beta2**self.t) / (1 - self.beta1**self.t)
        
        for i, (param, grad) in enumerate(zip(params, grads)):
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad**2)
            
            param -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + self.epsilon)

# ãƒ†ã‚¹ãƒˆ: ç°¡å˜ãªæœ€é©åŒ–å•é¡Œ
def rosenbrock(x):
    """Rosenbrocké–¢æ•°ï¼ˆæœ€å°å€¤ã¯ (1,1) ã§ 0ï¼‰"""
    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2

def rosenbrock_grad(x):
    """Rosenbrocké–¢æ•°ã®å‹¾é…"""
    grad = np.zeros_like(x)
    grad[0] = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)
    grad[1] = 200 * (x[1] - x[0]**2)
    return grad

# å„æœ€é©åŒ–æ‰‹æ³•ã§æ¯”è¼ƒ
x_init = np.array([-1.0, -1.0])
optimizers = {
    'SGD': SGD(learning_rate=0.001),
    'Momentum': Momentum(learning_rate=0.001, momentum=0.9),
    'Adam': Adam(learning_rate=0.01)
}

histories = {}
for name, opt in optimizers.items():
    x = x_init.copy()
    history = [x.copy()]
    
    for _ in range(1000):
        grad = rosenbrock_grad(x)
        opt.update([x], [grad])
        history.append(x.copy())
    
    histories[name] = np.array(history)

# å¯è¦–åŒ–
plt.figure(figsize=(12, 4))
for name, history in histories.items():
    plt.plot(history[:, 0], history[:, 1], label=name, alpha=0.7)

plt.plot(1, 1, 'r*', markersize=15, label='Optimum')
plt.xlabel('x[0]')
plt.ylabel('x[1]')
plt.legend()
plt.title('Optimization Paths on Rosenbrock Function')
plt.grid(True)
plt.tight_layout()
# plt.savefig('optimizer_comparison.png')
plt.show()
```

### C.5.2 å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°

```python
def linear_warmup_cosine_decay(step, warmup_steps, total_steps, lr_max, lr_min=0):
    """
    Warmup + Cosine Decay
    
    Args:
        step: ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—
        warmup_steps: ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°
        total_steps: ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°
        lr_max: æœ€å¤§å­¦ç¿’ç‡
        lr_min: æœ€å°å­¦ç¿’ç‡
    
    Returns:
        lr: å­¦ç¿’ç‡
    """
    if step < warmup_steps:
        # Linear warmup
        return lr_max * step / warmup_steps
    else:
        # Cosine decay
        progress = (step - warmup_steps) / (total_steps - warmup_steps)
        return lr_min + 0.5 * (lr_max - lr_min) * (1 + np.cos(np.pi * progress))

# å¯è¦–åŒ–
total_steps = 10000
warmup_steps = 1000
lr_max = 0.001
lr_min = 0.00001

steps = np.arange(total_steps)
lrs = [linear_warmup_cosine_decay(s, warmup_steps, total_steps, lr_max, lr_min) 
       for s in steps]

plt.figure(figsize=(10, 4))
plt.plot(steps, lrs)
plt.xlabel('Step')
plt.ylabel('Learning Rate')
plt.title('Warmup + Cosine Decay Schedule')
plt.grid(True)
plt.tight_layout()
# plt.savefig('lr_schedule.png')
plt.show()
```

---

## C.6 ãƒˆãƒ¼ã‚¯ãƒ³åŒ–

### C.6.1 ç°¡å˜ãªBPEå®Ÿè£…

```python
from collections import Counter, defaultdict
import re

class SimpleBPE:
    """Byte Pair Encodingï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
    
    def __init__(self, num_merges=100):
        self.num_merges = num_merges
        self.merges = {}
        self.vocab = set()
    
    def get_stats(self, word_freqs):
        """ãƒã‚¤ã‚°ãƒ©ãƒ ã®é »åº¦ã‚’è¨ˆç®—"""
        pairs = defaultdict(int)
        for word, freq in word_freqs.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[(symbols[i], symbols[i+1])] += freq
        return pairs
    
    def merge_pair(self, pair, word_freqs):
        """ãƒšã‚¢ã‚’ãƒãƒ¼ã‚¸"""
        new_word_freqs = {}
        bigram = ' '.join(pair)
        replacement = ''.join(pair)
        
        for word, freq in word_freqs.items():
            new_word = word.replace(bigram, replacement)
            new_word_freqs[new_word] = freq
        
        return new_word_freqs
    
    def train(self, texts):
        """BPEå­¦ç¿’"""
        # æ–‡å­—ãƒ¬ãƒ™ãƒ«ã«åˆ†å‰²
        word_freqs = Counter()
        for text in texts:
            words = text.lower().split()
            for word in words:
                word = ' '.join(list(word)) + ' </w>'
                word_freqs[word] += 1
        
        # åˆæœŸèªå½™
        self.vocab = set()
        for word in word_freqs.keys():
            self.vocab.update(word.split())
        
        # BPEãƒãƒ¼ã‚¸
        for i in range(self.num_merges):
            pairs = self.get_stats(word_freqs)
            if not pairs:
                break
            
            best_pair = max(pairs, key=pairs.get)
            word_freqs = self.merge_pair(best_pair, word_freqs)
            self.merges[best_pair] = i
            self.vocab.add(''.join(best_pair))
            
            if i % 20 == 0:
                print(f"Merge {i}: {best_pair} (freq: {pairs[best_pair]})")
        
        print(f"\nVocabulary size: {len(self.vocab)}")
    
    def tokenize(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        words = text.lower().split()
        tokens = []
        
        for word in words:
            word = ' '.join(list(word)) + ' </w>'
            
            # ãƒãƒ¼ã‚¸é©ç”¨
            for pair in sorted(self.merges.keys(), key=lambda x: self.merges[x]):
                bigram = ' '.join(pair)
                if bigram in word:
                    word = word.replace(bigram, ''.join(pair))
            
            tokens.extend(word.split())
        
        return tokens

# ä¾‹
texts = [
    "the quick brown fox jumps over the lazy dog",
    "the dog was lazy",
    "the fox was quick",
] * 10  # ç¹°ã‚Šè¿”ã—ã¦é »åº¦ã‚’ä¸Šã’ã‚‹

bpe = SimpleBPE(num_merges=50)
bpe.train(texts)

test_text = "the quickest fox"
tokens = bpe.tokenize(test_text)
print(f"\nTokenization of '{test_text}':")
print(tokens)
```

---

## C.7 è©•ä¾¡æŒ‡æ¨™

### C.7.1 BLEU ã‚¹ã‚³ã‚¢

```python
from collections import Counter
import numpy as np

def ngrams(tokens, n):
    """n-gramã‚’ç”Ÿæˆ"""
    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]

def bleu_score(reference, candidate, max_n=4):
    """
    BLEU ã‚¹ã‚³ã‚¢è¨ˆç®—
    
    Args:
        reference: å‚ç…§æ–‡ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆï¼‰
        candidate: å€™è£œæ–‡ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆï¼‰
        max_n: æœ€å¤§n-gram
    
    Returns:
        BLEU score
    """
    # Brevity penalty
    ref_len = len(reference)
    cand_len = len(candidate)
    
    if cand_len > ref_len:
        bp = 1
    else:
        bp = np.exp(1 - ref_len / cand_len)
    
    # n-gram precision
    precisions = []
    for n in range(1, max_n + 1):
        ref_ngrams = Counter(ngrams(reference, n))
        cand_ngrams = Counter(ngrams(candidate, n))
        
        # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã•ã‚ŒãŸä¸€è‡´æ•°
        clipped_count = sum(min(cand_ngrams[ng], ref_ngrams[ng]) 
                           for ng in cand_ngrams)
        total_count = sum(cand_ngrams.values())
        
        if total_count == 0:
            precisions.append(0)
        else:
            precisions.append(clipped_count / total_count)
    
    # å¹¾ä½•å¹³å‡
    if min(precisions) > 0:
        geo_mean = np.exp(np.mean(np.log(precisions)))
    else:
        geo_mean = 0
    
    return bp * geo_mean

# ä¾‹
reference = "the cat is on the mat".split()
candidate1 = "the cat is on the mat".split()  # å®Œå…¨ä¸€è‡´
candidate2 = "the cat on the mat".split()      # 1èªæ¬ è½
candidate3 = "there is a cat on the mat".split()  # ç•°ãªã‚‹

print(f"BLEU (perfect): {bleu_score(reference, candidate1):.4f}")
print(f"BLEU (missing word): {bleu_score(reference, candidate2):.4f}")
print(f"BLEU (different): {bleu_score(reference, candidate3):.4f}")
```

### C.7.2 ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£

```python
def compute_perplexity(model_probs, true_labels):
    """
    ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£è¨ˆç®—
    
    Args:
        model_probs: ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡ (n_samples, vocab_size)
        true_labels: æ­£è§£ãƒ©ãƒ™ãƒ« (n_samples,)
    
    Returns:
        perplexity
    """
    n_samples = len(true_labels)
    log_probs = np.log(model_probs[np.arange(n_samples), true_labels] + 1e-10)
    avg_log_prob = np.mean(log_probs)
    perplexity = np.exp(-avg_log_prob)
    return perplexity

# ä¾‹
vocab_size = 10000
n_samples = 100

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
model_probs = softmax(np.random.randn(n_samples, vocab_size), axis=1)
true_labels = np.random.randint(0, vocab_size, n_samples)

ppl = compute_perplexity(model_probs, true_labels)
print(f"Perplexity: {ppl:.2f}")
```

---

## ã¾ã¨ã‚

ã“ã®ä»˜éŒ²ã§ã¯ã€LLMã«é–¢é€£ã™ã‚‹æ•°å€¤è¨ˆç®—ã®å®Ÿè£…ä¾‹ã‚’æä¾›ã—ã¾ã—ãŸã€‚

**å®Ÿè£…ã—ãŸå†…å®¹ï¼š**

âœ… **åŸºæœ¬é–¢æ•°**: æ´»æ€§åŒ–é–¢æ•°ã€æå¤±é–¢æ•°  
âœ… **ç·šå½¢ä»£æ•°**: è¡Œåˆ—æ¼”ç®—ã€SVDã€ãƒãƒ«ãƒ   
âœ… **Attention**: Scaled Dot-Productã€Multi-Head  
âœ… **ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**: æ­£å¼¦æ³¢ã€RoPE  
âœ… **æœ€é©åŒ–**: SGDã€Momentumã€Adamã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°  
âœ… **ãƒˆãƒ¼ã‚¯ãƒ³åŒ–**: BPE  
âœ… **è©•ä¾¡æŒ‡æ¨™**: BLEUã€ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£

**å®Ÿç”¨çš„ãªå®Ÿè£…ã«ã¯ï¼š**

æœ¬ä»˜éŒ²ã®ã‚³ãƒ¼ãƒ‰ã¯æ•™è‚²ç›®çš„ã§ã™ã€‚å®Ÿç”¨ã«ã¯PyTorch/TensorFlow/JAXãªã©ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

**ã•ã‚‰ã«å­¦ã¶ã«ã¯ï¼š**

- HuggingFace Transformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- PyTorchå…¬å¼ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«
- å„ãƒ¢ãƒ‡ãƒ«ã®å…¬å¼å®Ÿè£…

---

**ğŸ“– å‰ã®ä»˜éŒ²ï¼š[ä»˜éŒ²B é‡è¦å®šç†ã¨è¨¼æ˜](../ä»˜éŒ²B_é‡è¦å®šç†ã¨è¨¼æ˜/ä»˜éŒ²B_é‡è¦å®šç†ã¨è¨¼æ˜.md)**  
**ğŸ“– æ¬¡ã®ä»˜éŒ²ï¼š[ä»˜éŒ²D ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](../ä»˜éŒ²D_ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/ä»˜éŒ²D_ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ.md)**
