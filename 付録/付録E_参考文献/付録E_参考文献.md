# ä»˜éŒ²Eï¼šå‚è€ƒæ–‡çŒ®

LLMç†è«–ã‚’å­¦ã¶ãŸã‚ã®é‡è¦ãªè«–æ–‡ã€æ›¸ç±ã€ãƒªã‚½ãƒ¼ã‚¹ã‚’ã¾ã¨ã‚ã¾ã™ã€‚

---

## E.1 åŸºç¤è«–æ–‡ï¼ˆå¿…èª­ï¼‰

### E.1.1 Transformer ã®èª•ç”Ÿ

**Attention Is All You Need (2017)**
- è‘—è€…: Vaswani et al. (Google)
- ä¼šè­°: NeurIPS 2017
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/1706.03762
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: Self-Attentionæ©Ÿæ§‹ã€Multi-Head Attentionã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

**é‡è¦ãªè²¢çŒ®:**
```
âœ“ RNNã‚’ä½¿ã‚ãªã„ç³»åˆ—ãƒ¢ãƒ‡ãƒ«
âœ“ Scaled Dot-Product Attention
âœ“ ä¸¦åˆ—åŒ–å¯èƒ½ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
âœ“ ç¿»è¨³ã‚¿ã‚¹ã‚¯ã§SOTAé”æˆ
```

### E.1.2 GPTã‚·ãƒªãƒ¼ã‚º

**Improving Language Understanding by Generative Pre-Training (GPT-1, 2018)**
- è‘—è€…: Radford et al. (OpenAI)
- ãƒªãƒ³ã‚¯: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: äº‹å‰å­¦ç¿’ + ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ 

**Language Models are Unsupervised Multitask Learners (GPT-2, 2019)**
- è‘—è€…: Radford et al. (OpenAI)
- ãƒªãƒ³ã‚¯: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: Zero-shotå­¦ç¿’ã€ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ï¼ˆ1.5B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰

**Language Models are Few-Shot Learners (GPT-3, 2020)**
- è‘—è€…: Brown et al. (OpenAI)
- ä¼šè­°: NeurIPS 2020
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2005.14165
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: In-Context Learningã€Few-shotå­¦ç¿’ã€å‰µç™ºçš„èƒ½åŠ›ï¼ˆ175B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰

### E.1.3 BERT ã¨åŒæ–¹å‘ãƒ¢ãƒ‡ãƒ«

**BERT: Pre-training of Deep Bidirectional Transformers (2018)**
- è‘—è€…: Devlin et al. (Google)
- ä¼šè­°: NAACL 2019
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/1810.04805
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: Masked Language Modelingã€åŒæ–¹å‘ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼

**RoBERTa: A Robustly Optimized BERT Pretraining Approach (2019)**
- è‘—è€…: Liu et al. (Facebook)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/1907.11692
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: BERTã®è¨“ç·´æ”¹å–„ã€NSPé™¤å»ã€å‹•çš„ãƒã‚¹ã‚­ãƒ³ã‚°

---

## E.2 ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡

**Scaling Laws for Neural Language Models (2020)**
- è‘—è€…: Kaplan et al. (OpenAI)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2001.08361
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: ã¹ãä¹—å‰‡ $L \propto N^{-\alpha}$ã€æœ€é©é…åˆ†

**Training Compute-Optimal Large Language Models (Chinchilla, 2022)**
- è‘—è€…: Hoffmann et al. (DeepMind)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2203.15556
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®åŒæ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€Chinchillaæœ€é©æ€§

**Emergent Abilities of Large Language Models (2022)**
- è‘—è€…: Wei et al. (Google)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2206.07682
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: å‰µç™ºçš„èƒ½åŠ›ã®åˆ†æã€ã‚¹ã‚±ãƒ¼ãƒ«ã«ã‚ˆã‚‹è³ªçš„å¤‰åŒ–

---

## E.3 å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

### E.3.1 æœ€é©åŒ–

**Adam: A Method for Stochastic Optimization (2014)**
- è‘—è€…: Kingma & Ba
- ä¼šè­°: ICLR 2015
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/1412.6980
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: Adaptive learning rateã€ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¨å®š

**Decoupled Weight Decay Regularization (AdamW, 2017)**
- è‘—è€…: Loshchilov & Hutter
- ä¼šè­°: ICLR 2019
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/1711.05101
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: Weight Decayã¨L2æ­£å‰‡åŒ–ã®åˆ†é›¢

### E.3.2 å¼·åŒ–å­¦ç¿’ã¨RLHF

**Proximal Policy Optimization Algorithms (PPO, 2017)**
- è‘—è€…: Schulman et al. (OpenAI)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/1707.06347
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: æ–¹ç­–æœ€é©åŒ–ã€ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç›®çš„é–¢æ•°

**Training language models to follow instructions with human feedback (InstructGPT, 2022)**
- è‘—è€…: Ouyang et al. (OpenAI)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2203.02155
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: RLHFã€å ±é…¬ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€äººé–“ã®å—œå¥½ã¨ã®æ•´åˆ

**Constitutional AI: Harmlessness from AI Feedback (2022)**
- è‘—è€…: Bai et al. (Anthropic)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2212.08073
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: AIè‡ªèº«ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã€æ†²æ³•çš„åŸå‰‡

---

## E.4 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

**Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (2022)**
- è‘—è€…: Wei et al. (Google)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2201.11903
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: æ€è€ƒé€£é–ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã€æ¨è«–èƒ½åŠ›ã®å‘ä¸Š

**Self-Consistency Improves Chain of Thought Reasoning (2022)**
- è‘—è€…: Wang et al. (Google)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2203.11171
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: è¤‡æ•°ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° + å¤šæ•°æ±º

**Large Language Models are Zero-Shot Reasoners (2022)**
- è‘—è€…: Kojima et al.
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2205.11916
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: "Let's think step by step" ã®åŠ¹æœ

---

## E.5 åŠ¹ç‡åŒ–æŠ€è¡“

### E.5.1 é‡å­åŒ–ã¨ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°

**LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale (2022)**
- è‘—è€…: Dettmers et al.
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2208.07339
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: 8bité‡å­åŒ–ã€å¤–ã‚Œå€¤ã®æ‰±ã„

**GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers (2022)**
- è‘—è€…: Frantar et al.
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2210.17323
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: Post-trainingé‡å­åŒ–ã€ç²¾åº¦ä¿æŒ

### E.5.2 Parameter-Efficient Fine-Tuning

**LoRA: Low-Rank Adaptation of Large Language Models (2021)**
- è‘—è€…: Hu et al. (Microsoft)
- ä¼šè­°: ICLR 2022
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2106.09685
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: ä½ãƒ©ãƒ³ã‚¯é©å¿œã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

**Prefix-Tuning: Optimizing Continuous Prompts for Generation (2021)**
- è‘—è€…: Li & Liang (Stanford)
- ä¼šè­°: ACL 2021
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2101.00190
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: é€£ç¶šçš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå­¦ç¿’

### E.5.3 åŠ¹ç‡çš„Attention

**FlashAttention: Fast and Memory-Efficient Exact Attention (2022)**
- è‘—è€…: Dao et al. (Stanford)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2205.14135
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: ãƒ¡ãƒ¢ãƒªéšå±¤æœ€é©åŒ–ã€IOåŠ¹ç‡åŒ–

**Longformer: The Long-Document Transformer (2020)**
- è‘—è€…: Beltagy et al. (Allen AI)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2004.05150
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: Sparse Attentionã€é•·æ–‡è„ˆå‡¦ç†

---

## E.6 å¤šè¨€èªãƒ»å¤šãƒ¢ãƒ¼ãƒ€ãƒ«

**Unsupervised Cross-lingual Representation Learning at Scale (XLM-R, 2019)**
- è‘—è€…: Conneau et al. (Facebook)
- ä¼šè­°: ACL 2020
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/1911.02116
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: 100è¨€èªã®å¤šè¨€èªãƒ¢ãƒ‡ãƒ«

**Learning Transferable Visual Models From Natural Language Supervision (CLIP, 2021)**
- è‘—è€…: Radford et al. (OpenAI)
- ä¼šè­°: ICML 2021
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2103.00020
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: Vision-Languageäº‹å‰å­¦ç¿’ã€ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå­¦ç¿’

**Flamingo: a Visual Language Model for Few-Shot Learning (2022)**
- è‘—è€…: Alayrac et al. (DeepMind)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2204.14198
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: è¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ã€Few-shotå­¦ç¿’

---

## E.7 è§£é‡ˆå¯èƒ½æ€§ã¨å®‰å…¨æ€§

**A Mathematical Framework for Transformer Circuits (2021)**
- è‘—è€…: Elhage et al. (Anthropic)
- ãƒªãƒ³ã‚¯: https://transformer-circuits.pub/2021/framework/index.html
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: Induction Headsã€ãƒ¡ã‚«ãƒ‹ã‚¹ãƒ†ã‚£ãƒƒã‚¯è§£é‡ˆå¯èƒ½æ€§

**In-context Learning and Induction Heads (2022)**
- è‘—è€…: Olsson et al. (Anthropic)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2209.11895
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: ICLã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã€Induction Heads

**Red Teaming Language Models to Reduce Harms (2022)**
- è‘—è€…: Perez et al. (Anthropic)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2209.07858
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: ãƒ¬ãƒƒãƒ‰ãƒãƒ¼ãƒŸãƒ³ã‚°ã€æœ‰å®³æ€§å‰Šæ¸›

---

## E.8 ç†è«–çš„åŸºç¤

**Understanding Deep Learning Requires Rethinking Generalization (2016)**
- è‘—è€…: Zhang et al.
- ä¼šè­°: ICLR 2017
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/1611.03530
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: éå‰°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã€æ±åŒ–ã®è¬

**Deep Double Descent (2019)**
- è‘—è€…: Nakkiran et al.
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/1912.02292
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: Double Descentç¾è±¡ã€æ±åŒ–æ›²ç·š

**Grokking: Generalization Beyond Overfitting (2021)**
- è‘—è€…: Power et al. (OpenAI)
- ãƒªãƒ³ã‚¯: https://arxiv.org/abs/2201.02177
- é‡è¦æ€§: â­â­â­
- å†…å®¹: é…å»¶æ±åŒ–ã€Grokkingç¾è±¡

---

## E.9 æ›¸ç±

### E.9.1 æ·±å±¤å­¦ç¿’ã®åŸºç¤

**Deep Learning (2016)**
- è‘—è€…: Ian Goodfellow, Yoshua Bengio, Aaron Courville
- å‡ºç‰ˆç¤¾: MIT Press
- ãƒªãƒ³ã‚¯: https://www.deeplearningbook.org/
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: æ·±å±¤å­¦ç¿’ã®åŒ…æ‹¬çš„æ•™ç§‘æ›¸

**Neural Networks and Deep Learning (2018)**
- è‘—è€…: Charu C. Aggarwal
- å‡ºç‰ˆç¤¾: Springer
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: ç†è«–ã¨å®Ÿè£…ã®ãƒãƒ©ãƒ³ã‚¹

### E.9.2 è‡ªç„¶è¨€èªå‡¦ç†

**Speech and Language Processing (3rd ed. draft)**
- è‘—è€…: Dan Jurafsky, James H. Martin
- ãƒªãƒ³ã‚¯: https://web.stanford.edu/~jurafsky/slp3/
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: NLPã®å¤å…¸çš„æ•™ç§‘æ›¸ã€LLMç« ã‚’å«ã‚€

**Natural Language Processing with Transformers (2022)**
- è‘—è€…: Lewis Tunstall, Leandro von Werra, Thomas Wolf
- å‡ºç‰ˆç¤¾: O'Reilly
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: HuggingFace Transformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å®Ÿè·µ

### E.9.3 æ•°å­¦çš„åŸºç¤

**Pattern Recognition and Machine Learning (2006)**
- è‘—è€…: Christopher M. Bishop
- å‡ºç‰ˆç¤¾: Springer
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: æ©Ÿæ¢°å­¦ç¿’ã®æ•°å­¦çš„åŸºç¤

**Information Theory, Inference, and Learning Algorithms (2003)**
- è‘—è€…: David J.C. MacKay
- ãƒªãƒ³ã‚¯: http://www.inference.org.uk/mackay/itila/
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: æƒ…å ±ç†è«–ã¨æ©Ÿæ¢°å­¦ç¿’

---

## E.10 ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

### E.10.1 ã‚³ãƒ¼ã‚¹

**Stanford CS224N: Natural Language Processing with Deep Learning**
- ãƒªãƒ³ã‚¯: http://web.stanford.edu/class/cs224n/
- è¬›å¸«: Christopher Manning
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: NLP + æ·±å±¤å­¦ç¿’ã®ä½“ç³»çš„ã‚³ãƒ¼ã‚¹

**Stanford CS25: Transformers United**
- ãƒªãƒ³ã‚¯: https://web.stanford.edu/class/cs25/
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: Transformerç‰¹åŒ–ã‚³ãƒ¼ã‚¹

**Fast.ai Practical Deep Learning for Coders**
- ãƒªãƒ³ã‚¯: https://course.fast.ai/
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: å®Ÿè·µé‡è¦–ã®æ·±å±¤å­¦ç¿’

### E.10.2 ãƒ–ãƒ­ã‚°ã¨ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

**The Illustrated Transformer**
- è‘—è€…: Jay Alammar
- ãƒªãƒ³ã‚¯: http://jalammar.github.io/illustrated-transformer/
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: Transformerã®è¦–è¦šçš„è§£èª¬

**The Annotated Transformer**
- è‘—è€…: Harvard NLP
- ãƒªãƒ³ã‚¯: http://nlp.seas.harvard.edu/2018/04/03/attention.html
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: ã‚³ãƒ¼ãƒ‰ä»˜ãè©³ç´°è§£èª¬

**Andrej Karpathy's Blog**
- ãƒªãƒ³ã‚¯: http://karpathy.github.io/
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: æ·±å±¤å­¦ç¿’ã®ç›´æ„Ÿçš„è§£èª¬

**Lil'Log (Lilian Weng's Blog)**
- ãƒªãƒ³ã‚¯: https://lilianweng.github.io/
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: æœ€æ–°ç ”ç©¶ã®ä¸å¯§ãªè§£èª¬

### E.10.3 å®Ÿè£…ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

**HuggingFace Transformers**
- ãƒªãƒ³ã‚¯: https://github.com/huggingface/transformers
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: æœ€ã‚‚åºƒãä½¿ã‚ã‚Œã‚‹Transformerãƒ©ã‚¤ãƒ–ãƒ©ãƒª

**PyTorch**
- ãƒªãƒ³ã‚¯: https://pytorch.org/
- é‡è¦æ€§: â­â­â­â­â­
- å†…å®¹: æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

**JAX**
- ãƒªãƒ³ã‚¯: https://github.com/google/jax
- é‡è¦æ€§: â­â­â­â­
- å†…å®¹: é«˜æ€§èƒ½æ•°å€¤è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

---

## E.11 ä¼šè­°ã¨ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«

### E.11.1 ä¸»è¦ä¼šè­°

**ãƒˆãƒƒãƒ—ãƒ†ã‚£ã‚¢ï¼š**
```
NeurIPS (Neural Information Processing Systems)
ICML (International Conference on Machine Learning)
ICLR (International Conference on Learning Representations)
ACL (Association for Computational Linguistics)
EMNLP (Empirical Methods in NLP)
NAACL (North American Chapter of ACL)
```

**ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ï¼ˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«é–¢é€£ï¼‰ï¼š**
```
CVPR (Computer Vision and Pattern Recognition)
ICCV (International Conference on Computer Vision)
ECCV (European Conference on Computer Vision)
```

### E.11.2 ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«

```
JMLR (Journal of Machine Learning Research)
TACL (Transactions of the ACL)
Nature Machine Intelligence
Science Robotics
```

### E.11.3 ãƒ—ãƒ¬ãƒ—ãƒªãƒ³ãƒˆã‚µãƒ¼ãƒãƒ¼

**arXiv**
- ãƒªãƒ³ã‚¯: https://arxiv.org/
- ã‚«ãƒ†ã‚´ãƒª: cs.CL (Computation and Language), cs.LG (Machine Learning)
- æœ€æ–°è«–æ–‡ãŒæœ€ã‚‚æ—©ãå…¬é–‹ã•ã‚Œã‚‹

---

## E.12 èª­ã¿æ–¹ã®ã‚¬ã‚¤ãƒ‰

### E.12.1 åˆå¿ƒè€…å‘ã‘èª­æ›¸é †åº

**Phase 1: åŸºç¤å›ºã‚**
1. Deep Learning (Goodfellow et al.) - ç¬¬1-5ç« 
2. The Illustrated Transformer (Jay Alammar)
3. Attention Is All You Need (è«–æ–‡)

**Phase 2: LLMã®ç†è§£**
4. BERT è«–æ–‡
5. GPT-2 è«–æ–‡
6. GPT-3 è«–æ–‡
7. Scaling Laws è«–æ–‡

**Phase 3: å®Ÿè·µã¨å¿œç”¨**
8. Natural Language Processing with Transformers (æ›¸ç±)
9. HuggingFace ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«
10. Chain-of-Thought è«–æ–‡

**Phase 4: æœ€æ–°ãƒˆãƒ”ãƒƒã‚¯**
11. InstructGPT / RLHF è«–æ–‡
12. FlashAttention è«–æ–‡
13. èˆˆå‘³ã®ã‚ã‚‹ç‰¹å®šãƒˆãƒ”ãƒƒã‚¯ã®æ·±æ˜ã‚Š

### E.12.2 è«–æ–‡ã®èª­ã¿æ–¹

**åŠ¹ç‡çš„ãªèª­ã¿æ–¹ï¼š**

```
1st pass (5-10åˆ†):
  âœ“ ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ
  âœ“ å›³è¡¨ã‚’å…¨ã¦è¦‹ã‚‹
  âœ“ çµè«–ã‚’èª­ã‚€
  â†’ èª­ã‚€ä¾¡å€¤ãŒã‚ã‚‹ã‹åˆ¤æ–­

2nd pass (30-60åˆ†):
  âœ“ ã‚¤ãƒ³ãƒˆãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³
  âœ“ é–¢é€£ç ”ç©¶
  âœ“ æ‰‹æ³•ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ
  âœ“ å®Ÿé¨“çµæœ
  â†’ ä¸»è¦ãªã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç†è§£

3rd pass (æ•°æ™‚é–“):
  âœ“ è©³ç´°ãªæ‰‹æ³•
  âœ“ è¨¼æ˜ã‚„å°å‡º
  âœ“ ã‚³ãƒ¼ãƒ‰ã‚’èª­ã‚€/å®Ÿè£…ã™ã‚‹
  â†’ å®Œå…¨ãªç†è§£ã¨å†ç¾
```

### E.12.3 æœ€æ–°æƒ…å ±ã®è¿½ã„æ–¹

**æ—¥å¸¸çš„ã«ãƒã‚§ãƒƒã‚¯ï¼š**
```
âœ“ arXiv cs.CL / cs.LGï¼ˆæ¯æ—¥ï¼‰
âœ“ Twitterï¼ˆç ”ç©¶è€…ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼‰
âœ“ Papers with Codeï¼ˆå®Ÿè£…ä»˜ãè«–æ–‡ï¼‰
âœ“ HuggingFace Daily Papers
```

**é€±æ¬¡ï¼š**
```
âœ“ ä¸»è¦ç ”ç©¶è€…ã®ãƒ–ãƒ­ã‚°
âœ“ Reddit r/MachineLearning
âœ“ ç ”ç©¶å®¤ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼
```

**ä¼šè­°ã‚·ãƒ¼ã‚ºãƒ³ï¼š**
```
âœ“ NeurIPS, ICML, ICLR, ACLç­‰ã®æ¡æŠè«–æ–‡ãƒªã‚¹ãƒˆ
âœ“ ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ã¨ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«
âœ“ ãƒ™ã‚¹ãƒˆãƒšãƒ¼ãƒ‘ãƒ¼è³
```

---

## ã¾ã¨ã‚

### å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹ã®éšå±¤

```
å…¥é–€ãƒ¬ãƒ™ãƒ«:
  ğŸ“˜ The Illustrated Transformer
  ğŸ“˜ Stanford CS224N
  ğŸ“˜ Fast.ai ã‚³ãƒ¼ã‚¹

ä¸­ç´šãƒ¬ãƒ™ãƒ«:
  ğŸ“• Deep Learning (Goodfellow et al.)
  ğŸ“• ä¸»è¦è«–æ–‡ï¼ˆBERT, GPT, Transformerï¼‰
  ğŸ“• HuggingFace Transformers

ä¸Šç´šãƒ¬ãƒ™ãƒ«:
  ğŸ“— æœ€æ–°è«–æ–‡ï¼ˆarXivï¼‰
  ğŸ“— ç†è«–çš„åŸºç¤ï¼ˆæ±åŒ–ç†è«–ã€æœ€é©åŒ–ï¼‰
  ğŸ“— å°‚é–€åˆ†é‡ã®æ·±æ˜ã‚Š

ç ”ç©¶ãƒ¬ãƒ™ãƒ«:
  ğŸ“™ æœªè§£æ±ºå•é¡Œ
  ğŸ“™ ä¼šè­°ç™ºè¡¨
  ğŸ“™ ç‹¬è‡ªã®ç ”ç©¶
```

### é‡è¦ãªæ•™è¨“

**ç†è«–ã¨å®Ÿè·µã®ä¸¡è¼ªï¼š**
- è«–æ–‡ã‚’èª­ã‚€ã ã‘ã§ãªãã€å®Ÿè£…ã™ã‚‹
- å®Ÿè£…ã™ã‚‹ã ã‘ã§ãªãã€ç†è«–ã‚’ç†è§£ã™ã‚‹

**ç¶™ç¶šçš„å­¦ç¿’ï¼š**
- åˆ†é‡ã¯æ€¥é€Ÿã«é€²åŒ–
- å®šæœŸçš„ãªã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—ãŒå¿…é ˆ

**ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å‚åŠ ï¼š**
- GitHubã€è«–æ–‡ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³
- å‹‰å¼·ä¼šã€èª­æ›¸ä¼š
- è‡ªåˆ†ã®ç†è§£ã‚’å…±æœ‰

---

**ã“ã®ã‚¬ã‚¤ãƒ‰ãƒ–ãƒƒã‚¯ã®æ—…ã¯ã“ã“ã§çµ‚ã‚ã‚Šã¾ã™ãŒã€LLMã®å­¦ç¿’ã¯ç¶šãã¾ã™ã€‚**

**Happy Learning! ğŸš€ğŸ“š**

---

**ğŸ“– å‰ã®ä»˜éŒ²ï¼š[ä»˜éŒ²D ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](../ä»˜éŒ²D_ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/ä»˜éŒ²D_ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ.md)**  
**ğŸ“– ç›®æ¬¡ã«æˆ»ã‚‹ï¼š[README](../../README.md)**
