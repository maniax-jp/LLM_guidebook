# LLMの図解入り初心者向けガイドブック
## ガイドブック目次

---

## 第I部：基礎理論

### [第1章：序論](./docs/第1章_序論.md)
1.1 大規模言語モデル（LLM）とは  
1.2 本書の目的と構成  
1.3 必要な数学的予備知識  
1.4 記号と表記法  

### [第2章：数学的基礎](./docs/第2章_数学的基礎.md)
2.1 線形代数の復習  
  - ベクトル空間とノルム  
  - 行列演算と固有値分解  
  - 特異値分解（SVD）  

2.2 確率論と統計学  
  - 確率分布と期待値  
  - ベイズの定理  
  - 情報理論の基礎  

2.3 最適化理論  
  - 凸最適化  
  - 勾配降下法  
  - 確率的最適化  

2.4 関数解析  
  - ヒルベルト空間  
  - 関数近似理論  

### [第3章：ニューラルネットワークの基礎](./docs/第3章_ニューラルネットワークの基礎.md)
3.1 パーセプトロンと線形分離可能性  
3.2 多層パーセプトロンと汎用近似定理  
3.3 活性化関数の性質  
3.4 バックプロパゲーションの導出  
3.5 勾配消失・爆発問題の理論的解析  

---

## 第II部：アーキテクチャの理論

### [第4章：Transformerアーキテクチャの基礎](./docs/第4章_Transformerアーキテクチャの基礎.md)
4.1 アテンション機構の定式化  
4.2 位置エンコーディングの理論  
4.3 Layer Normalizationの統計的性質  
4.4 フィードフォワードネットワークの役割  

#### [第4章補足：トークナイズとベクトル化](./docs/第4章補足_トークナイズとベクトル化.md)
4A.1 トークナイズの基礎  
4A.2 主要なトークナイズアルゴリズム  
4A.3 エンコーディング：トークンから整数へ  
4A.4 ベクトル化：埋め込み表現  
4A.5 完全なパイプライン  
4A.6 実践的な考慮事項  
4A.7 最新のトークナイズ手法  

### [第5章：自己回帰言語モデルの理論](./docs/第5章_自己回帰言語モデルの理論.md)
5.1 自己回帰モデリングの確率論的基礎  
5.2 因果的マスキングの意味  
5.3 次トークン予測の情報理論的解析  
5.4 文脈長と記憶容量の理論的限界  

### [第6章：スケーリング法則の基礎](./docs/第6章_スケーリング法則の基礎.md)
6.1 パラメータ数と性能の関係  
6.2 計算量スケーリングの理論  
6.3 データスケーリングの統計的解析  
6.4 最適な資源配分の導出  

---

## 第III部：学習アルゴリズムの理論

### [第7章：事前学習の原理](./docs/第7章_事前学習の原理.md)
7.1 最尤推定と言語モデリング  
7.2 クロスエントロピー損失の情報理論的解釈  
7.3 Adam最適化アルゴリズムの収束理論  
7.4 学習率スケジューリングの根拠  
7.5 正則化手法の理論的基礎  

### [第8章：ファインチューニングの理論](./docs/第8章_ファインチューニングの理論.md)
8.1 転移学習の統計学習理論  
8.2 ドメイン適応のモデリング  
8.3 破滅的忘却の理論的解析  
8.4 継続学習アルゴリズムの基礎  

### [第9章：強化学習からの人間フィードバック（RLHF）](./docs/第9章_強化学習からの人間フィードバック.md)
9.1 強化学習の基礎  
9.2 方策勾配法の理論  
9.3 PPO（Proximal Policy Optimization）の解析  
9.4 報酬モデリング  
9.5 Constitutional AI（憲法的AI）  
9.6 RLHFの完全パイプライン  

---

## 第IV部：生成と評価の理論

### [第10章：テキスト生成の理論](./docs/第10章_テキスト生成の理論.md)
10.1 デコーディング戦略の分類  
10.2 貪欲法とビームサーチ  
10.3 確率的サンプリング法  
10.4 生成品質の評価指標  
10.5 デコーディングの理論的解析  

### [第11章：プロンプトエンジニアリングの理論](./docs/第11章_プロンプトエンジニアリングの理論.md)
11.1 In-Context Learning（文脈内学習）  
11.2 Chain-of-Thought（思考の連鎖）  
11.3 プロンプト設計の原則  
11.4 高度なプロンプト技術  

### [第12章：創発的能力と汎化の理論](./docs/第12章_創発的能力と汎化の理論.md)
12.1 スケーリングと創発  
12.2 タスク汎化のメカニズム  
12.3 知識の圧縮と一般化  
12.4 ワールドモデルとしてのLLM  
12.5 汎化性能の理論的限界  

---

## 第V部：応用と拡張の理論

### [第13章：多言語・多モーダルモデルの理論](./docs/第13章_多言語・多モーダルモデルの理論.md)
13.1 多言語モデルのアーキテクチャ  
13.2 言語間転移のメカニズム  
13.3 Vision-Language モデル  
13.4 マルチモーダル統合の理論  
13.5 音声・その他のモダリティ  

### [第14章：効率化技術の理論](./docs/第14章_効率化技術の理論.md)
14.1 モデル圧縮：量子化  
14.2 モデル圧縮：プルーニング  
14.3 パラメータ効率的ファインチューニング（PEFT）  
14.4 効率的推論  
14.5 分散学習と推論  

### [第15章：解釈可能性と安全性の理論](./docs/第15章_解釈可能性と安全性の理論.md)
15.1 Mechanistic Interpretability（機構的解釈可能性）  
15.2 Attentionパターンの分析  
15.3 プロービング  
15.4 安全性とアライメント  
15.5 バイアスと公平性  
15.6 モデルの脆弱性  

---

## 第VI部：理論的基盤

### [第16章：計算複雑性の理論](./docs/第16章_計算複雑性の理論.md)
16.1 Transformerの計算複雑性  
16.2 メモリ複雑性  
16.3 長い系列への対処  
16.4 近似と下界  
16.5 効率的アーキテクチャの設計原理  

### [第17章：認知科学との接点](./docs/第17章_認知科学との接点.md)
17.1 言語獲得理論とLLM  
17.2 言語処理の神経相関  
17.3 意味理解のメカニズム  
17.4 意識と理解の問題  
17.5 認知バイアスの共有  
17.6 社会的認知  

### [第18章：未来の研究方向](./docs/第18章_未来の研究方向.md)
18.1 現在の限界と課題  
18.2 次世代アーキテクチャ  
18.3 AGIへの道筋  
18.4 理論的未解決問  
18.5 倫理的・社会的課題  
18.6 期待されるブレークスルー  

---

## 付録

### [付録A：数学記号の総まとめ](./docs/付録A_数学記号の総まとめ.md)  
### [付録B：重要定理と証明](./docs/付録B_重要定理と証明.md)  
### [付録C：数値計算の実装](./docs/付録C_数値計算の実装.md)  
### [付録D：ベンチマークデータセット](./docs/付録D_ベンチマークデータセット.md)  
### [付録E：参考文献](./docs/付録E_参考文献.md)  

---

**対象読者：AIエンジニアになりたい初学者**  
**前提知識：高校レベルの数学の基礎**
