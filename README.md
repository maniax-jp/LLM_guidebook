# LLMの図解入り初心者向けガイドブック
## ガイドブック目次

---

## 第I部：基礎理論

### [第1章：序論](./第I部_基礎理論/第1章_序論/第1章_序論.md)
1.1 大規模言語モデル（LLM）とは  
1.2 本書の目的と構成  
1.3 必要な数学的予備知識  
1.4 記号と表記法  

### [第2章：数学的基礎](./第I部_基礎理論/第2章_数学的基礎/第2章_数学的基礎.md)
2.1 線形代数の復習  
  - ベクトル空間とノルム  
  - 行列演算と固有値分解  
  - 特異値分解（SVD）  
2.2 確率論と統計学  
  - 確率分布と期待値  
  - ベイズの定理  
  - 情報理論の基礎  
2.3 最適化理論  
  - 凸最適化  
  - 勾配降下法  
  - 確率的最適化  
2.4 関数解析  
  - ヒルベルト空間  
  - 関数近似理論  

### [第3章：ニューラルネットワークの基礎](./第I部_基礎理論/第3章_ニューラルネットワークの基礎/第3章_ニューラルネットワークの基礎.md)
3.1 パーセプトロンと線形分離可能性  
3.2 多層パーセプトロンと汎用近似定理  
3.3 活性化関数の性質  
3.4 バックプロパゲーションの導出  
3.5 勾配消失・爆発問題の理論的解析  

---

## 第II部：アーキテクチャの理論

### [第4章：Transformerアーキテクチャの基礎](./第II部_アーキテクチャの理論/第4章_Transformerアーキテクチャの基礎/第4章_Transformerアーキテクチャの基礎.md)
4.1 アテンション機構の定式化  
  - スケールドドット積アテンション  
  - マルチヘッドアテンションの線形代数的解釈  
4.2 位置エンコーディングの理論  
  - 正弦波位置エンコーディングの根拠  
  - 回転位置エンコーディング（RoPE）の理論  
4.3 Layer Normalizationの統計的性質  
4.4 フィードフォワードネットワークの役割  

### [第5章：自己回帰言語モデルの理論](./第II部_アーキテクチャの理論/第5章_自己回帰言語モデルの理論/第5章_自己回帰言語モデルの理論.md)
5.1 自己回帰モデリングの確率論的基礎  
5.2 因果的マスキングの意味  
5.3 次トークン予測の情報理論的解析  
5.4 文脈長と記憶容量の理論的限界  

### [第6章：スケーリング法則の基礎](./第II部_アーキテクチャの理論/第6章_スケーリング法則の基礎/第6章_スケーリング法則の基礎.md)
6.1 パラメータ数と性能の関係  
6.2 計算量スケーリングの理論  
6.3 データスケーリングの統計的解析  
6.4 最適な資源配分の導出  

---

## 第III部：学習アルゴリズムの理論

### [第7章：事前学習の原理](./第III部_学習アルゴリズムの理論/第7章_事前学習の原理/第7章_事前学習の原理.md)
7.1 最尤推定と言語モデリング  
7.2 クロスエントロピー損失の情報理論的解釈  
7.3 Adam最適化アルゴリズムの収束理論  
7.4 学習率スケジューリングの根拠  
7.5 正則化手法の理論的基礎  

### [第8章：ファインチューニングの理論](./第III部_学習アルゴリズムの理論/第8章_ファインチューニングの理論/第8章_ファインチューニングの理論.md)
8.1 転移学習の統計学習理論  
8.2 ドメイン適応のモデリング  
8.3 破滅的忘却の理論的解析  
8.4 継続学習アルゴリズムの基礎  

### [第9章：強化学習からの人間フィードバック（RLHF）](./第III部_学習アルゴリズムの理論/第9章_強化学習からの人間フィードバック/第9章_強化学習からの人間フィードバック.md)
9.1 強化学習の基礎  
  - マルコフ決定過程  
  - ベルマン方程式  
9.2 方策勾配法の理論  
9.3 Proximal Policy Optimization（PPO）の解析  
9.4 報酬モデリングの統計的理論  
9.5 憲法的AI（Constitutional AI）のフレームワーク  

---

## 第IV部：推論と生成の理論

### [第10章：テキスト生成の確率論的基礎](./第IV部_推論と生成の理論/第10章_テキスト生成の確率論的基礎/第10章_テキスト生成の確率論的基礎.md)
10.1 サンプリング手法の理論  
  - Greedy Decoding  
  - Beam Search  
  - 核サンプリング（Nucleus Sampling）  
  - Top-k サンプリング  
10.2 温度パラメータの統計的効果  
10.3 多様性と品質のトレードオフ  

### [第11章：プロンプトエンジニアリングの基礎](./第IV部_推論と生成の理論/第11章_プロンプトエンジニアリングの基礎/第11章_プロンプトエンジニアリングの基礎.md)
11.1 文脈内学習（In-Context Learning）の理論  
11.2 Few-shot学習の統計的解析  
11.3 Chain-of-Thoughtの認知科学的モデリング  
11.4 プロンプトの情報理論的最適化  

### [第12章：創発能力の理解](./第IV部_推論と生成の理論/第12章_創発能力の理解/第12章_創発能力の理解.md)
12.1 相転移現象としての創発  
12.2 複雑系理論とLLM  
12.3 ジンジスのモデリング  
12.4 汎化能力の統計学習理論的解析  

---

## 第V部：高度なトピックスと応用

### [第13章：多言語・多モーダルモデルの理論](./第V部_先進的トピックと応用/第13章_多言語・マルチモーダルモデルの基礎/第13章_多言語・マルチモーダルモデルの基礎.md)
13.1 言語間転移のモデリング  
13.2 多言語表現学習の理論  
13.3 視覚・言語統合の基礎  
13.4 マルチモーダル注意機構の理論  

### [第14章：効率化手法の基礎](./第V部_先進的トピックと応用/第14章_効率化手法の基礎/第14章_効率化手法の基礎.md)
14.1 知識蒸留の情報理論的解析  
14.2 量子化の数値解析的理論  
14.3 プルーニングの統計的理論  
14.4 低ランク近似（LoRA）の線形代数的基礎  

### [第15章：解釈可能性のアプローチ](./第V部_先進的トピックと応用/第15章_解釈可能性とメカニズムの解明/第15章_解釈可能性とメカニズムの解明.md)
15.1 メカニスティック解釈可能性のフレームワーク  
15.2 アテンション重みの統計的解析  
15.3 表現空間の幾何学的構造  
15.4 因果推論とLLM  

---

## 第VI部：理論的限界と未来の展望

### [第16章：計算複雑性理論とLLM](./第VI部_理論的限界と展望/第16章_理論的限界と計算複雑性/第16章_理論的限界と計算複雑性.md)
16.1 言語生成の計算複雑性  
16.2 アプリシマビリティとハードネス  
16.3 量子計算との関連  

### [第17章：認知科学・神経科学との接点](./第VI部_理論的限界と展望/第17章_認知科学と神経科学との接続/第17章_認知科学と神経科学との接続.md)
17.1 脳の言語処理との類似性  
17.2 認知負荷理論とLLM  
17.3 意識と機械知能のモデリング  

### [第18章：未来の研究方向](./第VI部_理論的限界と展望/第18章_未解決問題と今後の研究方向/第18章_未解決問題と今後の研究方向.md)
18.1 次世代アーキテクチャの可能性  
18.2 AGI（汎用人工知能）への道筋  
18.3 未解決問題と研究課題  

---

## 付録

### [付録A：記号と表記法](./付録/付録A_記号と表記法/付録A_記号と表記法.md)  
### [付録B：主要な定理と証明](./付録/付録B_主要定理と証明/付録B_主要定理と証明.md)  
### [付録C：実装のための数値計算手法](./付録/付録C_数値計算アルゴリズムの実装/付録C_数値計算アルゴリズムの実装.md)  
### [付録D：ベンチマークとメトリクス](./付録/付録D_ベンチマークとデータセット/付録D_ベンチマークとデータセット.md)  
### [付録E：参考文献と文献案内](./付録/付録E_参考文献とリソース/付録E_参考文献とリソース.md)  

---

## 索引

---

**総ページ数予想：約800-1000ページ**  
**対象読者：AIエンジニアになりたい初学者**  
**前提知識：高校レベルの数学の基礎**