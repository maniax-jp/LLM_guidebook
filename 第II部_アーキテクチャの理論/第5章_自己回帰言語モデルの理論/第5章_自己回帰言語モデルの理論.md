# 第5章：自己回帰言語モデルの理論

この章では、GPTなどの**自己回帰言語モデル**の理論的基礎を学びます。なぜLLMが文章を生成できるのか、その数学的原理を理解していきます。

---

## 5.1 自己回帰モデリングの確率論的基礎

### 5.1.1 言語モデルとは

**定義：**

言語モデルは、文章（単語の系列）の確率分布をモデル化する

$$P(w_1, w_2, \ldots, w_n)$$

ここで $w_i$ は $i$ 番目の単語（トークン）

**直感的理解：**

> 「この文章がどれくらい自然か」を確率で表現

**例：**

```
P("猫が魚を食べる") = 0.001  （自然な文）
P("魚が猫を食べる") = 0.0001 （やや不自然）
P("食べる猫魚が") = 0.00001  （非文法的）
```

### 5.1.2 連鎖律による分解

**確率の連鎖律：**

$$P(w_1, w_2, \ldots, w_n) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_1,w_2) \cdots P(w_n|w_1,\ldots,w_{n-1})$$

一般形：

$$P(w_{1:n}) = \prod_{i=1}^{n} P(w_i | w_{1:i-1})$$

ここで $w_{1:i-1} = (w_1, \ldots, w_{i-1})$ は文脈

**直感的理解：**

> 文章の確率は、各単語を順番に予測する確率の積

**例：「猫が魚を食べる」**

$$P(\text{猫, が, 魚, を, 食べる})$$
$$= P(\text{猫}) \times P(\text{が}|\text{猫}) \times P(\text{魚}|\text{猫, が}) \times P(\text{を}|\text{猫, が, 魚}) \times P(\text{食べる}|\text{猫, が, 魚, を})$$

**視覚化：**

```
文章: [猫] [が] [魚] [を] [食べる]

ステップ1: P(猫) = 0.01
            ↓
ステップ2: P(が|猫) = 0.7
            ↓
ステップ3: P(魚|猫,が) = 0.3
            ↓
ステップ4: P(を|猫,が,魚) = 0.8
            ↓
ステップ5: P(食べる|猫,が,魚,を) = 0.6

全体: 0.01 × 0.7 × 0.3 × 0.8 × 0.6 = 0.001008
```

### 5.1.3 自己回帰モデル

**定義：**

自己回帰モデルは、過去の出力を入力として次を予測

$$P(w_t | w_{<t}) = f_\theta(w_1, w_2, \ldots, w_{t-1})$$

ここで：
- $w_{<t} = (w_1, \ldots, w_{t-1})$：文脈（コンテキスト）
- $f_\theta$：パラメータ $\theta$ を持つモデル（ニューラルネットワーク）

**比喩：**

> 「しりとり」を考える  
> 前の単語を見て、次に続く単語を選ぶ

**LLMでの実装：**

$$P(w_t | w_{<t}) = \text{softmax}(\mathbf{W} \cdot \text{Transformer}(w_1, \ldots, w_{t-1}))$$

```
入力: [猫, が, 魚, を]
    ↓
Transformer
    ↓
出力: 各単語の確率分布
    食べる: 0.6
    見る:   0.2
    追う:   0.1
    ...
```

### 5.1.4 最尤推定

**訓練の目的：**

訓練データの確率を最大化するパラメータ $\theta$ を見つける

**対数尤度：**

$$\mathcal{L}(\theta) = \sum_{(w_1,\ldots,w_n) \in \mathcal{D}} \log P_\theta(w_1, \ldots, w_n)$$

$$= \sum_{(w_1,\ldots,w_n) \in \mathcal{D}} \sum_{t=1}^{n} \log P_\theta(w_t | w_{<t})$$

**最適化問題：**

$$\theta^* = \arg\max_\theta \mathcal{L}(\theta)$$

または等価に、負の対数尤度を最小化：

$$\theta^* = \arg\min_\theta \left( -\sum_{t=1}^{n} \log P_\theta(w_t | w_{<t}) \right)$$

これが**クロスエントロピー損失**！

**直感的理解：**

> 「正解の単語に高い確率を割り当てるようにモデルを訓練」

**例：**

```
文脈: "猫が魚を"
正解: "食べる"

モデルの予測:
  食べる: 0.1  ← 正解に低い確率
  見る:   0.5
  ...

損失: -log(0.1) = 2.3 (大きい → 悪い)

訓練後:
  食べる: 0.8  ← 正解に高い確率
  見る:   0.1
  ...

損失: -log(0.8) = 0.22 (小さい → 良い)
```

---

## 5.2 因果的マスキングの意味

### 5.2.1 問題設定

**自己回帰の制約：**

> 位置 $t$ の予測は、位置 $1, \ldots, t-1$ のみを見られる  
> 未来（位置 $t, t+1, \ldots$）を見てはいけない

**なぜ？**

```
訓練時: "猫が魚を食べる"
        目標: 各位置で次の単語を予測

位置3で "魚" を予測:
  ✓ "猫", "が" を見て良い（過去）
  ✗ "を", "食べる" を見てはダメ（未来）
  
もし未来を見られたら:
  → カンニング！
  → 予測が無意味
```

### 5.2.2 因果的マスキング

**Self-Attentionの問題：**

通常のアテンションは全ての位置を見る

$$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{QK}^\top}{\sqrt{d_k}}\right)\mathbf{V}$$

→ 各位置が未来を見られる！

**解決策：因果的マスク（Causal Mask）**

未来へのアテンションを禁止する

$$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{QK}^\top + \mathbf{M}}{\sqrt{d_k}}\right)\mathbf{V}$$

ここで：

$$M_{ij} = \begin{cases}
0 & \text{if } i \geq j \quad \text{（過去または現在）} \\
-\infty & \text{if } i < j \quad \text{（未来）}
\end{cases}$$

**マスク行列の視覚化：**

```
    Key →
Query  1   2   3   4   5
  ↓
  1   0  -∞  -∞  -∞  -∞
  2   0   0  -∞  -∞  -∞
  3   0   0   0  -∞  -∞
  4   0   0   0   0  -∞
  5   0   0   0   0   0

下三角行列（lower triangular）
```

**Softmax後の効果：**

$$\text{softmax}([\cdots, -\infty, \cdots]) = [\cdots, 0, \cdots]$$

未来の位置への注目重みが0になる！

**例（3単語）：**

```
文: [猫, が, 魚]

マスクなし:
         猫  が  魚
    猫 [ 0.3 0.4 0.3 ]  ← "猫"が"魚"に注目（未来！）
    が [ 0.2 0.5 0.3 ]
    魚 [ 0.1 0.3 0.6 ]

マスクあり:
         猫  が  魚
    猫 [ 1.0 0.0 0.0 ]  ← "猫"は自分だけ見る
    が [ 0.4 0.6 0.0 ]  ← "が"は"猫","が"のみ見る
    魚 [ 0.2 0.3 0.5 ]  ← "魚"は全て見られる
```

### 5.2.3 実装の詳細

**効率的な実装：**

```python
import numpy as np

def causal_mask(seq_len):
    """因果的マスクを生成"""
    # 上三角行列（対角含む）を1で埋める
    mask = np.triu(np.ones((seq_len, seq_len)), k=1)
    # 1の部分を-infに
    mask = mask * -1e9  # -infの代わりに大きな負の数
    return mask

# 使用例
mask = causal_mask(5)
print(mask)

# 出力:
# [[0.  -inf -inf -inf -inf]
#  [0.  0.   -inf -inf -inf]
#  [0.  0.   0.   -inf -inf]
#  [0.  0.   0.   0.   -inf]
#  [0.  0.   0.   0.   0.  ]]
```

**注意点：**

実際には $-\infty$ の代わりに大きな負の数（例：$-10^9$）を使用  
（数値安定性のため）

---

## 5.3 次トークン予測の情報理論的解析

### 5.3.1 パープレキシティ（Perplexity）

**定義：**

パープレキシティは、モデルの「困惑度」を表す指標

$$\text{PPL} = \exp\left(-\frac{1}{N}\sum_{i=1}^{N}\log P(w_i|w_{<i})\right)$$

ここで $N$ は総トークン数

**別の表現：**

$$\text{PPL} = \exp(H)$$

ここで $H$ は平均クロスエントロピー

**直感的解釈：**

> 「モデルが次の単語を予測するとき、平均的に何通りの選択肢で迷っているか」

**例：**

```
完璧な予測:
  P(正解) = 1.0
  PPL = exp(-log(1.0)) = exp(0) = 1
  → 1通りの選択肢（迷わない）

2択で迷う:
  P(正解) = 0.5
  PPL = exp(-log(0.5)) = 2
  → 2通りの選択肢

10択で迷う:
  P(正解) = 0.1
  PPL = exp(-log(0.1)) ≈ 10
  → 10通りの選択肢
```

**実際のLLMの性能：**

```
GPT-2 (2019):  PPL ≈ 20-30 (Penn Treebank)
GPT-3 (2020):  PPL ≈ 15-20
GPT-4 (2023):  さらに改善（非公開）

人間: PPL ≈ 10-12（推定）
```

### 5.3.2 エントロピーと情報量

**文脈に応じたエントロピー：**

$$H(W_t | W_{<t}) = -\sum_{w \in \mathcal{V}} P(w | w_{<t}) \log P(w | w_{<t})$$

ここで $\mathcal{V}$ は語彙

**意味：**

> 「次の単語の不確実性」

**例：**

**文脈1：「太陽は東から」**
```
次の単語の分布:
  昇る: 0.9
  沈む: 0.05
  ...

H ≈ 0.5 bits（低いエントロピー = 予測しやすい）
```

**文脈2：「明日は」**
```
次の単語の分布:
  晴れ: 0.2
  雨: 0.2
  曇り: 0.15
  ...

H ≈ 3 bits（高いエントロピー = 予測しにくい）
```

### 5.3.3 相互情報量

**定義：**

文脈 $C$ と単語 $W$ の相互情報量：

$$I(C; W) = H(W) - H(W|C)$$

**意味：**

> 「文脈によってどれだけ不確実性が減るか」

**例：**

```
文脈なし:
  H(W) = log₂(50000) ≈ 15.6 bits
  （50,000語の語彙から等確率で選ぶ）

文脈「猫が魚を」あり:
  H(W|C) ≈ 3 bits
  （候補が絞られる）

相互情報量:
  I(C; W) = 15.6 - 3 = 12.6 bits
  
→ 文脈が12.6 bitsの情報を提供
```

### 5.3.4 長距離依存と情報減衰

**問題：**

文脈が長くなると、古い情報の影響が減る

**情報減衰のモデル：**

$$I(W_t; W_{t-k}) \propto e^{-\lambda k}$$

ここで：
- $k$：距離
- $\lambda$：減衰率

**視覚化：**

```
相互情報量
  ↑
  |●
  | ●
  |  ●
  |   ●
  |     ●
  |       ●___________
  +─────────────────→ 距離k
  0   5   10   15

近い単語: 強い相互情報量
遠い単語: 弱い相互情報量
```

**Transformerの利点：**

アテンション機構により、距離に関係なく情報を保持可能（理論上）

**実際には：**

- 位置エンコーディングの限界
- 注目の希薄化
- 計算量の制約

→ 文脈長には実質的な限界がある

---

## 5.4 文脈長と記憶容量の理論的限界

### 5.4.1 計算量の問題

**Self-Attentionの計算量：**

$$\mathcal{O}(n^2 d)$$

ここで：
- $n$：系列長
- $d$：モデル次元

**具体例：**

```
系列長n=512, d=768の場合:
  計算量: 512² × 768 ≈ 201M operations

系列長n=2048の場合:
  計算量: 2048² × 768 ≈ 3.2B operations（16倍！）

系列長n=8192の場合:
  計算量: 8192² × 768 ≈ 51B operations（256倍！）
```

**問題：**

系列長を2倍にすると、計算量は4倍に！

**メモリ使用量：**

注目重み行列 $\mathbf{A} \in \mathbb{R}^{n \times n}$

```
n=512:  0.25MB
n=2048: 4MB
n=8192: 64MB (1層、1ヘッド)

GPT-3 (96層, 96ヘッド):
  64MB × 96 × 96 ≈ 600GB！
```

### 5.4.2 効率的な注目機構

#### 1. Sparse Attention

**アイデア：**

全ての位置に注目せず、一部のみに注目

**Local Attention：**

窓幅 $w$ 内のみに注目

$$A_{ij} = \begin{cases}
\text{有効} & |i-j| \leq w \\
0 & \text{otherwise}
\end{cases}$$

**計算量：** $\mathcal{O}(nwd)$

**視覚化：**

```
密なアテンション:       疎なアテンション (w=3):
●●●●●●●●              ●●●○○○○○
●●●●●●●●              ●●●●○○○○
●●●●●●●●              ○●●●●○○○
●●●●●●●●              ○○●●●●○○
●●●●●●●●              ○○○●●●●○
●●●●●●●●              ○○○○●●●●
●●●●●●●●              ○○○○○●●●
●●●●●●●●              ○○○○○○●●
```

#### 2. Sliding Window Attention

**LongFormer, BigBird などで使用**

組み合わせ：
- Local attention（近傍）
- Global attention（特定トークン）
- Random attention（ランダムサンプリング）

#### 3. Linear Attention

**アイデア：**

計算順序を変更して線形時間に

**通常：**
$$\text{Attention} = \text{softmax}(\mathbf{QK}^\top)\mathbf{V}$$
計算順序：$(\mathbf{QK}^\top)\mathbf{V}$ → $\mathcal{O}(n^2d)$

**線形化：**
$$\text{Attention} \approx \phi(\mathbf{Q})(\phi(\mathbf{K})^\top\mathbf{V})$$
計算順序：$\mathbf{Q}(\mathbf{K}^\top\mathbf{V})$ → $\mathcal{O}(nd^2)$

ここで $\phi$ は特徴写像（例：ReLU、ELU）

**問題：**

精度が若干低下（softmaxの近似）

#### 4. Flash Attention

**アイデア：**

GPUメモリ階層を最適化

- 計算量は変わらない（$\mathcal{O}(n^2d)$）
- メモリアクセスを最適化 → 高速化

**GPT-4, LLaMA-2などで採用**

### 5.4.3 位置エンコーディングの外挿

**問題：**

訓練時の最大長を超えると性能低下

```
訓練: 最大2048トークン
推論: 4096トークンを入力

→ 位置2048以降の表現が不適切
```

**ALiBi（Attention with Linear Biases）：**

位置エンコーディングの代わりに、アテンションに直接バイアスを追加

$$\text{softmax}(\mathbf{QK}^\top + \mathbf{B})$$

ここで $B_{ij} = -m \cdot |i - j|$

**効果：**
- 長い系列への外挿が改善
- 位置埋め込み不要

**RoPE（第4章参照）：**

回転位置エンコーディングも良好な外挿性能

### 5.4.4 理論的な記憶容量

**情報理論的限界：**

$d$ 次元、$n$ トークンの表現：

$$C = n \times d \times \log_2(r)$$

ここで $r$ は数値の精度（例：float32なら約32bits）

**GPT-3の例：**

- $n = 2048$
- $d = 12288$（最終層）
- 精度：float16（16bits）

$$C = 2048 \times 12288 \times 16 \approx 402M \text{ bits} \approx 50MB$$

**実効的な容量：**

実際には圧縮された表現 → さらに少ない

**長期記憶のメカニズム：**

1. **パラメータ記憶**
   - 訓練中に獲得した知識
   - FFNに主に保存

2. **文脈記憶**
   - プロンプト内の情報
   - アテンションで保持

3. **外部記憶**（RAGなど）
   - データベース検索
   - モデル外部の知識

---

## 本章のまとめ

### 学んだこと

✅ **自己回帰モデリング**
- 確率の連鎖律による分解
- 次トークン予測
- 最尤推定による訓練

✅ **因果的マスキング**
- 未来を見ない制約
- 下三角マスク行列
- GPTなどのデコーダーモデル

✅ **情報理論的解析**
- パープレキシティ（困惑度）
- エントロピーと相互情報量
- 長距離依存の課題

✅ **文脈長の限界**
- $\mathcal{O}(n^2)$の計算量
- Sparse Attention、Linear Attentionなどの解決策
- 位置エンコーディングの外挿

### 重要な公式

| 概念 | 公式 |
|------|------|
| 連鎖律 | $P(w_{1:n}) = \prod_{i=1}^{n} P(w_i \| w_{<i})$ |
| クロスエントロピー | $\mathcal{L} = -\sum_t \log P(w_t \| w_{<t})$ |
| パープレキシティ | $\text{PPL} = \exp(H)$ |
| 因果マスク | $M_{ij} = \begin{cases} 0 & i \geq j \\ -\infty & i < j \end{cases}$ |
| 計算量 | $\mathcal{O}(n^2 d)$ |

### GPTとBERTの違い

| 特徴 | GPT (Decoder) | BERT (Encoder) |
|------|---------------|----------------|
| マスキング | 因果的（未来を見ない） | 双方向（全体を見る） |
| タスク | 生成 | 理解 |
| 訓練目標 | 次トークン予測 | マスク予測 |
| 用途 | チャット、文章生成 | 分類、抽出 |

### 次章の予告

第6章では、LLMの**スケーリング法則**を学びます：
- パラメータ数と性能の関係
- 計算量のスケーリング
- データ量の影響
- 最適な資源配分

「大きいほど良い」の数学的根拠を理解していきます。

---

## 練習問題

### 問題1：確率の計算
文「猫 が 魚」の確率が以下で与えられるとき、文全体の確率を計算せよ。
- $P(\text{猫}) = 0.02$
- $P(\text{が}|\text{猫}) = 0.8$
- $P(\text{魚}|\text{猫, が}) = 0.4$

### 問題2：パープレキシティ
3つのトークンの確率が $P(w_1|w_{<1})=0.5$, $P(w_2|w_{<2})=0.25$, $P(w_3|w_{<3})=0.5$ のとき、パープレキシティを計算せよ。

### 問題3：因果マスク
系列長4の因果的マスク行列を書け。

### 問題4：計算量
系列長を512から2048に増やしたとき、Self-Attentionの計算量は何倍になるか。

### 解答

**問題1:**
$$P = 0.02 \times 0.8 \times 0.4 = 0.0064$$

**問題2:**
$$H = -\frac{1}{3}(\log 0.5 + \log 0.25 + \log 0.5) = -\frac{1}{3}(-0.693 - 1.386 - 0.693) = 0.924$$
$$\text{PPL} = \exp(0.924) \approx 2.52$$

**問題3:**
$$\begin{bmatrix}
0 & -\infty & -\infty & -\infty \\
0 & 0 & -\infty & -\infty \\
0 & 0 & 0 & -\infty \\
0 & 0 & 0 & 0
\end{bmatrix}$$

**問題4:**
$$\frac{2048^2}{512^2} = \frac{2048^2}{512^2} = 4^2 = 16\text{倍}$$

---

**📖 前章：[第4章 Transformerアーキテクチャの基礎](../第4章_Transformerアーキテクチャの基礎/第4章_Transformerアーキテクチャの基礎.md)**  
**📖 次章：[第6章 スケーリング法則の基礎](../第6章_スケーリング法則の基礎/第6章_スケーリング法則の基礎.md)**
