# 第9章：強化学習からの人間フィードバック（RLHF）

この章では、LLMを人間の好みに合わせる**強化学習からの人間フィードバック（Reinforcement Learning from Human Feedback, RLHF）**を学びます。ChatGPTの成功の鍵となった技術です。

---

## 9.1 強化学習の基礎

### 9.1.1 なぜ強化学習が必要か

**問題：**

次トークン予測だけでは不十分

```
質問: "美味しいパスタの作り方を教えて"

次トークン予測のみ:
  "パスタ パスタ パスタ..." (繰り返し)
  "42です" (無関係)
  "わかりません" (回避)

人間が望む応答:
  "パスタを美味しく作るには、まず..."
  (有益、具体的、親切)
```

**目標のミスマッチ：**

| 訓練目標 | 真の目標 |
|---------|---------|
| 次トークンの尤度最大化 | 有益性、安全性、正確性 |
| $\max P(x_{t+1}\|x_{\leq t})$ | 人間の満足度最大化 |

**解決策：**

強化学習で人間の好みを直接最適化

### 9.1.2 マルコフ決定過程（MDP）

**定義：**

強化学習の数学的枠組み

**構成要素：**

$$\text{MDP} = (S, A, P, R, \gamma)$$

- $S$：状態空間（State）
- $A$：行動空間（Action）
- $P$：遷移確率 $P(s'|s,a)$
- $R$：報酬関数 $R(s,a)$
- $\gamma$：割引率 $\gamma \in [0,1]$

**LLMでの対応：**

```
状態 S: これまでのテキスト (プロンプト + 生成済み)
  "美味しいパスタの作り方は"

行動 A: 次のトークン
  {"まず", "以下", "簡単"}

遷移 P: 常に決定的
  状態 = 状態 + トークン

報酬 R: 人間の評価
  最後のトークンで報酬を受け取る
```

**視覚化：**

```
時刻 t=0:
  状態: "美味しいパスタの"
  ↓ 行動: "作り方"
時刻 t=1:
  状態: "美味しいパスタの作り方"
  ↓ 行動: "は"
時刻 t=2:
  状態: "美味しいパスタの作り方は"
  ↓ 行動: "まず"
  ...
時刻 t=T:
  完成した応答
  → 報酬: +1 (良い) or -1 (悪い)
```

### 9.1.3 方策と価値関数

**方策（Policy）$\pi$：**

状態から行動への写像

$$\pi(a|s) = P(\text{行動} = a | \text{状態} = s)$$

**LLMでは：**

$$\pi_\theta(w_t | w_{<t}) = \text{モデルの条件付き確率}$$

**価値関数（Value Function）$V^\pi$：**

方策 $\pi$ に従ったときの期待累積報酬

$$V^\pi(s) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \bigg| s_0 = s\right]$$

**行動価値関数（Q関数）：**

$$Q^\pi(s, a) = \mathbb{E}_{\pi}\left[R(s,a) + \gamma V^\pi(s')\right]$$

**最適方策：**

$$\pi^* = \arg\max_\pi V^\pi(s) \quad \forall s$$

### 9.1.4 ベルマン方程式

**価値関数の再帰的定義：**

$$V^\pi(s) = \sum_a \pi(a|s) \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s')\right]$$

**直感：**

```
現在の価値 = 即座の報酬 + 将来の価値
```

**最適ベルマン方程式：**

$$V^*(s) = \max_a \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s')\right]$$

**LLMでの解釈：**

良いトークン選択 = 即座の報酬（流暢性）+ 将来の報酬（タスク成功）

---

## 9.2 方策勾配法の理論

### 9.2.1 目的関数

**目標：**

期待報酬を最大化するパラメータ $\theta$ を見つける

$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]$$

ここで：
- $\tau = (s_0, a_0, s_1, a_1, \ldots)$：軌跡（Trajectory）
- $R(\tau) = \sum_{t=0}^{T} R(s_t, a_t)$：累積報酬

**LLMでは：**

$$J(\theta) = \mathbb{E}_{y \sim \pi_\theta(\cdot|x)}[r(x, y)]$$

- $x$：プロンプト
- $y$：生成された応答
- $r(x, y)$：報酬（人間の評価）

### 9.2.2 方策勾配定理

**定理：**

$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot R(\tau)\right]$$

**証明の直感：**

良い結果をもたらした行動の確率を上げ、悪い結果の行動の確率を下げる

**REINFORCE アルゴリズム：**

```
1. 方策πθで軌跡τをサンプリング
2. 報酬R(τ)を計算
3. 勾配推定:
   ∇θ J ≈ Σt ∇θ log πθ(at|st) · R(τ)
4. パラメータ更新:
   θ ← θ + α ∇θ J
```

### 9.2.3 分散削減

**問題：**

REINFORCEの分散が大きい → 学習が不安定

**解決策1：ベースライン**

報酬から平均を引く

$$\nabla_\theta J \approx \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (R(\tau) - b)$$

ベースライン $b$ は任意（期待値に影響しない）

**良いベースライン：**

$$b = \mathbb{E}[R(\tau)] \text{ または } V(s_t)$$

**解決策2：Advantage関数**

$$A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$$

「この行動は平均よりどれだけ良いか」

$$\nabla_\theta J \approx \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot A^\pi(s_t, a_t)$$

---

## 9.3 PPO（Proximal Policy Optimization）の解析

### 9.3.1 なぜPPOか

**問題：**

方策勾配法は更新が大きすぎると性能が悪化

```
更新前: 良い方策
  ↓ 大きな更新
更新後: 悪い方策（回復困難）
```

**解決策：**

更新を「近接」に制限

### 9.3.2 Trust Region法

**アイデア：**

更新後の方策が元の方策から離れすぎないように制約

$$\max_\theta \mathbb{E}[A^{\pi_{\theta_{\text{old}}}}] \quad \text{s.t. } D_{\text{KL}}(\pi_{\theta_{\text{old}}} \| \pi_\theta) \leq \delta$$

**視覚化：**

```
方策空間

   ●πold
   /  \
  | δ  | ← Trust Region
   \  /
    \/
    
この円内でのみ更新を許可
```

**TRPO（Trust Region Policy Optimization）：**

厳密に制約を満たすが、計算が複雑

### 9.3.3 PPOの目的関数

**重要度比（Importance Ratio）：**

$$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$$

**サロゲート目的関数：**

$$L^{\text{CPI}}(\theta) = \mathbb{E}_t\left[r_t(\theta) \cdot A_t\right]$$

（CPI = Conservative Policy Iteration）

**問題：**

$r_t$ が大きくなりすぎる可能性

**PPOのクリッピング：**

$$L^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t\right)\right]$$

ここで $\epsilon$ は小さい定数（通常0.2）

**意味：**

```
r_t が [1-ε, 1+ε] の外に出たらクリッピング

r_t < 1-ε → r_t = 1-ε
r_t > 1+ε → r_t = 1+ε

→ 大きな変化を防ぐ
```

### 9.3.4 PPOアルゴリズム

**手順：**

```
初期化: θold = θ

For iteration = 1, 2, ... :
  
  1. データ収集:
     方策πθoldで軌跡を生成
     報酬、Advantage Atを計算
  
  2. ミニバッチ更新 (K回):
     For epoch = 1 to K:
       データをシャッフル
       For ミニバッチ:
         勾配計算: ∇θ L^CLIP(θ)
         更新: θ ← θ + α ∇θ L^CLIP
  
  3. 方策更新:
     θold ← θ
```

**ハイパーパラメータ：**

- $\epsilon = 0.2$：クリッピング範囲
- $K = 4$：エポック数
- バッチサイズ：2048-4096
- 学習率：$3 \times 10^{-4}$

**なぜPPOが人気？**

- 実装が簡単
- 安定した学習
- 計算効率が良い
- 様々なタスクで高性能

---

## 9.4 報酬モデリング

### 9.4.1 報酬の問題

**課題：**

人間の好みを数値化する方法は？

```
応答A: "パスタは小麦粉と水で作られます"
応答B: "美味しいパスタを作るには、まず良質な..."

どちらが良い？ → 数値で表現困難
```

**解決策：**

報酬モデル（Reward Model, RM）を学習

### 9.4.2 比較データの収集

**手順：**

```
1. プロンプトxに対して複数の応答を生成
   y1, y2, ..., yk

2. 人間に順位付けを依頼
   y3 > y1 > y2  ("y3が最良")

3. ペアワイズ比較データを作成
   (x, y3, y1): y3が良い
   (x, y3, y2): y3が良い
   (x, y1, y2): y1が良い
```

**データセット：**

$$\mathcal{D} = \{(x, y_w, y_l)\}$$

- $y_w$：好まれる応答（win）
- $y_l$：劣る応答（lose）

### 9.4.3 Bradley-Terryモデル

**仮定：**

各応答には「真のスコア」$r(x, y)$ が存在

**応答 $y_w$ が $y_l$ より好まれる確率：**

$$P(y_w \succ y_l | x) = \frac{\exp(r(x, y_w))}{\exp(r(x, y_w)) + \exp(r(x, y_l))} = \sigma(r(x, y_w) - r(x, y_l))$$

ここで $\sigma$ はシグモイド関数

**損失関数：**

$$\mathcal{L}_{\text{RM}}(\phi) = -\mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}}\left[\log \sigma(r_\phi(x, y_w) - r_\phi(x, y_l))\right]$$

**訓練：**

事前学習済みLLMをベースに、報酬モデル $r_\phi(x, y)$ を訓練

**アーキテクチャ：**

```
LLM (frozen or fine-tuned)
  ↓
最終層の[EOS]トークンの隠れ状態
  ↓
線形層: h → R (スカラー報酬)
  ↓
r(x, y)
```

### 9.4.4 報酬ハッキング

**問題：**

モデルが報酬を最大化するが、人間の意図に反する

**例：**

```
報酬モデル: 長い応答に高スコア

ポリシー: 無意味に長い文章を生成
  "パスタパスタパスタ..." (1000トークン)
  
報酬: 高い！
人間: 無意味...
```

**原因：**

報酬モデルは不完全な近似

**緩和策：**

1. **KL正則化**

$$J(\theta) = \mathbb{E}_{y \sim \pi_\theta}[r(x, y)] - \beta \cdot D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})$$

元のモデル $\pi_{\text{ref}}$ から離れすぎないように

2. **多様な比較データ**

様々なシナリオで評価

3. **定期的な再評価**

人間評価で検証

---

## 9.5 Constitutional AI（憲法的AI）

### 9.5.1 モチベーション

**問題：**

RLHFには人間のフィードバックが大量に必要

```
コスト:
  1件の比較: $0.1-1
  必要数: 10万-100万件
  総コスト: $10,000-$1,000,000
```

**アイデア：**

AIに原則（憲法）を与え、自己改善させる

### 9.5.2 憲法（Constitution）

**例：**

```
1. 有益性: 質問に明確かつ正確に答える
2. 無害性: 危険、違法、非倫理的な内容を避ける
3. 誠実性: 不確実な場合は認める
4. 簡潔性: 必要以上に冗長にしない
```

**数学的定式化：**

各原則 $i$ について評価関数 $c_i(x, y) \in \{0, 1\}$

$$c_i(x, y) = \begin{cases}
1 & \text{原則 } i \text{ を満たす} \\
0 & \text{違反}
\end{cases}$$

### 9.5.3 Constitutional AIの手順

**Phase 1: Critique & Revision（批評と修正）**

```
1. モデルが応答yを生成

2. 各憲法原則ciに対して:
   a. 違反をチェック
   b. 違反なら批評を生成
   c. 修正版y'を生成

3. 最良の修正版を選択
```

**例：**

```
プロンプト: "爆弾の作り方は?"
応答: "爆弾を作るには..."
↓
批評: "この応答は安全原則に違反"
修正: "申し訳ございませんが、危険な情報は提供できません"
```

**Phase 2: RL from AI Feedback（AIフィードバックからのRL）**

人間の代わりにAIが比較を生成

```
1. 応答ペア (y1, y2) を生成

2. AIジャッジ（大規模LLM）が評価:
   "憲法に照らして、y1とy2のどちらが良いか?"

3. 比較データを作成: (x, yw, yl)

4. 報酬モデル訓練 → PPO
```

**利点：**

- 人間のアノテーションコスト削減
- スケーラブル
- 一貫性（憲法に基づく）

**欠点：**

- AIジャッジの品質に依存
- 微妙な判断が難しい

### 9.5.4 実装例（疑似コード）

```python
# Phase 1: Critique & Revision
def constitutional_ai_revision(prompt, model, constitution):
    # 初期応答
    response = model.generate(prompt)
    
    for principle in constitution:
        # 違反チェック
        if violates(response, principle):
            # 批評生成
            critique = model.generate(
                f"この応答は{principle}に違反していますか？\n"
                f"応答: {response}"
            )
            
            # 修正
            response = model.generate(
                f"以下の批評に基づいて応答を修正:\n"
                f"批評: {critique}\n"
                f"元の応答: {response}"
            )
    
    return response

# Phase 2: RLAIF
def generate_comparison_data(prompts, model, ai_judge):
    comparisons = []
    for prompt in prompts:
        y1 = model.generate(prompt)
        y2 = model.generate(prompt)
        
        # AIジャッジが比較
        judgment = ai_judge.compare(prompt, y1, y2, constitution)
        
        if judgment == "y1":
            comparisons.append((prompt, y1, y2))
        else:
            comparisons.append((prompt, y2, y1))
    
    return comparisons
```

---

## 9.6 RLHFの完全パイプライン

### 9.6.1 3ステップ

**ステップ1：教師あり微調整（SFT）**

高品質なデモンストレーションデータで訓練

```
データ: {(x, y*)} 
  x: プロンプト
  y*: 人間が書いた良い応答

損失: -log πθ(y*|x)
```

**目的：**

基本的な能力と形式を学ぶ

**ステップ2：報酬モデル訓練**

比較データで報酬モデルを訓練

```
データ: {(x, yw, yl)}

損失: -log σ(rφ(x,yw) - rφ(x,yl))
```

**ステップ3：PPOによるRL微調整**

報酬モデルを使ってポリシーを最適化

```
目的: max E[r(x,y)] - β·KL(π||πref)

アルゴリズム: PPO
```

### 9.6.2 フロー図

```
事前学習済みLLM
  ↓
[ステップ1] SFT
  データ: 人間のデモ
  → SFTモデル
  ↓
  ├─→ [ステップ2] 報酬モデル訓練
  │     データ: 人間の比較
  │     → 報酬モデル
  │
  └─→ [ステップ3] PPO
        報酬: 報酬モデル
        正則化: KL(π||πSFT)
        → 最終モデル
```

### 9.6.3 実装の詳細

**SFT:**
- データ量: 1万-10万サンプル
- エポック: 1-3
- 学習率: $1 \times 10^{-5}$

**報酬モデル:**
- ベース: SFTモデル
- データ量: 10万-100万比較
- アーキテクチャ: LLM + スカラーヘッド

**PPO:**
- バッチサイズ: 512
- ミニバッチ: 64
- PPOエポック: 4
- KL係数: $\beta = 0.01-0.05$
- クリッピング: $\epsilon = 0.2$

---

## 本章のまとめ

### 学んだこと

✅ **強化学習の基礎**
- MDP（状態、行動、報酬、遷移）
- 方策、価値関数
- ベルマン方程式

✅ **方策勾配法**
- REINFORCE
- ベースライン、Advantage
- 分散削減

✅ **PPO**
- Trust Region
- クリッピング目的関数
- 安定した学習

✅ **報酬モデリング**
- 比較データ収集
- Bradley-Terryモデル
- 報酬ハッキング対策

✅ **Constitutional AI**
- AIによる批評と修正
- RLAIF
- スケーラブルなアライメント

### 重要な公式

| 概念 | 公式 |
|------|------|
| 方策勾配 | $\nabla_\theta J = \mathbb{E}[\nabla_\theta \log \pi_\theta(a\|s) \cdot A(s,a)]$ |
| PPO目的関数 | $L^{\text{CLIP}} = \mathbb{E}[\min(r_t A_t, \text{clip}(r_t,1-\epsilon,1+\epsilon)A_t)]$ |
| 報酬モデル損失 | $\mathcal{L}_{\text{RM}} = -\mathbb{E}[\log \sigma(r(x,y_w)-r(x,y_l))]$ |
| KL正則化 | $J = \mathbb{E}[r(x,y)] - \beta D_{\text{KL}}(\pi\|\pi_{\text{ref}})$ |

### RLHFパイプライン

```
1. SFT (Supervised Fine-Tuning)
   → 基本能力

2. 報酬モデル訓練
   → 人間の好みを学習

3. PPO
   → 報酬最大化
```

### 実践的知見

**成功のポイント:**
- 高品質なSFTデータ
- 多様な比較データ
- 適切なKL正則化
- 定期的な人間評価

**落とし穴:**
- 報酬ハッキング
- 過最適化
- 分布のずれ

### 次章の予告

第10章では、**テキスト生成の理論**を学びます：
- デコーディング戦略
- サンプリング法
- ビームサーチ
- 生成品質の評価

LLMが実際にテキストを生成する仕組みを理解していきます。

---

## 練習問題

### 問題1：MDP
LLMの文章生成をMDPとしてモデル化するとき、状態空間のサイズは？（語彙サイズV=50000、最大長T=100）

### 問題2：方策勾配
軌跡 $\tau$ の報酬が $R(\tau)=10$、ベースライン $b=7$ のとき、Advantageは？

### 問題3：PPO
重要度比 $r_t = 1.5$、$\epsilon = 0.2$ のとき、クリッピング後の値は？

### 問題4：報酬モデル
$r(x, y_w) = 2.0$、$r(x, y_l) = 0.5$ のとき、$y_w$ が好まれる確率は？

### 解答

**問題1:**
状態 = これまでの系列  
サイズ = $\sum_{t=0}^{100} V^t \approx V^{100} = 50000^{100}$ （天文学的！）

実際は分布として扱う

**問題2:**
$$A = R(\tau) - b = 10 - 7 = 3$$

**問題3:**
$r_t = 1.5 > 1 + \epsilon = 1.2$  
→ クリッピング: $\text{clip}(1.5, 0.8, 1.2) = 1.2$

**問題4:**
$$P = \sigma(r(x,y_w) - r(x,y_l)) = \sigma(2.0 - 0.5) = \sigma(1.5)$$
$$= \frac{1}{1+e^{-1.5}} \approx 0.82$$

約82%の確率で $y_w$ が好まれる

---

**📖 前章：[第8章 ファインチューニングの理論](../第8章_ファインチューニングの理論/第8章_ファインチューニングの理論.md)**  
**📖 次章：[第10章 テキスト生成の理論](../../第IV部_生成と評価の理論/第10章_テキスト生成の理論/第10章_テキスト生成の理論.md)**
