# 第14章：効率化技術の理論

この章では、大規模LLMを実用的に使うための**効率化技術**の理論を学びます。モデル圧縮、効率的ファインチューニング、高速推論の技術を理解していきます。

---

## 14.1 モデル圧縮：量子化

### 14.1.1 量子化の基礎

**定義：**

高精度の数値を低精度で表現

```
元のパラメータ（32ビット浮動小数点）:
  w = 0.123456789...

量子化後（8ビット整数）:
  w_q = 123

メモリ使用量: 1/4に削減！
```

**数学的定式化：**

$$w_q = \text{round}\left(\frac{w - z}{s}\right)$$

- $s$：スケール（scale）
- $z$：ゼロ点（zero-point）

**逆量子化（推論時）：**

$$w \approx w_q \cdot s + z$$

### 14.1.2 量子化の種類

**対称量子化：**

$$w_q = \text{round}\left(\frac{w}{s}\right), \quad s = \frac{\max(|w|)}{2^{b-1}-1}$$

ゼロ点が0（$z=0$）

**非対称量子化：**

$$s = \frac{\max(w) - \min(w)}{2^b - 1}, \quad z = \text{round}\left(-\frac{\min(w)}{s}\right)$$

範囲を最大限活用

**例：**

```
重み: w = [-0.5, 0.3, 1.2, -0.1]

対称8ビット（範囲: -127〜127）:
  s = 1.2/127 ≈ 0.0094
  w_q = [-53, 32, 127, -11]

非対称8ビット（範囲: 0〜255）:
  min=-0.5, max=1.2
  s = 1.7/255 ≈ 0.0067
  z = 75
  w_q = [0, 120, 254, 60]
```

### 14.1.3 Post-Training Quantization (PTQ)

**手順：**

訓練済みモデルを直接量子化

```
1. 訓練済みモデルをロード
2. 校正データで統計を収集
   - 各層の重みの範囲
   - 活性化の範囲
3. スケールとゼロ点を計算
4. 重みと活性化を量子化
```

**実装例：**

```python
def post_training_quantize(model, calibration_data):
    # 統計収集
    stats = collect_statistics(model, calibration_data)
    
    for layer in model.layers:
        # 重みの量子化
        w = layer.weight
        w_min, w_max = stats[layer]['weight_range']
        
        scale = (w_max - w_min) / 255
        zero_point = -w_min / scale
        
        w_quantized = round((w - zero_point) / scale)
        w_quantized = clip(w_quantized, 0, 255)  # 8ビット
        
        layer.weight_quantized = w_quantized
        layer.scale = scale
        layer.zero_point = zero_point
    
    return model
```

**利点：**
- 追加訓練不要
- 高速

**欠点：**
- 精度低下（特に低ビット）

### 14.1.4 Quantization-Aware Training (QAT)

**アイデア：**

訓練中に量子化をシミュレート

```
Forward Pass:
  w → 量子化 → w_q → 逆量子化 → w' → 計算

Backward Pass:
  勾配は元の精度で計算（Straight-Through Estimator）
```

**Straight-Through Estimator (STE)：**

$$\frac{\partial \mathcal{L}}{\partial w} \approx \frac{\partial \mathcal{L}}{\partial w_q}$$

量子化関数の勾配を恒等関数で近似

**利点：**
- 高精度（PTQより）
- 低ビット（4ビット、2ビット）も可能

**欠点：**
- 訓練コスト

### 14.1.5 混合精度量子化

**アイデア：**

層ごとに異なるビット幅

```
重要な層（Attention）: 8ビット
通常の層（FFN）: 4ビット
最終層: 16ビット

平均: 6ビット
```

**最適化問題：**

$$\min_{\{b_i\}} \mathcal{L}(\text{モデル}) \quad \text{s.t.} \quad \sum_i b_i \cdot |w_i| \leq B$$

- $b_i$：層 $i$ のビット幅
- $B$：総メモリ予算

**感度分析：**

各層を量子化したときの精度への影響を測定

```
層1を4ビット化: 精度 -0.1%
層2を4ビット化: 精度 -2.5%  ← 重要！
層3を4ビット化: 精度 -0.3%

→ 層2は8ビット、他は4ビット
```

---

## 14.2 モデル圧縮：プルーニング

### 14.2.1 プルーニングの基礎

**定義：**

不要なパラメータを削除

```
元のモデル:
  W = [w₁, w₂, w₃, w₄, w₅, ...]

プルーニング後:
  W' = [w₁, 0, w₃, 0, w₅, ...]
         ↑      ↑
       削除   削除

疎化率50%
```

**疎化率（Sparsity）：**

$$\text{Sparsity} = \frac{\text{ゼロの数}}{\text{総パラメータ数}}$$

### 14.2.2 非構造化プルーニング

**重要度による選択：**

小さい重みを削除

```python
def magnitude_pruning(weights, sparsity):
    threshold = percentile(abs(weights), sparsity * 100)
    mask = abs(weights) > threshold
    return weights * mask
```

**例：**

```
重み: [0.5, -0.1, 0.8, -0.05, 0.3]
疎化率: 40%

閾値: 0.3（下位40%）

マスク: [1, 0, 1, 0, 1]
結果: [0.5, 0, 0.8, 0, 0.3]
```

**利点：**
- 高圧縮率

**欠点：**
- 不規則な疎行列 → ハードウェア高速化困難

### 14.2.3 構造化プルーニング

**単位：**

ニューロン単位、ヘッド単位、層単位で削除

```
Attention Heads:
  元: 12ヘッド
  削除: ヘッド3, 7, 10
  残: 9ヘッド

FFN ニューロン:
  元: 3072ニューロン
  削除: 下位25%
  残: 2304ニューロン
```

**利点：**
- 規則的 → ハードウェア高速化容易
- 実際のメモリ削減

**欠点：**
- 圧縮率が低い

### 14.2.4 宝くじチケット仮説

**仮説（Lottery Ticket Hypothesis）：**

大規模ネットワークには、単独で訓練可能な小規模サブネットワーク（「当たりチケット」）が含まれる

**手順：**

```
1. ランダム初期化: θ₀
2. 訓練: θ₀ → θ_trained
3. プルーニング: マスク m を作成
4. 巻き戻し: θ₀ の疎バージョン（m ⊙ θ₀）を訓練
5. 元と同等の性能を達成！
```

**視覚化：**

```
       ランダム初期化
            θ₀
           /  \
          /    \
  訓練   /      \ 巻き戻し
        /        \
    θ_trained   m⊙θ₀
        |          |
   プルーニング    訓練
        |          |
     マスクm    疎モデル
                   ↓
              高性能！
```

**LLMへの適用：**

大規模LLMから小規模LLMを抽出可能

### 14.2.5 勾配ベースプルーニング

**Taylor展開による重要度：**

パラメータ $w$ を削除したときの損失変化

$$\Delta \mathcal{L} \approx \frac{\partial \mathcal{L}}{\partial w} \cdot w + \frac{1}{2} w^2 \frac{\partial^2 \mathcal{L}}{\partial w^2}$$

一次近似：

$$\text{Importance}(w) = \left|\frac{\partial \mathcal{L}}{\partial w} \cdot w\right|$$

**利点：**
- タスク固有の重要度
- 高精度

---

## 14.3 パラメータ効率的ファインチューニング（PEFT）

### 14.3.1 なぜPEFTが必要か

**問題：**

大規模LLMの全パラメータをファインチューニング

```
GPT-3 (175B):
  必要メモリ: 700GB（float32）
  勾配・オプティマイザ状態: 2.1TB
  
総メモリ: ~3TB！
```

**解決策：**

少数のパラメータのみ訓練

### 14.3.2 Adapter

**アーキテクチャ：**

各Transformer層に小さなモジュールを挿入

```
Transformer層
  ↓
Layer Norm
  ↓
Self-Attention
  ↓
[Adapter] ← 追加（訓練可能）
  ↓
Layer Norm
  ↓
FFN
  ↓
[Adapter] ← 追加（訓練可能）
  ↓
出力
```

**Adapterモジュール：**

```
入力 x (d次元)
  ↓
Down-projection: d → r  (r << d)
  ↓
非線形活性化（ReLU/GELU）
  ↓
Up-projection: r → d
  ↓
残差接続: 出力 = x + Adapter(x)
```

**数式：**

$$\text{Adapter}(x) = W_{\text{up}} \cdot \text{ReLU}(W_{\text{down}} \cdot x)$$

$$\text{出力} = x + \text{Adapter}(x)$$

**パラメータ数：**

$$|W_{\text{down}}| + |W_{\text{up}}| = d \times r + r \times d = 2dr$$

$r$（ボトルネック次元）が小さい（例: 64）なら、大幅削減

**例：**

```
d = 1024, r = 64

Adapterパラメータ: 2 × 1024 × 64 = 131,072
元の層: 数百万〜数億

削減率: 99%以上
```

### 14.3.3 LoRA（Low-Rank Adaptation）

**アイデア：**

重み更新を低ランク分解

**数学的定式化：**

元の重み $W_0 \in \mathbb{R}^{d \times k}$ を固定

更新：

$$W = W_0 + \Delta W = W_0 + BA$$

ここで：
- $B \in \mathbb{R}^{d \times r}$
- $A \in \mathbb{R}^{r \times k}$
- $r \ll \min(d, k)$（低ランク）

**視覚化：**

```
元の重み W₀ (d×k)
    [固定]

     +
     
  ΔW = B × A
     (d×r) (r×k)
   [訓練可能]
   
低ランク分解により
パラメータ数激減！
```

**Forward Pass：**

$$y = W_0 x + BAx = W_0 x + B(Ax)$$

**パラメータ数：**

$$|B| + |A| = dr + rk$$

元の $dk$ と比較して、$r$ が小さければ大幅削減

**例：**

```
d = 4096, k = 4096, r = 8

元: 4096² = 16,777,216
LoRA: 4096×8 + 8×4096 = 65,536

削減率: 99.6%
```

**初期化：**

$$A \sim \mathcal{N}(0, \sigma^2), \quad B = 0$$

初期状態で $\Delta W = 0$

**利点：**

- 超低パラメータ
- 推論時のオーバーヘッドなし（$W_0 + BA$ を事前計算）
- 複数タスクで切り替え可能

**タスク切り替え：**

```
ベースモデル W₀ (固定)
  ↓
タスクA: W₀ + B_A A_A
タスクB: W₀ + B_B A_B
タスクC: W₀ + B_C A_C

実行時に適切な(B, A)をロード
```

### 14.3.4 Prefix Tuning

**アイデア：**

入力に訓練可能なプレフィックスを追加

```
入力:
  [Prefix₁] [Prefix₂] ... [Prefix_n] [実際の入力]
   ↑訓練可能                          ↑固定

Prefix: 仮想的なトークン（埋め込み）
```

**数式：**

$$\text{Input} = [\underbrace{P_1, P_2, \ldots, P_n}_{\text{訓練可能}}, x_1, x_2, \ldots, x_m]$$

Attention計算で全トークンを考慮

**利点：**
- パラメータ超少数
- タスクごとに異なるプレフィックス

**欠点：**
- 入力長が増加
- 計算コスト増

### 14.3.5 比較と選択

| 手法 | パラメータ数 | 推論速度 | 精度 | 複数タスク |
|------|------------|---------|------|----------|
| Full FT | 100% | 速い | 最高 | 困難 |
| Adapter | 0.5-8% | やや遅い | 高 | 可能 |
| LoRA | 0.1-1% | 速い* | 高 | 容易 |
| Prefix | 0.01-0.1% | 遅い | 中 | 容易 |

*事前にマージ可能

**推奨：**

- **リソース制約大**: LoRA（r=8-16）
- **複数タスク**: LoRA（タスクごとに(B,A)）
- **最高精度**: Adapter または 小さいFull FT

---

## 14.4 効率的推論

### 14.4.1 KVキャッシュ

**問題：**

自己回帰生成で同じ計算を繰り返す

```
時刻1: "The"
  Attention計算: "The" に対して

時刻2: "The cat"
  Attention計算: "The" と "cat" に対して
  → "The" の計算が重複！
```

**解決策：**

Key, Valueを キャッシュ

```
時刻t:
  新トークン: x_t
  Query: Q_t = x_t W_Q
  
  キャッシュから取得:
    K_{<t} = [K_1, K_2, ..., K_{t-1}]
    V_{<t} = [V_1, V_2, ..., V_{t-1}]
  
  新しいKey, Value:
    K_t = x_t W_K
    V_t = x_t W_V
  
  キャッシュに追加:
    K_{≤t} = [K_{<t}, K_t]
    V_{≤t} = [V_{<t}, V_t]
  
  Attention計算:
    Attention(Q_t, K_{≤t}, V_{≤t})
```

**メモリ使用量：**

系列長 $T$、層数 $L$、ヘッド数 $H$、次元 $d$

$$\text{KVキャッシュサイズ} = 2 \times L \times T \times H \times (d/H) = 2LTd$$

**例：**

```
GPT-3 (175B):
  L = 96, d = 12288, T = 2048
  
  KVキャッシュ: 2 × 96 × 2048 × 12288 × 2bytes
              ≈ 9.6GB (batch_size=1)
```

**トレードオフ：**

計算 ↓、メモリ ↑

### 14.4.2 Multi-Query Attention (MQA)

**問題：**

KVキャッシュが大きい

**アイデア：**

全ヘッドでKey, Valueを共有

```
標準Multi-Head Attention:
  ヘッド1: Q₁, K₁, V₁
  ヘッド2: Q₂, K₂, V₂
  ...
  ヘッドH: Q_H, K_H, V_H

Multi-Query Attention:
  ヘッド1: Q₁, K, V  ← 共有
  ヘッド2: Q₂, K, V  ← 共有
  ...
  ヘッドH: Q_H, K, V ← 共有
```

**KVキャッシュ削減：**

$$\text{標準} = 2LTdH$$
$$\text{MQA} = 2LTd$$

$H$ 倍の削減（通常 $H=32$ など）

**性能：**

わずかな精度低下（1-2%）で大幅な高速化

### 14.4.3 Grouped-Query Attention (GQA)

**折衷案：**

ヘッドをグループ化し、グループ内で共有

```
8ヘッド、2グループの例:

グループ1:
  ヘッド1: Q₁, K₁, V₁  ← グループ内で共有
  ヘッド2: Q₂, K₁, V₁
  ヘッド3: Q₃, K₁, V₁
  ヘッド4: Q₄, K₁, V₁

グループ2:
  ヘッド5: Q₅, K₂, V₂  ← グループ内で共有
  ヘッド6: Q₆, K₂, V₂
  ヘッド7: Q₇, K₂, V₂
  ヘッド8: Q₈, K₂, V₂
```

**削減率：**

グループ数 $G$

$$\text{削減率} = \frac{H}{G}$$

**LLaMA 2での採用：**

- 32ヘッド → 8グループ
- 4倍のKVキャッシュ削減
- 精度は維持

### 14.4.4 投機的デコーディング

**アイデア：**

小さいモデルで複数トークンを予測、大きいモデルで検証

```
1. ドラフトモデル（小）で k個予測
   "The cat sat on"

2. ターゲットモデル（大）で並列検証
   各トークンの確率を計算

3. 不一致があれば、そこで停止
   "The cat sat" ← "on"が不一致
   
4. ターゲットモデルから次を生成
   "The cat sat by"
```

**アルゴリズム：**

```python
def speculative_decoding(target_model, draft_model, prompt, k):
    tokens = [prompt]
    
    while not done:
        # ドラフトで k個生成
        draft_tokens = draft_model.generate(tokens, k)
        
        # ターゲットで検証
        verified = []
        for i, token in enumerate(draft_tokens):
            prob_target = target_model.prob(tokens + draft_tokens[:i], token)
            prob_draft = draft_model.prob(tokens + draft_tokens[:i], token)
            
            if random() < min(1, prob_target / prob_draft):
                verified.append(token)
            else:
                break
        
        # 検証されたトークンを追加
        tokens.extend(verified)
        
        # 必要なら追加生成
        if len(verified) < k:
            next_token = target_model.generate(tokens, 1)
            tokens.append(next_token)
    
    return tokens
```

**高速化：**

理想的には $k$ 倍（実際には2-3倍程度）

**条件：**

ドラフトモデルがターゲットモデルの近似として良好

---

## 14.5 分散学習と推論

### 14.5.1 データ並列

**アイデア：**

データをGPU間で分割

```
GPU 1: バッチ1-32
GPU 2: バッチ33-64
GPU 3: バッチ65-96
GPU 4: バッチ97-128

各GPUが独立に順伝播・逆伝播
  ↓
勾配を集約（All-Reduce）
  ↓
パラメータ更新
```

**利点：**
- 実装が簡単
- スループット向上

**制約：**
- モデルが各GPUに収まる必要

### 14.5.2 モデル並列

**テンソル並列：**

層内でパラメータを分割

```
FFN層: Y = XW (d_in × d_out)

GPU 1: W₁ (d_in × d_out/2)
GPU 2: W₂ (d_in × d_out/2)

W = [W₁, W₂]

計算:
  Y₁ = XW₁ (GPU 1)
  Y₂ = XW₂ (GPU 2)
  Y = [Y₁, Y₂]  (concat)
```

**パイプライン並列：**

層を GPU間で分割

```
GPU 1: 層 1-25
GPU 2: 層 26-50
GPU 3: 層 51-75
GPU 4: 層 76-100

データがパイプライン的に流れる
```

**視覚化：**

```
時刻1: [GPU1処理中] [GPU2待機] [GPU3待機] [GPU4待機]
時刻2: [GPU1処理中] [GPU2処理中] [GPU3待機] [GPU4待機]
時刻3: [GPU1処理中] [GPU2処理中] [GPU3処理中] [GPU4待機]
時刻4: [GPU1処理中] [GPU2処理中] [GPU3処理中] [GPU4処理中]
```

### 14.5.3 3D並列（DeepSpeed）

**組み合わせ：**

データ + テンソル + パイプライン並列

```
64 GPUs = 4 (データ) × 4 (テンソル) × 4 (パイプライン)

最大1兆パラメータのモデルを訓練可能
```

**ZeRO最適化：**

オプティマイザ状態を分散

```
通常:
  各GPU: パラメータ + 勾配 + オプティマイザ状態

ZeRO-1:
  オプティマイザ状態を分割

ZeRO-2:
  オプティマイザ状態 + 勾配を分割

ZeRO-3:
  パラメータ + 勾配 + オプティマイザ状態を分割
```

---

## 本章のまとめ

### 学んだこと

✅ **量子化**
- Post-Training Quantization (PTQ)
- Quantization-Aware Training (QAT)
- 混合精度量子化

✅ **プルーニング**
- 非構造化 vs 構造化
- 宝くじチケット仮説
- 勾配ベース重要度

✅ **PEFT**
- Adapter（ボトルネック）
- LoRA（低ランク分解）
- Prefix Tuning

✅ **効率的推論**
- KVキャッシュ
- Multi-Query / Grouped-Query Attention
- 投機的デコーディング

✅ **分散学習**
- データ並列
- モデル並列（テンソル・パイプライン）
- 3D並列、ZeRO

### 効率化技術の比較

| 技術 | メモリ削減 | 速度向上 | 精度影響 |
|------|----------|---------|---------|
| 8ビット量子化 | 4倍 | 2-3倍 | 小 |
| 4ビット量子化 | 8倍 | 3-4倍 | 中 |
| プルーニング50% | 2倍 | 1.5倍* | 小-中 |
| LoRA (r=8) | 100倍+ | - | 小 |
| KVキャッシュ | -1.5倍 | 10倍+ | なし |

*構造化の場合

### 実践的な組み合わせ

**推論最適化：**

```
1. 量子化（8ビット）: メモリ4倍削減
2. KVキャッシュ: 速度10倍向上
3. GQA: KVキャッシュ4倍削減

総合: メモリ16倍削減、速度10倍向上
```

**ファインチューニング最適化：**

```
1. LoRA (r=16): パラメータ99%削減
2. 8ビット訓練: メモリ4倍削減
3. Gradient Checkpointing: メモリさらに削減

→ 消費者向けGPUでも大規模LLMをFT可能
```

### 次章の予告

第15章では、**解釈可能性と安全性の理論**を学びます：
- Mechanistic Interpretability
- Attention パターンの分析
- プロービング
- 安全性とアライメント
- 有害な出力の防止

LLMの内部動作を理解し、安全に使う技術を学んでいきます。

---

## 練習問題

### 問題1：量子化のメモリ削減
175Bパラメータのモデルを32ビット→8ビット量子化したとき、メモリ削減量は？（1パラメータ=1数値）

### 問題2：LoRAのパラメータ数
$d=2048$、$k=2048$、$r=16$ のとき、LoRAのパラメータ数と元の重み行列のパラメータ数の比は？

### 問題3：KVキャッシュサイズ
層数48、系列長1024、ヘッド数32、ヘッド次元128、float16（2bytes）のとき、KVキャッシュのサイズは？

### 問題4：投機的デコーディング
ドラフトモデルが1トークン/10ms、ターゲットモデルが1トークン/100msで生成。投機的デコーディングで平均3トークン同時検証できる場合、1トークンあたりの時間は？

### 解答

**問題1:**

32ビット: $175 \times 10^9 \times 4\text{bytes} = 700\text{GB}$

8ビット: $175 \times 10^9 \times 1\text{byte} = 175\text{GB}$

削減: $700 - 175 = 525\text{GB}$

**問題2:**

元: $2048 \times 2048 = 4,194,304$

LoRA: $2048 \times 16 + 16 \times 2048 = 65,536$

比: $\frac{65,536}{4,194,304} = \frac{1}{64} \approx 1.56\%$

**問題3:**

$$\text{サイズ} = 2 \times 48 \times 1024 \times 32 \times 128 \times 2\text{bytes}$$
$$= 2 \times 48 \times 1024 \times 32 \times 128 \times 2$$
$$= 805,306,368\text{bytes} \approx 768\text{MB}$$

**問題4:**

通常のターゲットモデル: 100ms/トークン

投機的:
- ドラフト3トークン: $3 \times 10 = 30\text{ms}$
- ターゲット検証（並列）: $100\text{ms}$
- 総時間: $30 + 100 = 130\text{ms}$ で3トークン

1トークンあたり: $130/3 \approx 43.3\text{ms}$

高速化: $100/43.3 \approx 2.3$ 倍

---

**📖 前章：[第13章 多言語・多モーダルモデルの理論](../第13章_多言語・多モーダルモデルの理論/第13章_多言語・多モーダルモデルの理論.md)**  
**📖 次章：[第15章 解釈可能性と安全性の理論](../第15章_解釈可能性と安全性の理論/第15章_解釈可能性と安全性の理論.md)**
